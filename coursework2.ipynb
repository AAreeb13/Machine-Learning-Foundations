{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: 12/12/2025\n",
    "\n",
    "## CID: 02232170\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Approximation theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "$$ f(x) = A_2\\sigma(A_1x + b_1)$$\n",
    "Noting some important details: $x$ is of size d and non-negative ($ \\in [0,1]^d$) and the width of each layer is required to be $d+3$.  We want $w$ ReLU layers which is the same number of rows as $A_1$ and columns as $A_2$. We will rename A1 to $A^{(1)}$  A2 to $A^{(2)}$ and b1 as $b^{(1)}$ to denote the layer they belong to and reference their rows and columns easily.\n",
    "$$\n",
    "   [\\sigma(A^{(1)}x + b^{(1)})]_i = \\sigma(\\sum_{j = 1}^d A_{i, j}^{(1)}x + b_i^{(1)}) \\tag{ ith element after activation}\n",
    "$$\n",
    "We can rewrite f as: $\\sum_{i = 1}^w A_i^{(2)}[\\sigma(A^{(1)}x + b^{(1)})]_i$.\n",
    "\n",
    "<br> The new network we will construct will separate each of the rows of $A^{(1)}$ into w hidden layers, and use $ReLU$ as the activation function. what we need to do <br>\n",
    "1. Preserve x across all layers(of width d) so that we can apply each row of $A^{(1)}$ independently <br>\n",
    "2. compute $sigma(A^{(1)}x + b^{(1)})$ across w hidden and ReLU layers (of width 1) <br>\n",
    "3. store the sum of positive and negative values from previous layers (of width 1 each). We do this because $ReLU$ will be applied on our new layers and only outputs non-negative values, so we need to keep track of positive and negative contributions separately. Note that $x \\in [0,1]^d$ so we dont need to worry about $ReLU$ erasing any values from x. <br>\n",
    "4. The final layer will subtract the negative sum from the positive sum. <br>\n",
    "Dimensionally, we have d+3 nodes per layer, which meets the requirement. <br>\n",
    "\n",
    "Let's define the layers: <br>\n",
    "- $W^{(1)}$ : First layer weights of size (d+3) x d \n",
    "$$\n",
    "   W^{(1)} = \\begin{bmatrix}\n",
    "   I \\\\\n",
    "   A_1^{(1)} \\\\\n",
    "    \\bold{0} \\\\\n",
    "    \\bold{0} \n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "Where I is the identity matrix of size d x d,  $\\bold{0}$ are zero vectors of size 1 x d, $A_1^{(1)}$ is the first row of $A^{(1)}$. <br> We will denote our new bias vector as $c^{(i)}$ for layer i. \n",
    "$$\n",
    "c^{(1)} = \\begin{bmatrix}\n",
    "   \\bold{0} \\\\\n",
    "   b_1^{(1)} \\\\\n",
    "    0 \\\\\n",
    "    0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Thus post-activation we have the following calculations:\n",
    "$$\n",
    "\\sigma(W^{(1)}x + c^{(1)}) = \\begin{bmatrix}\n",
    "   x \\\\\n",
    "   \\sigma(A_1^{(1)}x + b_1^{(1)}) \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- For layer 2 to w we get the following input, Weight and bias: <br>\n",
    "$$\n",
    "\n",
    "    W^{(i)} = \\begin{bmatrix}\n",
    "    I & 0 & 0 & 0\\\\\n",
    "    A_i^{(1)} & 0 & 0 & 0 \\\\\n",
    "    \\bold{0} & \\sigma(A_{i-1}^{(2)}) & 1 & 0 \\\\\n",
    "    \\bold{0} & \\sigma(-A_{i-1}^{(2)}) & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "x' = \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of positive contributions from previous layers} \\\\\n",
    "    \\text{Sum of negative contributions from previous layers}\n",
    "    \\end{bmatrix}\n",
    "    \\;\n",
    "c^{(i)} = \\begin{bmatrix}\n",
    "    \\bold{0} \\\\\n",
    "    b_i^{(1)} \\\\\n",
    "    0 \\\\\n",
    "    0 \n",
    "    \\end{bmatrix}\n",
    "$$ \n",
    "Note that in all our $c^{(i)}$ vectors, $\\bold{0}$ is a zero vector of size d x 1. <br>\n",
    "Explanation of the weight matrix: <br>\n",
    "- First d rows preserve x  due to Identity matrix<br>\n",
    "- (d+1)th row computes $A_i^{(1)}x + b_i^{(1)}$ <br>\n",
    "- (d+2)th row adds the positive contribution from this layer(if any) to the sum of positive contributions from previous layers <br>\n",
    "- (d+3)th row adds the negative contribution from this layer(if any) to the sum of negative contributions from previous layers <br> \n",
    "I have used $\\sigma(A_{i-1}^{(2)})$ to denote whether the output from the previous layer was positive or negative, which determines whether we add to the positive or negative sum. But, if we were to construct it in reality, we would put a zero on row d+2 if $A_{i-1}^{(2)}$ is negative itself otherwise. Likewise for row d+3. This makes it so that the negative and positive contributions are stored as positive values and 0 is added to the layers. We have $I_2$ in the bottom right of W^(i) so that we sum the previously gained contributions to the new contribution. <br><br>\n",
    "\n",
    "To make it more clear we will write the output of 1 layer depending on the sign of $A_{i-1}^{(2)}$: <br>\n",
    "\n",
    "If $A_{i-1}^{(2)}$ is positive: <br>\n",
    "$$\n",
    "    \\implies \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of positive contributions from previous layers} + \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of negative contributions from previous layers}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "If $A_{i-1}^{(2)}$ is negative: <br>\n",
    "$$\n",
    "    \\implies \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of positive contributions from previous layers} \\\\\n",
    "    \\text{Sum of negative contributions from previous layers} + \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)})\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Finally for the output layer we have: <br>\n",
    "$$\n",
    "    W^{output} = \\begin{bmatrix} 0_{1 \\times d} & A^{(2)}_w & -1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "such that when applying to the output of the wth hidden layer we get the calculation:\n",
    "$$\n",
    "\\begin{bmatrix} 0_{1 \\times d} & A^{(2)}_w & -1 & 1 \\end{bmatrix} \\times\n",
    "\\begin{bmatrix} x \\\\ \\sigma(A_{w}^{(1)}x + b_{w}^{(1)}) \\\\ \n",
    "\\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}>0} \\\\\n",
    "\\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}<0}\n",
    "\\end{bmatrix} \\\\\n",
    "\n",
    "\n",
    " = A^{(2)}_w \\sigma(A_{w}^{(1)}x + b_{w}^{(1)}) + \n",
    " \\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}>0} \n",
    " - \\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}<0} \n",
    " = A^{(2)}_w \\sigma(A_{w}^{(1)}x + b_{w}^{(1)}) + \n",
    " \\sum_{j=1}^{w-1} A_{j}^{(2)} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})  \\\\\n",
    " = \\sum_{i = 1}^w A_i^{(2)}[\\sigma(A^{(1)}x + b^{(1)})]_i = f(x)\n",
    "$$\n",
    "Therefore, we define $N(x)$ as the following:\n",
    "$$\n",
    "N(x) = W^{output} \\sigma(W^{(w)} \\sigma(W^{(w-1)} ... \\sigma(W^{(1)}x + c^{(1)}) ... + c^{(w-1)}) + c^{(w)}) \n",
    "$$ \n",
    "and have proved that $$ N(x) = f(x) \\; \\forall x \\in [0,1]^d$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Enter your answer here\n",
    "In our previous answer, we assumed that $ x \\in [0,1]^d$. However, if we relax this assumption and allow $x$ to be in $[-1,1]^d$. One way to handle this is to introduce d extra rows in W and c, $W \\in R^{(2d + 3)\\times d}$ and $c \\in R^{2d + 3}$. We will use it to store $ -x$ alongside x in the first layer.\n",
    "Our new Matrices and vectors will look like this:\n",
    "$$\n",
    "    W^{(1)} = \\begin{bmatrix}\n",
    "    I \\\\\n",
    "    -I \\\\\n",
    "    A_1^{(1)} \\\\\n",
    "     \\bold{0} \\\\\n",
    "     \\bold{0} \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "where, I is the identity matrix of size d x d,  $\\bold{0}$ are zero vectors of size 1 x d, $A_1^{(1)}$ is the first row of $A^{(1)}$. <br> We will denote our new bias vector as $c^{(i)}$ for layer i.\n",
    "$$\n",
    "c^{(1)} = \\begin{bmatrix}\n",
    "    \\bold{0} \\\\\n",
    "    \\bold{0} \\\\\n",
    "    b_1^{(1)} \\\\\n",
    "     0 \\\\\n",
    "     0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $\\bold{0}$ is a zero vector of size d x 1. <br>\n",
    "For $i >1$ we have:\n",
    "$$\n",
    "    W^{(i)} = \\begin{bmatrix}\n",
    "    I & \\bold{0} & 0 & 0 & 0\\\\\n",
    "    \\bold{0} & -I & 0 & 0 & 0\\\\\n",
    "    A_i^{(1)} & -A_i^{(1)} & 0 & 0 & 0 \\\\\n",
    "    \\bold{0} & \\bold{0} & \\sigma(A_{i-1}^{(2)}) & 1 & 0 \\\\\n",
    "    \\bold{0} & \\bold{0} & \\sigma(-A_{i-1}^{(2)}) & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally for the output layer we have: <br>\n",
    "$$ \n",
    "    W^{output} = \\begin{bmatrix} 0_{1 \\times d} & 0_{1 \\times d} & A^{(2)}_w & 1 & -1 \\end{bmatrix}\n",
    "$$\n",
    "Example Output of hidden layer:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(x) \\\\\n",
    "    \\sigma(-x) \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\sum \\sigma(A^{(1)}_j x + b_j^{(1)}) * 1_{A_j^{(2)}>0} \\\\\n",
    "    \\sum \\sigma(A^{(1)}_j x + b_j^{(1)}) * 1_{A_j^{(2)}<0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "We can begin by using the hint.\n",
    "$$\n",
    "h(x) = \\lim_{\\delta \\to 0} \\frac{f((w+\\delta)x) - f(wx)}{\\delta}\n",
    "$$\n",
    "Let's define $g_\\delta(x) = \\frac{f((w+\\delta)x) - f(wx)}{\\delta}$ which is $\\in \\mathcal{F}_{\\sigma, 1} $ since $\\mathcal{F}_{\\sigma, 1}$ is closed under addition and scalar multiplication from the question.\n",
    "Using the Taylor expansion of $f$ about $wx$ we have:\n",
    "$$\n",
    "f((w+\\delta)x) = f(wx) + \\delta x f'(wx) + \\frac{\\delta^2 x^2}{2} f''(z)\n",
    "$$\n",
    "for some $z$ between $wx$ and $(w+\\delta)x$. Thus we have:\n",
    "$$\n",
    "g_\\delta(x) = \\frac{f((w+\\delta)x) - f(wx)}{\\delta} \\\\ =  \\frac{f(wx) + \\delta x f'(wx) + \\frac{\\delta^2 x^2}{2} f''(z) - f(wx)}{\\delta} \\\\ =  x f'(wx) + \\frac{\\delta x^2}{2} f''(z) = h(sx) + \\frac{\\delta x^2}{2} f''(z)\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "|| h - g_\\delta ||_u = \\sup_{x \\in [0,1]} |h(x) - g_\\delta(x)| \\\\\n",
    "= \\sup_{x \\in [0,1]} | \\frac{\\delta x^2}{2} f''(z) | \\leq \\frac{\\delta}{2} \\sup_{x \\in [0,1]} |z^2 f''(z)| \\\\\n",
    "\\leq \\frac{\\delta}{2} \\sup_{x \\in [0,1]} | f''(z)|\n",
    "$$\n",
    "I used that $x \\in [0,1]$. we know that $z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Exercise 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 2: Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Part 3: Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
