{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CID: 02232170\n",
    "\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: [20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "$h^*$ is a minimiser of $\\phi$ means that $\\phi(h^*) \\leq \\phi(h), \\forall h$. <br>\n",
    "\n",
    "$$\n",
    "    \\phi(h^*) + \\langle g, h'- h^* \\rangle = \\phi(h^*) + \\langle 0, h'- h^* \\rangle = \\phi(h^*) \\leq \\phi(h')\n",
    "$$\n",
    "We have proven that with $g=0$, we are able to derive the inequality in the subgradient definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Suppose that  $h^*_1$ and $h^*_2$ both minimise $\\phi$, then using 1.1.1, we know that $g = 0$ is a sub-gradient for both.<br><br>\n",
    "From the definition of $\\alpha$-strong convexity and 1.1.1 we get that\n",
    "$$\n",
    "    \\phi(h^*_1) \\geq \\phi(h^*_2) + \\alpha/2*||h^*_1 - h^*_2||^2.\n",
    "$$\n",
    "We know that $\\phi(h^*_1)$ = $\\phi(h^*_2)$ since they both minimise. Thus, \n",
    "$$\n",
    "  0 \\geq \\alpha/2*||h^*_1 - h^*_2||^2.\n",
    "$$\n",
    "Since $\\alpha$ is positive, it must be that $||h^*_1 - h^*_2|| \\leq 0$ which is only possible if $||h^*_1 - h^*_2|| = 0$, hence $h^*_1 = h^*_2$, from properties of the norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "$\\tilde{h}$ is a minimiser of $\\phi + \\psi $ means $\\phi(\\tilde{h}) + \\psi(\\tilde{h}) \\leq \\phi(h)+ \\psi(h) , \\forall h \\in \\mathcal{H}$.<br><br>\n",
    "\n",
    "Then,\n",
    "$$\n",
    "    \\phi(\\tilde{h}) + \\psi(\\tilde{h}) \\leq \\phi(h^*)+ \\psi(h^*) \\implies  \\phi(\\tilde{h}) - \\phi(h^*) \\leq \\psi(h^*)-\\psi(\\tilde{h}) \n",
    "$$\n",
    "Using L-lipschitz of $\\psi$\n",
    "$$\n",
    "    \\phi(\\tilde{h}) - \\phi(h^*) \\leq \\psi(h^*)- \\psi(\\tilde{h})  \\leq L||h^*-\\tilde{h}||. \n",
    "$$\n",
    "Using $\\alpha$-strong convexity of $\\phi$ and using 1.1.1 (0 is a sub gradient of $h^*$)\n",
    "$$\n",
    "  \\phi(\\~{h}) \\geq \\phi(h) + \\langle 0, \\~{h}- h^* \\rangle + \\frac{\\alpha}{2}||h^*-\\tilde{h}||^2 \\implies   \\phi(\\~{h}) -  \\phi(h) \\geq \\frac{\\alpha}{2}||h^*-\\tilde{h}||^2 \n",
    "\n",
    "$$\n",
    "Combining the two inequalities above and below $\\phi(\\tilde{h}) - \\phi(h^*)$ \n",
    "$$\n",
    "    \\frac{\\alpha}{2}||h^*-\\tilde{h}||^2 \\leq L||h^*-\\tilde{h}||. \\implies ||h^*-\\tilde{h}|| \\leq \\frac{2L}{\\alpha}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "We can define an $f$ as follows:<br>\n",
    "$$\n",
    " f(h) = \\frac{1}{n} \\left( \\ell(h, z_i) - \\ell(h, z_i') \\right) \\\\\n",
    "$$\n",
    "$$\n",
    "f(h) - f(h') = \\frac{1}{n} \\left( \\left( \\ell(h, z_i) - \\ell(h, z_i') \\right) -  \\left( \\ell(h', z_i) - \\ell(h', z_i') \\right) \\right) \\\\\n",
    "\\left| f(h) - f(h') \\right| \\leq \\frac{1}{n} \\left| \\ell(h, z_i) - \\ell(h', z_i) \\right| + \\left| \\ell(h, z_i') - \\ell(h', z_i') \\right| \\\\\n",
    "$$\n",
    "Using that $g_{z_i}(h) = \\ell(h, z_i)$ is $L$-Lipschitz for any $z_i$, we have:\n",
    "$$\n",
    "\\left| f(h) - f(h') \\right| \\leq \\frac{1}{n} \\left( \\sum_{i=1}^n \\left| g_{z_i}(h) - g_{z_i}(h') \\right| + \\left| g_{z_i}(h) - g_{z_i}(h') \\right| \\right) \\\\\n",
    "\\leq \\frac{1}{n} \\left( (L + L) \\|h - h'\\| \\right) = \\frac{2L}{n} \\|h - h'\\| \n",
    "$$\n",
    "hence, the Lipschitz constant of $f$ is $\\frac{2L}{n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "##### Preliminary Proofs:\n",
    "\"The sum of $\\alpha$-strongly convex functions is $\\alpha$-strongly convex.\"<br><br>\n",
    "Proof:<br>\n",
    "Let $\\phi_1$ and $\\phi_2$ be $\\alpha$-strongly convex and $\\beta$-strongly convex functions. Then, for any $h, h' \\in \\mathcal{H}$ and $t \\in [0,1]$, we have:\n",
    "$$\n",
    "\\phi_1(th + (1-t)h') \\leq t\\phi_1(h) + (1-t)\\phi_1(h') - \\frac{\\alpha}{2}t(1-t)||h-h'||^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\phi_2(th + (1-t)h') \\leq t\\phi_2(h) + (1-t)\\phi_2(h') - \\frac{\\beta}{2}t(1-t)||h-h'||^2\n",
    "$$\n",
    "Adding these two inequalities together, we get:\n",
    "$$\n",
    "\\phi_1(th + (1-t)h') + \\phi_2(th + (1-t)h') \\leq t(\\phi_1(h) + \\phi_2(h)) + (1-t)(\\phi_1(h') + \\phi_2(h')) - \\frac{(\\alpha+ \\beta)}{2} t(1-t)||h-h'||^2\n",
    "$$\n",
    "This shows that $\\phi_1 + \\phi_2$ is also $\\\\alpha+\\beta/2$-strongly convex.\n",
    "Hence, by induction, the sum of $n$ $\\alpha$-strongly convex functions is $\\alpha$-strongly convex. For different $\\alpha_i$ for each function, the sum is $(\\sum_{i=1}^{n} \\alpha_i)$-strongly convex.<br><br>\n",
    "\n",
    "Also, note that $c\\phi$ is $\\alpha c$-strongly convex for any $\\alpha$-strongly convex function $\\phi$ and $c > 0$. This follows directly from the usual definition of strong convexity.\n",
    "\n",
    "##### Proof:\n",
    "\n",
    "Using the Lipschitz property of $\\ell$, we have: \n",
    "$$\n",
    "| \\ell(\\hat{h}, z) - \\ell(\\hat{h^{(i)}}, z) | \\leq L ||\\hat{h}- \\hat{h^{(i)}}||\n",
    "$$\n",
    "Now, we would like to bound $||\\hat{h}- \\hat{h^{(i)}}||$.\n",
    "Note that both $\\hat{h}$ and $\\hat{h^{(i)}}$ are minimisers of their respective functions. <br>\n",
    "Define:\n",
    "$$\n",
    "    \\phi(h) = \\frac{1}{n} \\sum_{z_i \\in S}^{n} \\ell(h, z_i)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\phi_{S^{(i)}}(h) = \\frac{1}{n} \\sum_{z_i \\in S^{(i)}} \\ell(h, z_i)\n",
    "$$\n",
    "where $S^{(i)}$ is the dataset $S$ with $z_i$ replaced by $z_i'$.<br>\n",
    "Both $\\phi$ and $\\phi_{S^{(i)}}$ are $\\alpha$-strongly convex as they are sums of $\\frac{1}{n}$-scaled $\\alpha$-strongly convex functions (from the preliminary proof).<br>\n",
    "Now, using the definition of $\\hat{h}$ and $\\hat{h_n^{(i)}}$ as minimisers and $\\alpha$-strong convexity, we have:\n",
    "$$\n",
    "    \\phi(\\hat{h^{(i)}}) \\geq \\phi(\\hat(h)) +\\frac{\\alpha}{2}||\\hat{h}- \\hat{h^{(i)}}||^2 \\implies \\phi(\\hat{h^{(i)}}) - \\phi(\\hat{h}) \\geq \\frac{\\alpha}{2}||\\hat{h}- \\hat{h^{(i)}}||^2\n",
    "$$\n",
    "Similarly,\n",
    "$$\n",
    "    \\phi_{S^{(i)}}(\\hat{h}) \\geq \\phi_{S^{(i)}}(\\hat{h^{(i)}}) +\\frac{\\alpha}{2}||\\hat{h}- \\hat{h^{(i)}}||^2 \\implies \\phi_{S^{(i)}}(\\hat{h}) - \\phi_{S^{(i)}}(\\hat{h^{(i)}}) \\geq \\frac{\\alpha}{2}||\\hat{h}- \\hat{h^{(i)}}||^2\n",
    "$$\n",
    "Adding these two inequalities, we get:\n",
    "$$\n",
    "    \\phi(\\hat{h^{(i)}}) - \\phi(\\hat{h}) + \\phi_{S^{(i)}}(\\hat{h}) - \\phi_{S^{(i)}}(\\hat{h^{(i)}}) \n",
    "    \\geq \\alpha ||\\hat{h}- \\hat{h^{(i)}}||^2\n",
    "$$\n",
    "This equivalent to only considering the terms with $z_i$ and $z_i'$ as all other terms cancel out:\n",
    "$$\n",
    "  \\frac{1}{n} (\\ell(\\hat{h^{(i)}}, z_i) - \\ell(\\hat{h}, z_i) + \\ell(\\hat{h}, z_i) - \\ell(\\hat{h^{(i)}}, z_i')) \n",
    "  \\geq \\alpha ||\\hat{h}- \\hat{h^{(i)}}||^2\n",
    "$$\n",
    "We know that $\\ell$ is L-Lipschitz and from 1.2.1 so using the absolute value to bound the left hand side, we have:\n",
    "$$\n",
    "  \\frac{1}{n} (|\\ell(\\hat{h^{(i)}}, z_i) - \\ell(\\hat{h}, z_i)| + |\\ell(\\hat{h}, z_i) - \\ell(\\hat{h^{(i)}}, z_i')|) \\leq \\frac{L}{n} ||\\hat{h^{(i)}} - \\hat{h}|| + \\frac{L}{n} ||\\hat{h^{(i)}} - \\hat{h}|| = \\frac{2L}{n} ||\\hat{h^{(i)}} - \\hat{h}||\n",
    "$$\n",
    "$$\n",
    "\\alpha ||\\hat{h}- \\hat{h^{(i)}}||^2 \\leq  \\frac{2L}{n} ||\\hat{h^{(i)}} - \\hat{h}|| \\implies ||\\hat{h}- \\hat{h^{(i)}}|| \\leq \\frac{2L}{\\alpha n}\n",
    "$$\n",
    "Using this bound in the Lipschitz inequality at the start, we have:\n",
    "$$\n",
    "| \\ell(\\hat{h}, z) - \\ell(\\hat{h^{(i)}}, z) | \\leq L ||\\hat{h}- \\hat{h^{(i)}}|| \\leq L * \\frac{2L}{\\alpha n} = \\frac{2L^2}{\\alpha n}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_S[R(\\hat{h}) - \\hat{R}_S(\\hat{h})] =  \\mathbb{E}_S[R(\\hat{h})] - \\mathbb{E}_S[\\hat{R}_S(\\hat{h})]\\tag{linearity of Expectation}\n",
    "$$\n",
    "$$\n",
    "= \\mathbb{E}_S[\\mathbb{E}_z[\\ell(\\hat{h}, z)]] - \\mathbb{E}_S[\\frac{1}{n}\\Sigma_{i=1}^{n} \\ell(\\hat{h}, z_i)] \n",
    "$$\n",
    "$$\n",
    "= \\mathbb{E}_{S,z}[\\ell(\\hat{h}, z)] - \\frac{1}{n}\\Sigma_{i=1}^{n}\\mathbb{E}_S[ \\ell(\\hat{h}, z_i)] \\tag{Fubini's Theorem (1)}\n",
    "$$\n",
    "\n",
    "Focusing on the second term, we will introduce a $z$: $$ \\mathbb{E}_{S,z}[\\ell(\\hat{h}, z_i)] $$ We can argue that it is equivalent to: \n",
    "$$ \n",
    "\\mathbb{E}_{S,z}[\\ell(\\hat{h}, z)] \n",
    "$$ \n",
    "each $z_i$ is drawn i.i.d from the same distribution as $z$, hence we can replace one of the $z_i$ with $z$ without changing the expectation. You can think of the expectations as integrals across $ z_1,..,z_n$ and can introduce z as a new element in a new sample. then move the $\\int_z$ to the inside and remove the integral that is outside now. The new $\\hat{h}^{(i)}$ will be training across $z_1,..,z_{i-1},z,z_{i+1},..,z_n$ where we have lost $z_i$ for $z$. Hence, we have:\n",
    "$$\n",
    "\\mathbb{E}_{S,z}[ \\ell(\\hat{h}, z_i)] = \\mathbb{E}_{S,z}[\\ell(\\hat{h^{(i)}}, z)] \n",
    "$$\n",
    "Substituting this back into the original equation, we have:\n",
    "$$\n",
    "\\mathbb{E}_S[R(\\hat{h}) - \\hat{R}_S(\\hat{h})] = \\mathbb{E}_{S,z}[\\ell(\\hat{h}, z)] - \\frac{1}{n}\\Sigma_{i=1}^{n}\\mathbb{E}_{S,z}[\\ell(\\hat{h^{(i)}}, z)] \n",
    "$$\n",
    "Now lets try find a bound for this:\n",
    "$$\n",
    "|\\mathbb{E}_S[R(\\hat{h}) - \\hat{R}_S(\\hat{h})] | = |\\mathbb{E}_{S,z}[\\ell(\\hat{h}, z)] - \\frac{1}{n}\\Sigma_{i=1}^{n}\\mathbb{E}_{S,z}[\\ell(\\hat{h^{(i)}}, z)] |\\\\\n",
    "= |\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{S,z}[\\ell(\\hat{h}, z)] - \\mathbb{E}_{S,z}[\\ell(\\hat{h^{(i)}}, z)] | = |\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{S,z}[\\ell(\\hat{h}, z) - \\ell(\\hat{h^{(i)}}, z)] | \\\\\n",
    "\\leq \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{S,z}[|\\ell(\\hat{h}, z) - \\ell(\\hat{h^{(i)}}, z)|] \\leq \\frac{1}{n}\\sum_{i=1}^{n}\\frac{2L^2}{\\alpha n} = \\frac{2L^2}{\\alpha n}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "Since H is closed, we can say that the infimum is attained within H. Thus, $R^*_H= inf_{h \\in H} R(h) = \\min_{h \\in H} R(h) = R(h^*) $ where h* is the minimiser of R in H. <br>\n",
    "Then, we have:\n",
    "$$\n",
    "\\mathbb{E}_S[R(\\hat{h}) - R_\\mathcal{H}^*] = \\mathbb{E}_S[R(\\hat{h}) - \\hat{R}_S(\\hat{h}) + \\hat{R}_S(\\hat{h}) - R_\\mathcal{H}^*]\\\\\n",
    " = \\mathbb{E}_S[R(\\hat{h}) - \\hat{R}_S(\\hat{h})] + \\mathbb{E}_S[\\hat{R}_S(\\hat{h}) - R(h^*)]\n",
    "$$\n",
    "Focussing on the second term\n",
    "$$\n",
    "\\mathbb{E}_S[\\hat{R}_S(\\hat{h}) - R(h^*)] = \\mathbb{E}_S[\\hat{R}_S(\\hat{h})] - R(h^*) = \\mathbb{E}_S[\\hat{R}_S(\\hat{h})] - \\mathbb{E}_S[\\hat{R}_S(h^*)] = E_S[\\hat{R}_S(\\hat{h}) - \\hat{R}_S(h^*)]\n",
    "$$\n",
    "$\\hat{h}$ minimises $\\hat{R}_S(h)$, hence $\\hat{R}_S(\\hat{h}) \\leq \\hat{R}_S(h^*)$ and thus $\\mathbb{E}_S[\\hat{R}_S(\\hat{h}) - \\hat{R}_S(h^*)] \\leq 0$.<br>\n",
    "\n",
    "Thus, we have:\n",
    "$$\n",
    "\\mathbb{E}_S[R(\\hat{h}) - \\hat{R}_S(\\hat{h})] + \\mathbb{E}_S[\\hat{R}_S(\\hat{h}) - R(h^*)] \\leq \\frac{2L^2}{\\alpha n} + 0 = \\frac{2L^2}{\\alpha n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "We want to provide a $1-\\delta$ bound for $R(\\hat{h}_n) - R^*_H$. In other words, $ P(R(\\hat{h}_n) - R^*_H \\leq K) \\geq 1-\\delta$<br>\n",
    "Define $f(S) = R(\\hat{h}) - R_H^*$ . hence \n",
    "$$\n",
    "|f(S) - f(S^{i})| = |(R(\\hat{h}) - R_H^*) - (R(\\hat{h}^{(i)}) - R_H^*)| = |R(\\hat{h}) - R(\\hat{h}^{(i)})|\\\\ = |\\mathbb{E}_z(\\ell(\\hat{h},z) - \\ell(\\hat{h}^{(i)}, z))| \\leq \\mathbb{E}_z|\\ell(\\hat{h},z) - \\ell(\\hat{h}^{(i)}, z)| \\leq \\frac{2L^2}{\\alpha n}\n",
    "$$ <br>\n",
    "Matching this to McDiarmid's inequality, we are setting $c_i = \\frac{2L^2}{\\alpha n}$ for all i. Hence, $\\Sigma_{i=1}^{n} c_i^2 = n * (\\frac{2L^2}{\\alpha n})^2 = \\frac{4L^4}{\\alpha^2 n}$.<br>\n",
    "Using McDiarmid's inequality, we have an expression for $\\delta$ in terms of $\\epsilon$ and $c_i$:\n",
    "$$\n",
    "    \\mathbb{P}[f(S) - \\mathbb{E}_S[f(S)] \\geq \\epsilon] \\leq \\exp(\\frac{-2\\epsilon^2}{\\sum_i^n c_i^2})\\\\\n",
    "    \\implies \\delta = \\exp(\\frac{-2\\epsilon^2}{\\sum_i^n c_i^2}) \\implies \\log(\\delta) = \\frac{-2\\epsilon^2}{\\sum_i^n c_i^2} \\implies \\epsilon = \\frac{L^2}{\\alpha}\\sqrt{\\frac{2\\log(\\delta^{-1})}{n}}\n",
    "$$\n",
    "$$\\mathbb{E}_S[f(S)] = \\mathbb{E}_S[R(\\hat{h}) - R_H^*] \\leq \\frac{2L^2}{\\alpha n}\\tag{from Question 4}$$\n",
    "Thus, we have:\n",
    "$$\n",
    "\\mathbb{P}[R(\\hat{h}) - R_H^* \\leq \\frac{2L^2}{\\alpha n} + \\frac{L^2}{\\alpha}\\sqrt{\\frac{2\\log(\\delta^{-1})}{n}}] \\geq 1 - \\delta\n",
    "$$\n",
    "By subbing in $f(S) = R(\\hat{h}) - R_H^*$, using the bound from question 4 and rearranging to get  $1 - \\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Define $\\phi_z(h) = \\ell(h,z) + \\frac{\\lambda}{2}||h||^2$\n",
    "Aim: Show that $\\phi_z$ is strongly convex and find its constant.<br>\n",
    "i.e $\\phi_z(th + (1-t)h') \\leq t\\phi_z(h) + (1-t)\\phi_z(h') - \\frac{\\alpha}{2}t(1-t)||h-h'||^2$ for some $\\alpha > 0$. Where $t \\in [0,1]$.\n",
    "Substituting $\\phi_z$,\n",
    "$$\n",
    "    \\ell(th + (1-t)h',z) + \\frac{\\lambda}{2}||th + (1-t)h'||^2 \\leq t\\ell(h,z) + (1-t)\\ell(h',z) + \\frac{\\lambda}{2}(t||h||^2 + (1-t)||h'||^2) - \\frac{\\alpha}{2}t(1-t)||h-h'||^2\n",
    "$$\n",
    "Isolating the term with $\\alpha$, we want to bound the left hand side in the same format as the right hand side:\n",
    "$$\n",
    "    \\ell(th + (1-t)h',z) + \\frac{\\lambda}{2}||th + (1-t)h'||^2 - t\\ell(h,z) - (1-t)\\ell(h',z) - \\frac{\\lambda}{2}(t||h||^2 + (1-t)||h'||^2) \\leq - \\frac{\\alpha}{2}t(1-t)||h-h'||^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ell(th + (1-t)h',z) + \\frac{\\lambda}{2}||th + (1-t)h'||^2 - t\\ell(h,z) - (1-t)\\ell(h',z) - \\frac{\\lambda}{2}(t||h||^2 + (1-t)||h'||^2)\\\\\n",
    "\\leq \\frac{\\lambda}{2}||th + (1-t)h'||^2 - \\frac{\\lambda}{2}(t||h||^2 + (1-t)||h'||^2) \\\\\n",
    "\\text{(since $\\ell$ is convex)} \\\\\n",
    " = \\frac{\\lambda}{2} (||th + (1-t)h'||^2 - t||h||^2 - (1-t)||h'||^2) \\\\\n",
    " \\text{(expanding norms with inner product)} \\\\\n",
    "\n",
    " = \\frac{\\lambda}{2} (t^2||h||^2 + 2t(1-t)\\langle h, h' \\rangle + (1-t)^2||h'||^2 - t||h||^2 - (1-t)||h'||^2) \\\\\n",
    " = \\frac{\\lambda}{2} ( -t(1-t)(||h||^2 - 2\\langle h, h' \\rangle + ||h'||^2)) \\\\\n",
    " = \\frac{\\lambda}{2} ( -t(1-t)||h-h'||^2) \\\\\n",
    "$$\n",
    "We're we collected like terms and factorised out -t(1-t).<br>\n",
    "Thus, we have:\n",
    "$$\n",
    "    \\ell(th + (1-t)h',z) + \\frac{\\lambda}{2}||th + (1-t)h'||^2 - t\\ell(h,z) - (1-t)\\ell(h',z) - \\frac{\\lambda}{2}(t||h||^2 + (1-t)||h'||^2) \\leq - \\frac{\\lambda}{2}t(1-t)||h-h'||^2\n",
    "$$\n",
    "Hence, $\\phi_z$ is $\\lambda$-strongly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "Using the same defintition of $\\phi_z$, we can show that it is also Lipschitz.<br>\n",
    "Aim: Show that $|\\phi_z(h) - \\phi_z(h')| \\leq L'||h-h'||$ for some $L' > 0$.<br>\n",
    "Substituting $\\phi_z$,\n",
    "$$\n",
    "    |\\phi_z(h) - \\phi_z(h')| = |\\ell(h,z) + \\frac{\\lambda}{2}||h||^2 - \\ell(h',z) - \\frac{\\lambda}{2}||h'||^2| \\\\\n",
    "    \\leq |\\ell(h,z) - \\ell(h',z)| + \\frac{\\lambda}{2} | ||h||^2 - ||h'||^2 | \\\\\n",
    "    \\leq L||h-h'|| + \\frac{\\lambda}{2} | ||h||^2 - ||h'||^2 | = L||h-h'||^2 + \\frac{\\lambda}{2}| (||h|| + ||h'||)(||h|| - ||h'||) | \\\\\n",
    "    \\leq L||h-h'|| + \\frac{\\lambda}{2}(||h|| + ||h'||)||h-h'|| \\\\\n",
    "    \\leq L||h-h'|| + \\frac{\\lambda}{2}(B + B)||h-h'|| \\\\\n",
    "    = (L + \\lambda B)||h-h'||\n",
    "    \n",
    "$$\n",
    "We used the cauchy-schwarz and then reverse triangle inequality above. Hence, $\\phi_z$ is Lipschitz with constant $L + \\lambda B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "Idea: Define a new Loss function for which we can define a new risk function and then use the results from part 2 to get the bound.<br>\n",
    "Define a new loss function using our $\\phi_z$ from before:\n",
    "$$\n",
    "   \\ell'(h,z) = \\phi_z(h, z) = \\ell(h,z) + \\frac{\\lambda}{2}||h||^2\n",
    "$$\n",
    "Then our empirical risk function is:\n",
    "$$\n",
    "    \\hat{R}'_S(h) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell'(h, z_i) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\ell(h,z_i) + \\frac{\\lambda}{2}||h||^2 \\right) = \\hat{R}_S(h) + \\frac{\\lambda}{2}||h||^2\n",
    "$$\n",
    "Our new risk function is: \n",
    "$$\n",
    "R'(h) = \\mathbb{E}_z[\\ell'(h,z)] = \\mathbb{E}_z[\\ell(h,z) + \\frac{\\lambda}{2}||h||^2] = R(h) + \\frac{\\lambda}{2}||h||^2\n",
    "$$\n",
    "Our new infimum value is:\n",
    "$$\n",
    "R^*_{H,\\lambda} = inf(R'(h)) = inf ({R(h) + \\frac{\\lambda}{2}||h||^2 | }) \\text{, for h in H}\n",
    "$$\n",
    "Note that ${\\hat{h}_\\lambda}$ minimises $\\hat{R}'_S(h)$. Using 1.2.5, since our new loss function is lipschitz and strongly convex (from 1.3.1 and 1.3.2), we want something of the form:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[R(\\hat{h}) - R_H^* \\leq \\frac{2L^2}{\\alpha n} + \\frac{L^2}{\\alpha}\\sqrt{\\frac{2\\log(\\delta^{-1})}{n}}] \\geq 1 - \\delta\n",
    "$$\n",
    "And with our lipschitz and strong convex constants from before, we have:\n",
    "$$\n",
    "\\mathbb{P}[R'(\\hat{h}_\\lambda) - R^*_{H,\\lambda} \\leq \\frac{2(L + \\lambda B)^2}{\\lambda n} + \\frac{(L + \\lambda B)^2}{\\lambda}\\sqrt{\\frac{2\\log(\\delta^{-1})}{n}}] \\geq 1 - \\delta \\\\\n",
    "\\implies \\mathbb{P}[R'(\\hat{h}_\\lambda) - R^*_{H,\\lambda} \\leq \\frac{(L + \\lambda B)^2}{\\lambda}(\\frac{2}{n} + \\sqrt{\\frac{2\\log(\\delta^{-1})}{n}})] \\geq 1 - \\delta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "We just need to minimise\n",
    "$$ f(\\lambda) = \\frac{(L + \\lambda B)^2}{\\lambda} $$\n",
    " wrt. $\\lambda$ since the other terms do not depend on $\\lambda$.<br>\n",
    "Taking the derivative wrt. $\\lambda$ and setting to 0:\n",
    "$$\n",
    "f'(\\lambda) = \\frac{2B(L + \\lambda B)}{\\lambda} - \\frac{(L + \\lambda B)^2}{\\lambda^2} = 0 \\\\ . \\\\\n",
    "\\implies \\frac{L + \\lambda B}{\\lambda}[2B - \\frac{(L + \\lambda B)}{\\lambda}] = 0 \\\\\n",
    "$$\n",
    "So either $L + \\lambda B = 0$ which is not possible since $\\lambda > 0$ or $\\lambda = \\frac{L}{B}$.<br>\n",
    "\n",
    "providing the second derivative to confirm it's a minima:\n",
    "$$\n",
    "f'(\\lambda) = 2B + B^2 - \\frac{L^2}{\\lambda^2}\n",
    "\\implies f''(\\lambda) = \\frac{2L^2}{\\lambda^3} > 0\n",
    "$$\n",
    "hence $\\lambda = \\frac{L}{B}$ is a minima.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Exercise 2: [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Since $\\nabla f$ is L-Lipschitz, we have: $ \\||\\nabla f(x) - \\nabla f(y)\\||_{2} \\leq L \\||x - y\\||_{2} $.\n",
    "Let's also define $\\Phi(t) = f(x-td)$ such that $ \\Phi'(t) = \\nabla f(x-td)^T (-d) = \\langle -\\nabla f(x-td), d \\rangle $\n",
    ". Using the Fundamental Theorem of Calculus, we have:\n",
    "$$\n",
    "f(x - d) - f(x) = \\Phi(1) - \\Phi(0) = \\int_{0}^{1} \\Phi'(t) ds = \\int_{0}^{1} \\langle  -\\nabla f(x-td), d \\rangle ds\n",
    "$$\n",
    "adding and subtracting $\\nabla f(x)$ inside the inner product, we get:\n",
    "$$\n",
    " = \\int_{0}^{1} \\langle  -\\nabla f(x), d \\rangle ds + \\int_{0}^{1} \\langle  \\nabla f(x) - \\nabla f(x-td), d \\rangle ds\n",
    "$$\n",
    "Using Cauchy-Schwarz inequality on the second term, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\leq \\langle - \\nabla f(x), d \\rangle + \\int_{0}^{1} \\||\\nabla f(x) - \\nabla f(x-td)\\||_{2} \\||d\\||_{2} ds\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "Using the L-Lipschitz property of $\\nabla f$, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\leq \\langle - \\nabla f(x), d \\rangle + \\int_{0}^{1} L \\||td\\||_{2} \\||d\\||_{2} ds = \\langle  -\\nabla f(x), d \\rangle + \\frac{L}{2} \\||d\\||_{2}^2\\\\\n",
    "=- \\langle  \\nabla f(x), d \\rangle + \\frac{L}{2} \\||d\\||_{2}^2 \\\\\n",
    "\\implies f(x - d) \\leq f(x) - \\langle  \\nabla f(x), d \\rangle + \\frac{L}{2} \\||d\\||_{2}^2\n",
    "\\end{align*}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimise we set the gradient to 0 wrt. d:\n",
    "$\\nabla_d (f(x) - \\langle \\nabla f(x), d \\rangle + \\frac{L}{2} ||d||^2) = 0 \\\\ $\n",
    "$\\implies - \\nabla f(x) + L d = 0 \\implies d = \\frac{1}{L} \\nabla f(x)$\n",
    "We know this is a minimise since since the second derivating wrt. d is $L*I$ which is positive definite as $L > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can formulate two inequalities from:<br>\n",
    "    1. f is convex and differentiable everywhere: $f(x) \\geq f(\\bar{x}) + \\langle \\nabla f(\\bar{x}), x - \\bar{x} \\rangle$<br>\n",
    "    2. the equation from part 1: $f(\\hat{x}) \\leq f(\\bar{x}) - \\langle \\nabla f(\\bar{x}), \\bar{d} \\rangle + \\frac{L}{2} ||\\bar{d}||^2$.<br>\n",
    "\n",
    "reversing the second inequality and adding it to the first inequality, we get:\n",
    "$$\n",
    "f(x) - f(\\hat{x}) \\geq \\langle \\nabla f(\\bar{x}), x - \\bar{x} \\rangle - \\langle \\nabla f(\\bar{x}), \\bar{d} \\rangle + \\frac{L}{2} ||\\bar{d}||^2 \\\\ \n",
    "= \\langle \\nabla f(\\bar{x}), x - \\hat{x} \\rangle - \\frac{L}{2} ||\\bar{d}||^2\n",
    "$$\n",
    "Where we used that $L \\bar{d} = \\nabla f(x) $ in the last step.<br><br>\n",
    "We must keep in mind that $\\bar{d} = \\bar{x} - \\hat{x}$<br>\n",
    "Substituting this in, we have:\n",
    "$$\n",
    "\\text{We use that $\\nabla f(x) = L (\\bar{x}- \\hat{x})$ here} \\\\\n",
    "\\begin{align*}\n",
    "f(x) - f(\\hat{x}) \\geq L \\langle \\bar{x} - \\hat{x}, x - \\bar{x} \\rangle + \\frac{L}{2} ||\\bar{x} - \\hat{x}||^2_2 \\\\\n",
    "= \\frac{L}{2} (2 \\langle \\bar{x} - \\hat{x}, x - \\bar{x} \\rangle \n",
    "+ \\langle \\bar{x} - \\hat{x},\\bar{x} - \\hat{x} \\rangle\n",
    "+ \\langle x - \\bar{x}, x - \\bar{x} \\rangle - \\langle x - \\bar{x}, x - \\bar{x} \\rangle) \\\\\n",
    "= \\frac{L}{2} ( \\langle \\bar{x} - \\hat{x} + x - \\bar{x}, \\bar{x} - \\hat{x} + x - \\bar{x} \\rangle \n",
    "- \\langle x - \\bar{x}, x - \\bar{x} \\rangle) \\\\\n",
    "= \\frac{L}{2} ( ||x - \\hat{x}||^2 - ||x - \\bar{x}||^2) \\\\ \n",
    "f(x) - f(\\hat{x}) \\geq \\frac{L}{2} ( ||x - \\hat{x}||^2 - ||x - \\bar{x}||^2) \\\\ \\implies  f(x) + \\frac{L}{2} ||x - \\bar{x}||^2 \\geq f(\\hat{x}) + \\frac{L}{2} ||x - \\hat{x}||^2\n",
    "\\end{align*}\n",
    "$$\n",
    "Note that we used the identity $||a+b||^2  = 2 \\langle a, b \\rangle + ||a||^2 + ||b||^2$ in the third step. by setting a = $\\bar{x} - \\hat{x}$ and b = $x - \\bar{x}$ so that $a + b = x - \\hat{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will prove by induction\n",
    "Base Case: For k = 1\n",
    "$$\n",
    "t_1 = \\frac{1 + \\sqrt{1 + 4 t_0^2}}{2} = \\frac{1 + \\sqrt{1 + 0}}{2} = 1 \\geq 1 = 1 + 1 / 2\n",
    "$$\n",
    "Assume true for some k i.e $ t_{k} \\geq \\frac{k+1}{2}$\n",
    "$$\n",
    "t_{k+1} = \\frac{1 + \\sqrt{1 + 4 t_k^2}}{2} \\geq \\frac{1 + \\sqrt{1 + 4 \\frac{(k+1)^2}{4}}}{2} = \\\\ \\frac{1 + \\sqrt{(k+1)^2 + 1}}{2} \\geq \\frac{1 + \\sqrt{(k+1)^2}}{2} = \\frac{(k+1) + 1}{2} = \\frac{k+2}{2}\n",
    "$$ \n",
    "\n",
    "\n",
    "Hence, by induction, the statement holds for all k >= 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show that $||t_{k+1}(x - \\bar{x}_k) ||^2_2 = ||u_k - x^*||^2_2$\n",
    "$$\n",
    "t_{k+1}(x - \\bar{x}_k) = \\frac{t_{k+1}}{t_{k+1}}((t_{k+1}-1)x_k +x^*) - t_{k+1}(x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1}))\\\\\n",
    "= t_{k+1}x_k-x_k +x^* - (t_{k+1}x_k + t_k(x_k - x_{k-1}) -x_k + x_{k-1})\\\\\n",
    "= t_{k+1}x_k - x_k +x^* - t_{k+1}x_k - t_k(x_k - x_{k-1})  + x_k - x_{k-1}\\\\\n",
    "= - (t_k(x_k-x_{k-1}) - (x_k - x^*)) = u_k - x^*\n",
    "$$\n",
    "Hence, $||t_{k+1}(x - x) ||^2_2 = ||u_k - x^*||^2_2$\n",
    "\n",
    "Now we will show that $t_{k+1}||x-x_{k+1}||^2_2 = ||u_{k+1} -x^*||^2_2$\n",
    "$$\n",
    "    t_{k+1}(x - x_{k+1}) = t_{k+1}x_k - x_k + x^* - t_{k+1}x_{k+1}\\\\\n",
    "    = -( x_k + t_{k+1}(x_{k+1} -x_k) - x^*) = -(u_{k+1} - x^*)\n",
    "\n",
    "$$\n",
    "Hence, $t_{k+1}||x-x_{k+1}||^2_2 = ||u_{k+1} -x^*||^2_2$\n",
    "Thus, \n",
    "$$\n",
    "t_{k+1}^2(||(x - \\bar{x}_k) ||^2_2 - ||x - x_{k+1}||^2_2) = ||u_k - x^*||^2_2 - ||u_{k+1} -x^*||^2_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from convexity of f, we have \n",
    "$$f(x) = f(\\frac{t_{k+1} - 1}{t_{k+1}}x_k + \\frac{1}{t_{k+1}}x^*) \\leq \\frac{t_{k+1} - 1}{t_{k+1}}f(x_k) + \\frac{1}{t_{k+1}}f(x^*)$$\n",
    "where $x = \\frac{t_{k+1} - 1}{t_{k+1}}x_k + \\frac{1}{t_{k+1}}x^*$<br>\n",
    "From 2.3, we have: \n",
    "$$ f(x) + \\frac{L}{2} ||x - \\bar{x_k}||^2 \\geq f(\\hat{x}) + \\frac{L}{2} ||x - \\hat{x}||^2 \\\\ \n",
    "\\implies f(\\hat{x}) - \\frac{L}{2} ( ||x - \\bar{x_k}||^2 - ||x - \\hat{x}||^2) \\leq f(x)\n",
    "$$\n",
    "Combining these two inequalities, we have:\n",
    "$$ f(\\hat{x}) - \\frac{L}{2} ( ||x - \\bar{x_k}||^2 - ||x - \\hat{x}||^2) \\leq\\frac{t_{k+1} - 1}{t_{k+1}}f(x_k) + \\frac{1}{t_{k+1}}f(x^*)\\\\ \n",
    "\\implies t_{k+1}^2 (f(\\hat{x}) - f(x^*)) -  \\frac{L}{2}(||u_k - x^*||^2_2 - ||u_{k+1} -x^*||^2_2) \\leq t_{k+1}(t_{k+1} - 1)(f(x_k) - f(x^*)) \\\\\n",
    "\\implies t_{k+1}^2(f(\\hat{x}) - f(x^*)) - \\frac{L}{2}(||u_k - x^*||^2_2 - ||u_{k+1} -x^*||^2_2) \\leq t_{k}^2(f(x_k) - f(x^*))\n",
    "$$\n",
    "setting f(\\hat{x}) = f(x_{k+1}) we get:\n",
    "$$ t_{k+1}^2(f(x_{k+1}) - f(x^*)) - \\frac{L}{2}(||u_k - x^*||^2_2 - ||u_{k+1} -x^*||^2_2) \\leq t_{k}^2(f(x_k) - f(x^*)) \\\\ \n",
    "\\implies t_{k+1}^2(f(x_{k+1}) - f(x^*)) + \\frac{L}{2}||u_{k+1} -x^*||^2_2 \\leq t_{k}^2(f(x_k) - f(x^*)) + \\frac{L}{2}||u_k - x^*||^2_2\n",
    "$$\n",
    "Inductively, we see that:\n",
    "$$\n",
    "t_{k+1}^2(f(x_{k+1}) - f(x^*)) + \\frac{L}{2}||u_{k+1} -x^*||^2_2 \\leq t_{k}^2(f(x_k) - f(x^*)) + \\frac{L}{2}||u_k - x^*||^2_2 \\leq ... \\leq t_0^2(f(x_0) - f(x^*)) + \\frac{L}{2}||u_0 - x^*||^2_2\n",
    "$$\n",
    "$t_0 = 0, u_0 = x_{-1} + t_0(x_0 - x_{-1}) = x_{-1} = x_0$, hence:\n",
    "$$t_{k}^2(f(x_k) - f(x^*)) \\leq t_{k}^2(f(x_k) - f(x^*)) + \\frac{L}{2}||u_k - x^*||^2_2 \\leq \\frac{L}{2}||x_0 - x^*||^2_2 \\\\\n",
    "\\implies f(x_k) - f(x^*) \\leq \\frac{L||x_0 - x^*||^2_2}{2t_k^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From question 4, we have that $$t_k \\geq \\frac{k+1}{2} \\implies \\frac{1}{t_k^2} \\leq \\frac{4}{(k+1)^2} < \\frac{4}{k^2}$$\n",
    "\n",
    "for k>1.<br>\n",
    "Using this bound in the result from question 6, we have:\n",
    "\n",
    "$$\n",
    "    \\frac{L||x_0 - x^{*}||^2_2}{2t_k^2} \\leq \\frac{L||x_0 - x^{*}||^2_2}{2} * \\frac{4}{k^2} = \\frac{2L||x_0 - x^*||^2_2}{k^2}\n",
    "$$\n",
    "\n",
    "Hence, this has a convergence rate of $O(\\frac{1}{k^2})$. In the lecture notes we have the following Theorem for standard gradient descent:<br>\n",
    "Theorem\n",
    "$$\n",
    "    f(x_k) - f(x^*) \\leq \\frac{2L||x_0 - x^*||^2_2}{t_k}\n",
    "$$\n",
    "\n",
    "This shows that standard gradient descent has a convergence rate of $O(\\frac{1}{k})$. Our accelerated gradient descent method has a convergence rate of $O(\\frac{1}{k^2})$ which is significantly faster than standard gradient descent. Going from linear to quadratic means that we require about the square root of the number of iterations to reach the same error level.\n",
    "\n",
    "What makes this possible is the lookahead step that is used, i.e. $x_{k+1} = \\bar{x}_k - \\frac{1}{L} \\nabla f(\\bar{x}_k)$ where $\\bar{x}_k = x_k + \\frac{t_k - 1}{t_{k+1}} (x_k - x_{k-1})$. Instead of using the gradient at the current point, we use the gradient that is extrapolated from the previous two points. In this case, we make larger steps to our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Exercise 3: [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14447, 8]) torch.Size([14447, 1])\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Train-validation-test split\n",
    "num_samples = X_tensor.shape[0]\n",
    "train_size = int(0.7 * num_samples)\n",
    "val_size = int(0.15 * num_samples)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "X_train = X_tensor[:train_size]\n",
    "y_train = y_tensor[:train_size]\n",
    "X_val = X_tensor[train_size:train_size + val_size]\n",
    "y_val = y_tensor[train_size:train_size + val_size]\n",
    "X_test = X_tensor[train_size + val_size:]\n",
    "y_test = y_tensor[train_size + val_size:]\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have designed a neural network that has an input layer, n hidden lauers and an output layer. When doing the forward pass, each layer applies a linear transformation followed by a activation function step except for the output layer. \n",
    "\n",
    "We used nn.ModuleList so that we can have a variable number of hidden layers n. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module):\n",
    "    # TODO fill in the arguments\n",
    "    def __init__(self, act_func,input_size, output_size, hidden_sizes):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            act_fn - Object of the activation function that should be used as non-linearity in the network.\n",
    "            input_size - Size of the input images in pixels\n",
    "            output_size - Number of classes we want to predict\n",
    "            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO Define and initialize layers here\n",
    "        layers = []\n",
    "        input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "        layers.append(input_layer)\n",
    "        layers.append(act_func)\n",
    "        self.act_func = act_func\n",
    "\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_sizes[i], hidden_sizes[i + 1])\n",
    "            layers.append(hidden_layer)\n",
    "            layers.append(act_func)\n",
    "        output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        # self.act_func = act_func\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO Implement the forward pass\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the network, allows us to see the sizes of alll the layers. The output layer is separated from the ModuleList since it is not followed by an activation function. We have copies of the relu function after every hidden layer. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseNetwork(\n",
      "  (act_func): ReLU()\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([32, 8])\n",
      "None\n",
      "torch.Size([32])\n",
      "None\n",
      "torch.Size([16, 32])\n",
      "None\n",
      "torch.Size([16])\n",
      "None\n",
      "torch.Size([8, 16])\n",
      "None\n",
      "torch.Size([8])\n",
      "None\n",
      "torch.Size([1, 8])\n",
      "None\n",
      "torch.Size([1])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "network = BaseNetwork(nn.ReLU(), X_train.shape[1],y_train.shape[1],[32,16,8])\n",
    "print(network)\n",
    "\n",
    "# ANSWER TO QUESTION 2:\n",
    "for p in network.parameters():\n",
    "    print(p.shape)\n",
    "    print(p.grad)\n",
    "\n",
    "# parameters() is an inbuilt function in PyTorch nn.module class that returns a list of all Tensor parameters of the module.\n",
    "# It outputs the shape and gradient of each parameter in the network. The grad will be none because we haven't performed any bacpropagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch's mse loss function we comput e the mean squared error between the predicted output and the true output. in myLoss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "     \n",
    "    def __init__(self, mynet, X, y, learning_rate, tolerance=1e-6):\n",
    "        '''\n",
    "        Initializes the Optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mynet : nn.Module\n",
    "            The neural network model to be optimized.\n",
    "        X : 2D torch Tensor\n",
    "            Input data with dimensions (n_samples, n_features).\n",
    "        y : 1D torch Tensor\n",
    "            Output/target data with dimension (n_samples,).\n",
    "        learning_rate : float\n",
    "            Learning rate or regularization parameter.\n",
    "        tolerance : float, optional\n",
    "            Tolerance for convergence criteria.\n",
    "        '''\n",
    "\n",
    "        # Storing initial data\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Extracting dimensions\n",
    "        self.n_sample, self.n_feature = X.shape\n",
    "\n",
    "        self.net = mynet\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        # Algorithm history\n",
    "        self.obj_history = []\n",
    "\n",
    "    # TODO: implement myloss method\n",
    "    def myLoss(self, X, y):\n",
    "        out = self.net(X)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(out, y)\n",
    "        return loss\n",
    "    \n",
    "    def plot_loss(self):\n",
    "\n",
    "        if len(self.obj_history) == 1:\n",
    "            raise ValueError('No history to plot.')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.plot(self.obj_history, color='k')\n",
    "        ax.set_ylabel('Objective Function Value')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.grid()\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent optimiser repeatedly updates the weights of the network based on how well the loss function is minimised. We use a constant learning rate when updating the weights. We also make use of an obj_history to track the loss at each iteration so that we can plot it later\n",
    "\n",
    " We tend to finish all iterations without hitting the tolerance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimiser(Optimizer):\n",
    "    def __init__(self, mynet, X, y, learning_rate, tolerance=1e-6):\n",
    "        super().__init__(mynet, X, y, learning_rate, tolerance)\n",
    "\n",
    "        self.algorithm_name = 'Gradient Descent'\n",
    "\n",
    "    def run(self, max_iter=10000):\n",
    "\n",
    "        for it in range (max_iter):\n",
    "            # print(self.obj_history)\n",
    "            self.net.zero_grad()\n",
    "\n",
    "            loss = self.myLoss(self.X, self.y)\n",
    "            loss.backward()\n",
    "            self.obj_history.append(loss.item())\n",
    "            # print(self.obj_history)\n",
    "# \n",
    "\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                for param in self.net.parameters():\n",
    "                    # print(param.grad)\n",
    "                    param -= self.learning_rate * param.grad\n",
    "            if len(self.obj_history) > 1 and abs(self.obj_history[-2] - self.obj_history[-1]) < self.tolerance:\n",
    "                break\n",
    "        print(\"abs(self.obj_history[-2] - self.obj_history[-1]):\", abs(self.obj_history[-2] - self.obj_history[-1]))\n",
    "        print(\"Finished at iteration:\", it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our optimiser for 100 iterations and then plot the loss with our network, that initially uses RELU as the activation function. \n",
    "SELU gives us the lowest final loss value. \n",
    "Equation of SELU:\n",
    "$$\n",
    "selu(x) = \\lambda \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha e^{x} - \\alpha & \\text{if } x \\leq   0 \\end{cases}\n",
    "$$\n",
    "\n",
    "The benefit of selu is that we can be sure that the output is standardised as it is self normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs(self.obj_history[-2] - self.obj_history[-1]): 0.0002816915512084961\n",
      "Finished at iteration: 999\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set: 1.2544015645980835\n",
      "-------------------------\n",
      "Activation Function: Tanh()\n",
      "abs(self.obj_history[-2] - self.obj_history[-1]): 0.0002777576446533203\n",
      "Finished at iteration: 999\n",
      "validation set: 1.2365306615829468\n",
      "-------------------------\n",
      "-------------------------\n",
      "Activation Function: Sigmoid()\n",
      "abs(self.obj_history[-2] - self.obj_history[-1]): 9.5367431640625e-07\n",
      "Finished at iteration: 874\n",
      "validation set: 1.706063151359558\n",
      "-------------------------\n",
      "-------------------------\n",
      "Activation Function: LeakyReLU(negative_slope=0.01)\n",
      "abs(self.obj_history[-2] - self.obj_history[-1]): 0.0001596212387084961\n",
      "Finished at iteration: 999\n",
      "validation set: 1.7359092235565186\n",
      "-------------------------\n",
      "-------------------------\n",
      "Activation Function: ELU(alpha=1.0)\n",
      "abs(self.obj_history[-2] - self.obj_history[-1]): 0.00029855966567993164\n",
      "Finished at iteration: 999\n",
      "validation set: 1.3474937677383423\n",
      "-------------------------\n",
      "-------------------------\n",
      "Activation Function: SELU()\n",
      "abs(self.obj_history[-2] - self.obj_history[-1]): 9.769201278686523e-05\n",
      "Finished at iteration: 999\n",
      "validation set: 0.9403632879257202\n",
      "-------------------------\n",
      "Activation Function: ReLU(), Validation Loss: 1.2544015645980835\n",
      "Activation Function: Tanh(), Validation Loss: 1.2365306615829468\n",
      "Activation Function: Sigmoid(), Validation Loss: 1.706063151359558\n",
      "Activation Function: LeakyReLU(negative_slope=0.01), Validation Loss: 1.7359092235565186\n",
      "Activation Function: ELU(alpha=1.0), Validation Loss: 1.3474937677383423\n",
      "Activation Function: SELU(), Validation Loss: 0.9403632879257202\n"
     ]
    }
   ],
   "source": [
    "#Training and optimising\n",
    "learning_rate = 0.001\n",
    "gd_optimizer = GradientDescentOptimiser(network, X_train, y_train, learning_rate=learning_rate)\n",
    "gd_optimizer.run(max_iter=1000)\n",
    "\n",
    "# plotting training\n",
    "gd_optimizer.plot_loss()\n",
    "\n",
    "# computing loss over validation set\n",
    "val_loss = gd_optimizer.myLoss(X_val, y_val)\n",
    "print(f'validation set: {val_loss.item()}')\n",
    "\n",
    "# test with different activation function\n",
    "losses = {}\n",
    "losses[network.act_func] = val_loss.item()\n",
    "act_funcs = [nn.Tanh(), nn.Sigmoid(), nn.LeakyReLU(), nn.ELU(), nn.SELU()]\n",
    "for act in act_funcs:\n",
    "    print('-------------------------')\n",
    "    network = BaseNetwork(act, X_train.shape[1],y_train.shape[1],[32,16,8])\n",
    "    print(f'Activation Function: {act}')\n",
    "    # network.act_func = act\n",
    "    gd_optimizer = GradientDescentOptimiser(network, X_train, y_train, learning_rate=0.001)\n",
    "    gd_optimizer.run(max_iter=1000)\n",
    "    # gd_optimizer.plot_loss()\n",
    "    val_loss = gd_optimizer.myLoss(X_val, y_val)\n",
    "    losses[act] = val_loss.item()\n",
    "    print(f'validation set: {val_loss.item()}')\n",
    "    print('-------------------------')\n",
    "\n",
    "for act, loss in losses.items():\n",
    "    print(f'Activation Function: {act}, Validation Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt(x):\n",
    "    return x ** 0.5\n",
    "\n",
    "class AcceleratedGradient(Optimizer):\n",
    "    def __init__(self, mynet, X, y, learning_rate, tolerance=1e-6):\n",
    "        super().__init__(mynet, X, y, learning_rate, tolerance)\n",
    "   \n",
    "    def run(self, max_iters=1000):\n",
    "        '''\n",
    "        Runs the optimization algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_iters : int, optional\n",
    "            Maximum number of iterations to run the algorithm. The default is 1000.\n",
    "        '''\n",
    "        t_old = 0\n",
    "        x_old = [p.clone() for p in self.net.parameters()]\n",
    "        for i in range (1, max_iters + 1):\n",
    "\n",
    "            t_new = (1 + sqrt(1 + 4*(t_old**2)))/2\n",
    "            for j, p in enumerate(self.net.parameters()):\n",
    "                x_cur = p.data.clone()\n",
    "                x_bar_cur = x_cur + ((t_old - 1)/t_new) * (x_cur - x_old[j])\n",
    "                x_old[j] = x_cur\n",
    "                p.data = x_bar_cur\n",
    "\n",
    "            #  [x_c + ((t_old - 1)/t_new) * (x_c - x_o) for x_c, x_o in zip(x_cur, x_old)]\n",
    "\n",
    "            self.net.zero_grad()\n",
    "            loss = self.myLoss(self.X, self.y)\n",
    "            loss.backward()\n",
    "            self.obj_history.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for param in self.net.parameters():\n",
    "                    param.data -= self.learning_rate * param.grad\n",
    "            if len(self.obj_history) > 1 and abs(self.obj_history[-2] - self.obj_history[-1]) < self.tolerance:\n",
    "                break\n",
    "            t_old = t_new\n",
    "\n",
    "        # TODO: Implement the accelerated gradient steps here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new Accelerated Gradient optimiser we can see that the loss decreases much faster than with standard gradient descent. Within the first 100 itereations, we can see that the loss has decreased to a very small value. As we showed in Exercise 2, accelerated gradient descent has a convergence rate of $O(\\frac{1}{k^2})$ compared to standard gradient descent which has a convergence rate of $O(\\frac{1}{k})$\n",
    "\n",
    "Comparing this to standard gradient descent, we can see that the loss is still quite high even after 500 iterations. The previous gradient descent method was about 4 times higher in lost going from 1.0 to 0.25. This is surprising as it was not something we explored in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGFCAYAAABg02VjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPEBJREFUeJzt3Ql4U2Xa//G7pQtUKFvZCpUiILLLogy4oLIJCjL4OiqIIIijgDLgBs6LAzoswsigI4LLqOOCqDjiDhRkEWUpICCIbLJvpUBboEhLm/91P++V/LsBTZP0JOd8P9d1THISkqfJ48n55dnCXC6XSwAAAADAB+G+/GMAAAAAIFgAAAAA8AtaLAAAAAD4jGABAAAAwGcECwAAAAA+I1gAAAAA8BnBAgAAAIDPIiSE5ebmyqFDh6RChQoSFhZmdXEAAAAAW9El706dOiXx8fESHh5u32ChoSIhIcHqYgAAAAC2tn//fqlTp459g4W2VLj/0NjYWEvKkJ2dLQsXLpSuXbtKZGSkJWVA8KFegDoBjhPguwN2OJ/IyMgwP+S7z7ttGyzc3Z80VFgZLGJiYszrEyxAvQDHCvD9Ac4pYMfzzOIMO2DwNgAAAACfESwAAAAA+IxgAQAAAMBnBAsAAAAAPiNYAAAAAPAZwQIAAACAzwgWAAAAAHxGsAAAAADgM4IFAAAAAJ8RLAAAAACEdrBITEw0y4MX3IYNGyah4vfff5ctW7bIkSNHrC4KAAAAYJkI615aJDk5WXJycjy3N2/eLF26dJG77rpLQsWdd94pSUlJUqlSJRk6dKjVxQEAAACcFyyqVauW7/bkyZOlfv360rFjxyIff+7cObO5ZWRkmMvs7GyzWaFt27YmWHz//fcyZMgQS8qA4OOuj1bVSwQf6gSoE+A4gVD87vDmtS0NFnllZWXJ+++/L6NGjTLdoYoyadIkGT9+fKH9CxculJiYGLFCVFSUufzuu+/km2++saQMCF4aOgHqBDhOgO8OhOr5RGZmZrEfG+ZyuVwSBD7++GPp27ev7Nu3T+Lj44vdYpGQkCCpqakSGxsrVjh+/LjUrl1bcnNzZc+ePRcsO5xF070eBLRrX2RkpNXFQRCgToA6AY4TCMXvDj3fjouLk/T09EuebwdNi8W///1v6d69+0VPzKOjo81WkL7RVr3ZVatWNWU+cOCAGcRdt25dS8qB4GRl3URwok6AOgGOEwil7w5vXjcoppvdu3evLFq0SB588EEJRfXq1TOXGzdutLooAAAAgCWCIli8/fbbUr16dbntttsklIPFpk2brC4KAAAA4MxgoWMTNFgMGDBAIiKCpmeWV9zdn3S6XAAAAMCJLA8W2gVKB2wPGjRIQpV7XMiuXbtMUAIAAACcxvImgq5du0qQTExVYtqNS1tbzp49K4cOHZI6depYXSQAAADAWS0WdlCmTBlJTEw013fs2GF1cQAAAIBSR7DwkwYNGpjLnTt3+uspAQAAgJBBsPBzsKDFAgAAAE5EsPCT+vXrm0taLAAAAOBEBAs/ocUCAAAATkawCECLBVPOAgAAwGkIFn6is0LplLO///67mXIWAAAAcBKChZ9oqKhdu7a5vn//fn89LQAAABASCBZ+lJCQYC4JFgAAAHAagoUfESwAAADgVAQLP6pTp465pMUCAAAATkOwCECLxYEDB/z5tAAAAEDQI1j4EV2hAAAA4FQECz8iWAAAAMCpCBYBCBZHjhyRrKwsfz41AAAAENQIFn4UFxcnUVFR4nK5WCQPAAAAjkKw8OebGR7umRmKAdwAAABwEoKFnzHOAgAAAE5EsPAzppwFAACAExEs/KxWrVrm8vDhw/5+agAAACBoESz8rGbNmp6ZoQAAAACnIFgEqMWCYAEAAAAnIVgEqMWCrlAAAABwEoKFn9EVCgAAAE5EsAhQsEhLS5OzZ8/6++kBAACAoESw8LNKlSpJdHS0uX706FF/Pz0AAAAQlAgWfhYWFkZ3KAAAADgOwSIAGMANAAAApyFYBABTzgIAAMBpCBYBQIsFAAAAnIZgEQBMOQsAAACnIVgEAF2hAAAA4DQEiwCgKxQAAACcxvJgcfDgQbnvvvukatWqUq5cOWnevLmsXbtWQlm1atXM5bFjx6wuCgAAAFAqIsRCJ0+elOuuu05uvvlm+fbbb80J+Y4dO6Ry5coSyggWAAAAcBpLg8ULL7wgCQkJ8vbbb3v21atX74KPP3funNncMjIyzGV2drbZrOB+3byvr6tvq8zMTElPT5eYmBhLygbrFFUv4GzUCVAnwHECofjd4c1rh7lcLpdYpEmTJtKtWzc5cOCALFu2TGrXri1Dhw6VIUOGFPn4cePGyfjx4wvtnz17dlCdvOtbetddd8n58+fl9ddfl+rVq1tdJAAAAMBr+kN53759zY/lsbGxwRssypYtay5HjRplTsSTk5NlxIgRMmvWLBkwYECxWiy0xSM1NfWSf2ggU1xSUpJ06dJFIiMjPfsTExPl0KFDsnLlSmnTpo0lZYN1LlQv4FzUCVAnwHECofjdoefbcXFxxQoWlnaFys3NlbZt28rEiRPN7VatWsnmzZsvGCyio6PNVpC+0VafvBUsg46z0GCRlpZmedlgnWComwgu1AlQJ8BxAqH03eHN64Zbvd6DdofKq3HjxrJv3z4JdQzgBgAAgJNYGix0Rqht27bl27d9+3apW7euhDqCBQAAAJzE0mAxcuRIWbVqlekKtXPnTjMIWwc7Dxs2TEIdwQIAAABOYmmwuOaaa+Szzz6TDz/8UJo1aybPP/+8TJ8+Xfr16yehjmABAAAAJ7F08La6/fbbzWY3BAsAAAA4iaUtFnZGsAAAAICTECwChGABAAAAJyFYBAjBAgAAAE5CsAhwsNBVCrOysgL1MgAAAEBQIFgESOXKlaVMmTLmempqaqBeBgAAAAgKBItAvbHh4VK1alVz/dixY4F6GQAAACAoECwCiHEWAAAAcAqCRQARLAAAAOAUBIsAIlgAAADAKQgWAUSwAAAAgFMQLAKIYAEAAACnKHGw0LUZtm3bJufPn/dviWyEYAEAAACn8DpYZGZmyuDBgyUmJkaaNm0q+/btM/sfffRRmTx5ciDKGLIIFgAAAHAKr4PFmDFjZOPGjbJ06VIpW7asZ3/nzp3lo48+8nf5QlpcXJy5PH78uNVFAQAAAAIqwtt/MG/ePBMg/vCHP0hYWJhnv7Ze7Nq1y9/lC2nuBfJOnDhhdVEAAACA4Gqx0FWkq1evXmj/mTNn8gUNiFSpUsXTYuFyuXhLAAAAYFteB4u2bdvK119/7bntDhNvvvmmtG/f3r+ls0mLRXZ2tgleAAAAgF153RVq4sSJ0r17d/nll1/MjFAvvfSSuf7jjz/KsmXLAlPKEFWuXDmJjo6Wc+fOmVaL8uXLW10kAAAAIDhaLK6//nrZsGGDCRXNmzeXhQsXmq5RK1eulDZt2gSmlCFKW3MYZwEAAAAn8LrFQtWvX1/eeOMN/5fGpuMsDh06xMxQAAAAsDWvg4V73YoLufzyy30pj+3QYgEAAAAn8DpYJCYmXnT2p5ycHF/LZNuZoQAAAAC78jpY/PTTT/lu64xHum/atGkyYcIEf5bNVi0WBAsAAADYmdfBomXLlkVOQRsfHy9Tp06VPn36+KtstmqxYJE8AAAA2JnXs0JdSKNGjSQ5OdlfT2cbtFgAAADACbxuscjIyMh3W1eUPnz4sIwbN04aNmzoz7LZAi0WAAAAcAKvg0WlSpUKDd7WcJGQkCBz5szxZ9lsgRYLAAAAOIHXwWLJkiX5boeHh0u1atWkQYMGEhFRomUxbI0WCwAAADiB10mgY8eOgSmJTdFiAQAAACcoVrD44osviv2EvXr18qU8tm6xyM3NNS08AAAAgCODRe/evYv1ZDr2ggXyig4WGip04LuOUQEAAADsplg/n+tJcXE2QkVhZcuWlZiYGHOdtSwAAABgV/TLKQWMswAAAIDdlWgapzNnzsiyZctk3759kpWVle++xx57rNjPo2tfjB8/vtBCe7/++qvYrTvU/v37abEAAACAbXkdLH766Sfp0aOHZGZmmoChJ82pqammu0/16tW9ChaqadOmsmjRov9fIBtOWUuLBQAAAOzO67P4kSNHSs+ePWXWrFlSsWJFWbVqlURGRsp9990nI0aM8L4AERFSs2bNYj323LlzZiu4Cnh2drbZrOB+3Yu9vnvAdkpKimXlRPDVCzgLdQLUCXCcQCh+d3jz2l4Hiw0bNshrr71mpk0tU6aMOdG/4oorZMqUKTJgwADp06ePV8+3Y8cOiY+PN4Oc27dvL5MmTZLLL7+8yMfqfQW7TqmFCxd6BkhbJSkp6YL3acuOWr16tdSrV68USwWrXaxewJmoE6BOgOMEvGXld4f2UgpYsNDWCfdaDNr1ScdZNG7c2LRe6DgCb7Rr107eeecdM67i8OHDJjTccMMNsnnzZqlQoUKhx48ZM0ZGjRqVr8UiISFBunbtKrGxsWJVitMPu0uXLua9KcqPP/4oCxYsMF2itBsZ7K849QLOQp0AdQIcJxCK3x3uHkIBCRatWrWS5ORkadiwoVmF+9lnnzVjLN577z1p1qyZV8/VvXt3z/UWLVqYoFG3bl35+OOPZfDgwYUeHx0dbbaC9I22+uTtYmXQAKbS0tIsLydKVzDUTQQX6gSoE+A4gVD67vDmdYs93ax7jYqJEydKrVq1zPUJEyZI5cqV5ZFHHpFjx47J66+/Lr7QsQhXXnml7Ny5U+yEwdsAAACwu2IHi9q1a8vo0aNNl6Obb77Z80v8/PnzTRPJunXrpGXLlj4V5vTp07Jr1y5PcLHb6tsskAcAAABxerAYNmyYzJ0714yn0HEQOjbCm8EcRXniiSfMehh79uwx4xD++Mc/mgHh9957r9iJtuqokydPWl0UAAAAwNpgMXbsWNNFafHixWYWqOHDh5uWhSFDhpjZjkriwIEDJkTo4O0//elPpsuQTl9brVo1sROCBQAAAOzO68HbN910k9lmzJghc+bMMS0XOk2stmTogOu8szZdiv57J8gbLFwul4SFhVldJAAAAMCaFouCypcvLw8++KCsWLFCvvzySzly5Ig8+eST/i2dzYKFDoDXcSQAAACA3ZQ4WOj4Cm2t0Clne/XqZbox6SxRKKxcuXISFRVlrjPOAgAAAHbkdbDQQdbaUqHjK3RAd2JioixZskS2b99uZo1CYdr1iXEWAAAAsLNij7GYMmWKvP322yZAtG3bVqZOnWoGXhe1QjaKXqPj6NGjtFgAAADA2cFCg8R9990nn3zyidcrbIOZoQAAAGBvxQ4Whw4dsmwpcTugKxQAAADsrNhjLAgVviFYAAAAwM5KPCsUvEOwAAAAgJ0RLEoJwQIAAAB2RrAo5WCRlpZWWi8JAAAABN/g7bxyc3Nl586dkpKSYq7ndeONN/qrbLZCiwUAAADszOtgsWrVKunbt6/s3btXXC5XoYXgcnJy/Fk+2yBYAAAAwM68DhYPP/ywWSDv66+/Nqtva5jApREsAAAAYGdeB4sdO3bI3LlzpUGDBoEpkU0RLAAAAGBnXg/ebteunRlfgZIHi4JdyAAAAADHtVg8+uij8vjjj8uRI0ekefPmhRbOa9GihT/LZ7tgkZ2dLZmZmXLZZZdZXSQAAADAumBx5513mstBgwZ59uk4C/0VnsHbF6ZBIiIiQs6fP29aLQgWAAAAcHSw2L17d2BKYnMaurTV4tixYyZY1KlTx+oiAQAAANYFi7p16/rv1R0mb7AAAAAAxOkL5O3atUumT58uW7duNbebNGkiI0aMkPr16/u7fLbCzFAAAACwK69nhVqwYIEJEmvWrDEDtXVbvXq1NG3aVJKSkgJTSpuoVKmSuaTFAgAAAOL0FovRo0fLyJEjZfLkyYX2P/3009KlSxd/ls9WaLEAAACAXXndYqHdnwYPHlxov84S9csvv/irXLZEsAAAAIBdeR0sqlWrJhs2bCi0X/dVr17dX+WyJYIFAAAA7MrrrlBDhgyRhx56SH777Tfp0KGD2ffDDz/ICy+8IKNGjQpEGW2DYAEAAAC78jpYjB07VipUqCAvvviijBkzxuyLj4+XcePGyWOPPRaIMtouWKSlpVldFAAAAMDaYKELvengbd1OnTpl9mnQwKXRYgEAAAC7KtE6Fm4ECu8QLAAAAODoYNG6dWtZvHixOTFu1aqVabW4kPXr1/uzfLZCsAAAAICjg8Udd9wh0dHRnusXCxa4MIIFAAAAHB0s/va3v3mu6yBt+BYszp07J2fPnpVy5crxVgIAAMCZ61hcccUVcvz48UL7daYjvQ8XH5NSpkwZc/3kyZO8VQAAAHBusNizZ4/k5OQU2q+/wh84cMBf5bIl7UJWqVIlc51gAQAAAEfOCvXFF194ri9YsEAqVqzoua1BQwd316tXr8QFmTx5slkXY8SIETJ9+nSxc3cobfEhWAAAAMCRwaJ3796eX90HDBiQ777IyEhJTEw0i+aVRHJysrz22mvSokULsTsGcAMAAMDRwSI3N9dcaquEBoG4uDi/FOD06dPSr18/eeONN+Tvf//7RR+r3a10c8vIyDCX2dnZZrOC+3WL+/rurlDHjh2zrMwIvnoB+6NOgDoBjhMIxe8Ob17b6wXydu/eLf40bNgwue2226Rz586XDBaTJk2S8ePHF9q/cOFCiYmJESslJSUV63E6G5T68ccfpWrVqgEuFaxW3HoB56BOgDoBjhMIpe+OzMzMwAWLxx57TBo0aGAu83rllVdk586dXo2PmDNnjllQT1tAikPHYIwaNSpfi0VCQoJ07dpVYmNjxaoUpx92ly5dTJewS/n6669lxYoVEh8fLz169CiVMiL46wXsjzoB6gQ4TiAUvzvcPYQCEiw+/fTTfAO53Tp06GAGYBc3WOzfv98M1NY3q2zZssX6N7pIn3uhvrz0jbb65K24ZXC3UuiHZHWZEXjBUDcRXKgToE6A4wRC6bvDm9f1OljojEZ5Z4Ry0xaD1NTUYj/PunXrJCUlRVq3bp1vdqnly5eb1g8dS+Fe88FOGLwNAAAAO/J6HQvtBjV//vxC+7/99luvFsjr1KmT/Pzzz7JhwwbP1rZtWzOQW6/bMVQoggUAAADsyOsWCx3jMHz4cDOr0S233GL26RoWOtWsN+MrdBXqZs2a5dt32WWXma5CBffbCQvkAQAAwI68DhaDBg0y3ZQmTJggzz//vNmna1jMnDlT7r///kCU0ZYtFmlpaVYXBQAAALAuWKhHHnnEbNpqUa5cOSlfvrxfCrN06VKxO7pCAQAAwI5KFCzcqlWr5r+SOATBAgAAAHbk9eDto0ePSv/+/c06DBEREWaQdd4NxRtjoYuNZGVl8XYBAADAmS0WAwcOlH379snYsWOlVq1aEhYWFpiS2VTeqXp1nEX16tUtLQ8AAABgSbDQVaO///57ufrqq/1SAKfRVh0NF+np6XLy5EmCBQAAAJzZFSohIUFcLldgSuMQjLMAAACAOD1Y6FoVo0ePlj179gSmRA5AsAAAAIA4vSvU3XffbQYe169fX2JiYiQyMjLf/SdOnPBn+Ww9gJu1LAAAAODYYOHN6tooGi0WAAAAEKcHiwEDBgSmJA5CsAAAAIA4PVjoVLMXc/nll/tSHkcgWAAAAECcHiwSExMvunZFTk6Or2WyPcZYAAAAQJweLH766ad8t7Ozs82+adOmyYQJE/xZNtuixQIAAADi9GDRsmXLQvvatm0r8fHxMnXqVOnTp4+/ymZbBAsAAACI09exuJBGjRpJcnKyv57O1ggWAAAAEKe3WGRkZOS7ratwHz58WMaNGycNGzb0Z9lsizEWAAAAEKcHCz0pLjh4W8NFQkKCzJkzx59lsy1aLAAAACBODxZLlizJdzs8PFyqVasmDRo0kIgIr5/O0cEiPT3dzKJVpkwZq4sEAAAA+KTYSeDZZ5+V0aNHS8eOHc3tkydPek6QUbKuUO5wUaVKFd5CAAAAOGPwtk4le/r0ac/tunXrym+//RaoctlaVFSUxMTEmOtpaWlWFwcAAAAovWCh4ygudhveYZwFAAAA7MRv083COwQLAAAAOHKMhc4EderUKSlbtqxprdDb2jWq4PSzsbGxgSin7RAsAAAA4MhgoWHiyiuvzHe7VatW+W5r2NBZjnBprGUBAAAARwaLgtPMwje0WAAAAMCRwcI9zSz8g2ABAAAAO2HwtkUIFgAAALATgoXFYyx0oUEAAAAg1BEsLG6xYIE8AAAA2AHBwiJ0hQIAAICdlDhY7Ny5UxYsWCBnz541t1mJ2zsECwAAADg6WBw/flw6d+5s1rTo0aOHHD582OwfPHiwPP7444Eooy0xxgIAAACODhYjR46UiIgI2bdvn8TExHj233333TJ//nx/l88RYyxo7QEAAIBj1rFwW7hwoekCVadOnXz7GzZsKHv37vVn2RwRLHSl8tOnT0uFChWsLhIAAABQei0WZ86cyddS4XbixAmJjo726rlmzpwpLVq0kNjYWLO1b99evv32W3GCcuXKSVRUlLnOlLMAAABwXLC44YYb5N133/XcDgsLk9zcXJkyZYrcfPPNXj2XtnpMnjxZ1q1bJ2vXrpVbbrlF7rjjDtmyZYvYnb5vjLMAAACAY7tCaYDo1KmTCQJZWVny1FNPmSCgLRY//PCDV8/Vs2fPfLcnTJhgWjFWrVolTZs2LfT4c+fOmc0tIyPDXGZnZ5vNCu7XLcnra7BISUmR1NRUy8qP4KsXsCfqBKgT4DiBUPzu8Oa1vQ4WzZo1k+3bt8srr7xixgXo+IA+ffrIsGHDpFatWlJSOtbgk08+MV2ttEtUUSZNmiTjx48vctxHUd2zSlNSUlKJWi3UokWLzPsI+ylJvYC9USdAnQDHCYTSd0dmZmaxHxvmsnhKop9//tkEid9//13Kly8vs2fPNtPYFqWoFouEhATzi7+O0bAqxemH3aVLF4mMjPTq3/bq1cvMpPXGG2/IgAEDAlZGhFa9gD1RJ0CdAMcJhOJ3h55vx8XFSXp6+iXPt71usWjQoIHcd9990q9fPzMTlK8aNWokGzZsMIWdO3euOcFetmyZNGnSpNBjdXB4UQPE9Y22+uStJGVwzwylH5jV5UdgBEPdRHChToA6AY4TCKXvDm9e1+vB29rl6euvvzaB4JprrpGXXnpJjhw5IiWlMyNpWGnTpo3p6tSyZUvznE5bywIAAABw3AJ5ycnJ8uuvv5ouSzNmzDDdkbp27ZpvtqiS0hmm8nZ3ckKwYLpZAAAAOC5YuF155ZVmILUO5P7+++/l2LFj8sADD3j1HGPGjJHly5fLnj17zFgLvb106VLTzcoJCBYAAACwC6/HWOS1Zs0aM9j6o48+MuME7rrrLq/+vU61ev/998vhw4elYsWKZrE8XdVbB6g4AetYAAAAwLHBQlsoPvjgA/nwww9l9+7dZlG7F154wUw5q7M6eePf//63OBljLAAAAODYYHHVVVeZQds6iPuee+6RGjVqBKZkDkBXKAAAADg2WGzbts0v08yCYAEAAAAHD94mVPgPLRYAAABwVItFlSpVzNgKXXVPT4bDwsIu+NgTJ074s3yOGLyt0+vqyuNly5a1ukgAAABA4ILFP//5T6lQoYLn+sWCBYpP39Pw8HCzdoeuZVGrVi3ePgAAANg3WAwYMMBzfeDAgYEsj6NoqNBWC23lIVgAAADAUWMsypQpY9afKOj48ePmPniHcRYAAABwZLBwuVxF7tdxAlFRUf4okyPHWaSlpVldFAAAACDw082+/PLL5lLHV7z55pv5FsPLycmR5cuXmzUu4B1aLAAAAOCoYKGDtt0tFrNmzcrX7UlbKhITE81+eIdgAQAAAEcFi927d5vLm2++Wf773/96TojhG4IFAAAAHLny9pIlSwJTEoePsdBZoQAAAADHDN6+88475YUXXii0f8qUKXLXXXf5q1yOa7Fg8DYAAAAcFSx0kHaPHj0K7e/evbu5D96hKxQAAAAcGSxOnz5d5LSykZGRkpGR4a9yOQbBAgAAAI4MFs2bN5ePPvqo0P45c+ZIkyZN/FUux2CMBQAAABw5eHvs2LHSp08f2bVrl9xyyy1m3+LFi+XDDz+UTz75JBBltDXGWAAAAMCRwaJnz54yb948mThxosydO1fKlSsnLVq0kEWLFknHjh0DU0oboysUAAAAHBks1G233WY2+C9Y6NiV7OxsM1YFAAAACDVej7FwT4365ptvyjPPPCMnTpww+9avXy8HDx70d/lsr2LFip7rTDkLAAAAx7RYbNq0STp37mxOiPfs2SMPPvigVKlSxazGvW/fPnn33XcDU1KbioiIkNjYWDOjloa0atWqWV0kAAAAIPAtFqNGjZKBAwfKjh07pGzZsp79urYF61iUTFxcnLk8fvx4CZ8BAAAACLFgkZycLH/+858L7a9du7YcOXLEX+VylKpVq5pLggUAAAAcEyyio6OLXAhv+/btdOPxMVikpqaW9CkAAACA0AoWvXr1kueee87MYKTCwsLM2Iqnn35a7rzzzkCU0fZosQAAAIDjgsWLL75opkatXr26nD171qxd0aBBA6lQoYJMmDAhMKW0OcZYAAAAwHGzQulsUElJSbJixQozQ5SGjNatW5uZolAytFgAAADAkQvkqeuvv95s8B1jLAAAAOCIYPHyyy/LQw89ZKaX1esXU758eWnatKm0a9fOX2W0PbpCAQAAwBHB4p///Kf069fPBAu9fjHnzp2TlJQUGTlypEydOtVf5bQ1ukIBAADAEcFi9+7dRV6/EB2D0bdvX4JFMREsAAAA4LhZoYpDx1787//+byCe2tZdoXQdC5fLZXVxAAAAgNIJFosXL5bbb79d6tevbza9vmjRIs/95cqVkxEjRpTkqR3dYnH+/Hk5deqU1cUBAAAAAh8sXn31Vbn11lvNuhUaHnSLjY2VHj16yIwZM7x6rkmTJsk111xjnkvXxejdu7ds27ZNnEaDmG7q+PHjVhcHAAAACHywmDhxohnA/eGHH8pjjz1mttmzZ5t9ep83li1bJsOGDZNVq1aZcRm6mnfXrl3lzJkz4jSMswAAAICj1rFIS0szLRYFaSB4+umnvXqu+fPn57v9zjvvmJaLdevWyY033ljkjFO6uWVkZJhLDSS6WcH9ur6+vgaLAwcOyJEjRyz7WxB89QL2QZ0AdQIcJxCK3x3evLbXwaJXr17y2WefyZNPPplv/+eff27GWvgiPT3dXFapUuWCXafGjx9faP/ChQslJiZGrKQtLr5wD9r+7rvvJCcnx0+lgtV8rRewH+oEqBPgOIFQ+u7IzMws9mPDXMWYhijvonjaSvCPf/xDrrvuOmnfvr3Zp12ZfvjhB3n88cdLPBtUbm6uCS3aIrJixYoiH1NUi0VCQoKZTUnHeViV4vTD7tKli0RGRpb4eXR63rlz58qLL74ojz76qF/LiNCtF7AP6gSoE+A4gVD87tDzbZ3BVBsALnW+XewF8vKqXLmy/PLLL2Zzq1Spkrz11lslDhY61mLz5s0XDBUqOjrabAXpG231yZuvZahZs6a5PHHihOV/C/wnGOomggt1AtQJcJxAKH13ePO6Xi+QFwjDhw+Xr776SpYvXy516tQRJ6pRo4a5PHr0qNVFAQAAALzm9RgLN+1+lHdxt5LQXlja7UfHbCxdulTq1asnTqWD1hXBAgAAALafblbHP2iXJQ0T+gu7bnpdWxz0Pm/pc73//vtmulpdy0JnRNLt7Nmz4tQWi5SUFKuLAgAAAASuxUL7/utg7YMHD0q/fv2kcePGZr+Os9BpYnU17h9//NGMvyiumTNnmsubbrop3/63335bBg4cKE5CiwUAAAAcESyee+45iYqKkl27dnl+Xc97n65joZcFB3pfTDEmpHIMWiwAAADgiK5Q8+bNM9PMFgwV7hmNpkyZYsZKwLcWC50r+PTp07yNAAAAsGewOHz4sDRt2vSC9zdr1syMj0DJlC9f3rPIH+MsAAAAYNtgoYO09+zZc9EpaS+0YjaKh3EWAAAAsH2w6Natm/z1r3+VrKysQvfpathjx46VW2+91d/lcxTGWQAAAMARg7fbtm0rDRs2NNPEXnXVVWbw9datW+XVV1814eK9994LbGltjhYLAAAA2D5Y6IrYK1eulKFDh8qYMWM8MzqFhYVJly5d5JVXXpGEhIRAltX2WH0bAAAAjlh5W1fG/vbbb+XkyZOyY8cOs69BgwaMrfBziwWDtwEAAGDrYOGmi+Bde+21/i+Nw9FiAQAAANsP3kbgMcYCAAAAoYpgEUTi4+PN5cGDB60uCgAAAOAVgkUQcQ9+P3DggGdwPAAAABAKCBZB2GKhU/empqZaXRwAAACg2AgWQSQ6OtozgHv//v1WFwcAAAAoNoJFEHeHAgAAAEIFwSJIgwUtFgAAAAglBIsgoyucK4IFAAAAQgnBIsjQFQoAAAChiGARZOgKBQAAgFBEsAgydIUCAABAKCJYBGmLha6+nZuba3VxAAAAgGIhWAThInnh4eGSlZUlKSkpVhcHAAAAKBaCRZCJjIz0tFrs3LnT6uIAAAAAxUKwCEINGzY0lzt27LC6KAAAAECxECyCEMECAAAAoYZgEYQIFgAAAAg1BIsg1KhRI3O5detWq4sCAAAAFAvBIgg1b97cXG7btk3OnTtndXEAAACASyJYBOkieZUqVZLz58/TagEAAICQQLAIQmFhYdKiRQtzfePGjVYXBwAAALgkgkWQatOmjblctWqV1UUBAAAALolgEaSuv/56c7lixQqriwIAAABcEsEiyIPF5s2b5dixY1YXBwAAALgogkWQql69ulx99dXm+hdffGF1cQAAAIDgDRbLly+Xnj17Snx8vBmwPG/ePCuLE3T+53/+x1y+9957VhcFAAAACN5gcebMGWnZsqXMmDHDymIErf79+0tkZKQsW7bMtFromhZr1qyRTz75RPbs2WN18QAAAACPCLFQ9+7dzVZcemKdd8G4jIwMc5mdnW02K7hfNxCvX6tWLfnzn/8sr7zyitxxxx1SpkwZycnJMffp9b/+9a9m09YeBJdA1guEJuoEqBPgOIFQ/O7w5rXDXC6XS4KAnhx/9tln0rt37ws+Zty4cTJ+/PhC+2fPni0xMTFiR/phvvTSS57ZoSpUqCBVqlSRvXv3mtuDBg2SXr16WVxKAAAA2FFmZqb07dtX0tPTJTY21j7BoqgWi4SEBElNTb3kHxrIE/+kpCTp0qWL6bYUKEePHjUrcbvHo0ybNk1Gjx4tUVFRsmnTJrniiisC9toI3nqB0EGdAHUCHCcQit8der4dFxdXrGBhaVcob0VHR5utIH2jrT55C3QZ6tSpk+/2U089ZSra4sWL5bnnnpMPPvggYK+NkguGuongQp0AdQIcJxBK3x3evC7TzYYobbWYMmWKpyvYhg0brC4SAAAAHIxgEcJat24t99xzj7k+ceJEq4sDAAAAB7M0WJw+fdr80u7+tX337t3m+r59+6wsVkh55plnzOXcuXNlx44dVhcHAAAADmVpsFi7dq20atXKbGrUqFHm+rPPPmtlsUJK8+bN5fbbbxcdgz916lSriwMAAACHsjRY3HTTTeaEuOD2zjvvWFmskKOzQ6n//Oc/cujQIauLAwAAAAdijIUNXHfddXL99ddLVlaWTJ8+3eriAAAAwIEIFjZrtZg1a5akpaVZXRwAAAA4DMHCJnr06CHNmjWTU6dOyauvvmp1cQAAAOAwBAsbrWvhbrWYNGmS7N+/3+oiAQAAwEEIFjZy7733SocOHcw0vg8//LAZCA8AAACUBoKFjYSHh8ubb74pUVFR8s0338gTTzwhubm5VhcLAAAADhBhdQHgX40bN5aZM2fK4MGDZdq0abJ06VLp3r271KhRw9zvbsXQrlP6WJ1Rqly5cnwMAAAA8AnBwoYGDRokkZGRMnToUFm/fr3ZLqRChQqmC9WDDz4obdu2NYEDAAAA8BbBwqb69+8v3bp1k48//lg2b94sJ0+e9IQGvdQ1L1avXi0HDx6U119/3WzagqEho1OnTtKiRQspX7681X8GAAAAQgTBwsaqV68uw4cPv+D9Ov5i+fLlZlzG3LlzZevWrfLss8+aTcXGxpqtYsWK5rm0O1ViYqJp2WjXrp3UqVOnFP8aAAAABDOChcMHe990001mmzFjhsybN08+/fRTWbdunRw6dEgyMjLMduDAAdmyZUuhf1+vXj254YYb5MYbbzSXDRo0MM8JAAAA5yFYwNBWiQEDBphNHT9+3GwaLLQb1bFjx+To0aOmVSM5OVk2bdoku3fvNtu7775r/o2O69BWDN3i4uKkatWqUqVKFXOZd9P79DE6vgMAAAD2QLBAkdwh4EI0cKxcuVK+//57051qzZo1cu7cOU/YKI5KlSpJQkKCXH755Z7L+Ph4qVatmtm0+5VexsTE8CkBAAAEOYIFSkTHXujgcN3U+fPnTfcpXfFbu06dOHHC0+qhW97bKSkpkp6eLmlpaWb7+eefL/paGizyBo2irmuLS9myZc3UuXqZ93p0dDRdtAAAAAKMYAH/VKSICNPioFtxnDp1yoSQffv25bs8fPiw6XalmwYQnb0qMzNT9u7da7aS0nBRMHAUvO4OIe7Huq9faLvYY3SsiZb/yJEjZnYt9/4yZcqU+G8AAAAIZgQLWELHVzRp0sRsF6KL+WkAcYeMvIHDfd19Wx939uxZ+f33382m1/OuOq7dtHTTlhKrA9iFgok/buuq6zrWRS8LbhfbT+ABAAC+IlggaOl6G+4pb+vXr+/Vv9VQot2z3CHDHTgK3i543R1ALrTp44r7GH2+7Oxsz2rnSsuk25kzZySYaAuLt2GkqP2l8W90H7OPAQAQfAgWsG0o0RNQ3ayYfUoDxTfffCPdu3c3ZSkqlOS9fbH7vLmtr6vdx9xbwdvuLScnJ195tXXHHbBCgbawXCqkuD//km7auuTrc+Td1OnTp02o1HFD+jdoQGK1ewCAXRAsgFIKOMG0krkGiQuFDn/t9+dz5e3WpjQYaYuQbqFOw4WGDA0yenmprbiPC9RjL/W4vPe7rwdqH8EMAIILwQJwID0hc4/NCAUaJEoSUoJpKxiO3HS/O+jBexcLNL4GmUAHPqWLj+psee7JHUo7GNJiBsCfCBYAgp77JEgHqIcq7ar25ZdfSqdOnTxhqeCm42+K2n+xLdj+TcHHuW/n3e/NPt0uxv1YDZPwngaLQLdiOeE5aT0D/g/BAgBKgZ54uLvEucdcoHi0RcdfQcWbfb4EquI8VlupdEpqXYw079/oy3MWtRVnootLBTgUv1ujL2FFn0NnOfzHP/5RrMcHQ6jy93MyOUdoI1gAAIKanmi4g5mduCd56NGjR0D/tqKCWSBbqez6nBfqzpj3faZbo3/YLSyV8eE5tU4dPXpUQgXBAgAAG7NrMCtt2rpTsGXJ3wFIu0yuWbNGWrZsabqphWIAK87j8k7DXhT34/B/rrzySnnggQckFBAsAAAAvBiPEshWLA2BgW7FspoGi1AIQMHynBUsmDa/pAgWAAAAKNWQpt1+dEPxukyGinCrCwAAAAAg9BEsAAAAAPiMYAEAAADAZwQLAAAAAD4jWAAAAADwGcECAAAAgM8IFgAAAADsESxmzJghiYmJUrZsWWnXrp1ZdRIAAABA6LA8WHz00UcyatQo+dvf/ibr1683y9h369ZNUlJSrC4aAAAAgGKyfMnDadOmyZAhQ+SBBx4wt2fNmiVff/21vPXWWzJ69Oh8jz137pzZ3DIyMjyrEupmBffrWvX6CE7UC1AnwHECfHfADucT3rx2mMvlcolFsrKyJCYmRubOnSu9e/f27B8wYICkpaXJ559/nu/x48aNk/Hjxxd6ntmzZ5vnAQAAAOA/mZmZ0rdvX0lPT5fY2NjgbbFITU2VnJwcqVGjRr79evvXX38t9PgxY8aYblN5WywSEhKka9eul/xDA5nikpKSpEuXLhIZGWlJGRB8qBegToDjBPjugB3OJ9w9hEKiK5Q3oqOjzebmbmw5e/asZW+2fuCa5LQM58+ft6QMCD7UC1AnwHECfHfADucT+tqqOJ2cLA0WcXFxUqZMGTl69Gi+/Xq7Zs2al/z3p06dMpfaagEAAAAgMPS8u2LFisEbLKKioqRNmzayePFizxiL3Nxcc3v48OGX/Pfx8fGyf/9+qVChgoSFhYkV3N2xtBxWdcdC8KFegDoBjhPguwN2OJ/QlgoNFXreHfRdoXTMhA7Wbtu2rVx77bUyffp0OXPmjGeWqIsJDw+XOnXqSDDQD5tgAeoFOFaA7w9wTgG7nWdeqqUiaILF3XffLceOHZNnn31Wjhw5IldffbXMnz+/0IBuAAAAAMHL8mChtNtTcbo+AQAAAAhOlq+8Hep0lipdNTzvbFUA9QIcK8D3BzingNPOJyxdIA8AAACAPdBiAQAAAMBnBAsAAAAAPiNYAAAAAPAZwQIAAACAzwgWPpoxY4YkJiZK2bJlpV27drJmzRrfPxUEnUmTJsk111xjVnmvXr26WSl+27Zt+R7z+++/y7Bhw6Rq1apSvnx5ufPOO+Xo0aP5HrNv3z657bbbJCYmxjzPk08+KefPny/lvwaBMHnyZAkLC5O//OUvnn3UCWc6ePCg3HfffeZYUK5cOWnevLmsXbvWc7/OmaJrN9WqVcvc37lzZ9mxY0e+5zhx4oT069fPLIhVqVIlGTx4sJw+fdqCvwa+ysnJkbFjx0q9evXM512/fn15/vnnTT1wo07Y2/Lly6Vnz55m5Wr9npg3b16++/31+W/atEluuOEGc06qq3VPmTJFSp3OCoWSmTNnjisqKsr11ltvubZs2eIaMmSIq1KlSq6jR4/yltpMt27dXG+//bZr8+bNrg0bNrh69Ojhuvzyy12nT5/2PObhhx92JSQkuBYvXuxau3at6w9/+IOrQ4cOnvvPnz/vatasmatz586un376yfXNN9+44uLiXGPGjLHor4K/rFmzxpWYmOhq0aKFa8SIEZ791AnnOXHihKtu3bqugQMHulavXu367bffXAsWLHDt3LnT85jJkye7Klas6Jo3b55r48aNrl69ernq1avnOnv2rOcxt956q6tly5auVatWub7//ntXgwYNXPfee69FfxV8MWHCBFfVqlVdX331lWv37t2uTz75xFW+fHnXSy+95HkMdcLevvnmG9df//pX13//+19Nk67PPvss3/3++PzT09NdNWrUcPXr18+cq3z44YeucuXKuV577bVS/VsJFj649tprXcOGDfPczsnJccXHx7smTZrkj88GQSwlJcUcHJYtW2Zup6WluSIjI80XhtvWrVvNY1auXOk5sISHh7uOHDnieczMmTNdsbGxrnPnzlnwV8AfTp065WrYsKErKSnJ1bFjR0+woE4409NPP+26/vrrL3h/bm6uq2bNmq6pU6d69mldiY6ONicC6pdffjHHjuTkZM9jvv32W1dYWJjr4MGDAf4L4G+33Xaba9CgQfn29enTx5wAKuqEs0iBYOGvz//VV191Va5cOd/5hB6PGjVq5CpNdIUqoaysLFm3bp1prnILDw83t1euXOmvBiUEqfT0dHNZpUoVc6l1ITs7O199uOqqq+Tyyy/31Ae91C4RNWrU8DymW7dukpGRIVu2bCn1vwH+od3ftHtb3s9eUSec6YsvvpC2bdvKXXfdZbo7tmrVSt544w3P/bt375YjR47kqy8VK1Y0XWnzHiu0q4M+j5s+Xr9jVq9eXcp/EXzVoUMHWbx4sWzfvt3c3rhxo6xYsUK6d+9ublMnnG23n44J+pgbb7xRoqKi8p1jaLftkydPltrfE1Fqr2Qzqamppt9k3pNEpbd//fVXy8qFwMvNzTX96K+77jpp1qyZ2acHBf2fWf/HL1gf9D73Y4qqL+77EHrmzJkj69evl+Tk5EL3USec6bfffpOZM2fKqFGj5JlnnjF147HHHjPHhwEDBnj+Xy/qWJD3WKGhJK+IiAjzQwbHitAzevRo8wOS/thUpkwZc+4wYcIE019eUSec7Yifjgl6qeN4Cj6H+77KlSsH9O/wlKtUXgWw2S/UmzdvNr84wbn2798vI0aMkKSkJDNQDnD/8KC/Kk6cONHc1hYLPV7MmjXLBAs4z8cffywffPCBzJ49W5o2bSobNmwwP07pQF7qBOyGrlAlFBcXZ355KDjrj96uWbOmPz4bBKHhw4fLV199JUuWLJE6dep49utnrt3j0tLSLlgf9LKo+uK+D6FFuzqlpKRI69atzS9Hui1btkxefvllc11/KaJOOI/O6tKkSZN8+xo3bmxmhMv7//rFvjv0UutWXjp7nM4Kw7Ei9Ojsf9pqcc8995jusP3795eRI0ea2QYVdcLZavrpmBAs5xgEixLSZu02bdqYfpN5f6nS2+3bt/fX54MgoeOtNFR89tln8t133xVqbtS6EBkZma8+aL9GPZlw1we9/Pnnn/MdHPTXbp06ruCJCIJfp06dzOepvz66N/2lWrs3uK9TJ5xHu0gWnIpa+9bXrVvXXNdjh37J5z1WaDcZ7Sed91ihP1JoeHXT445+x2i/a4SWzMxM0xc+L/1hUj9PRZ1wtnp+OiboY3RaWx3vmfcco1GjRqXWDcoo1aHiNpxuVkftv/POO2bE/kMPPWSmm8076w/s4ZFHHjFTwS1dutR1+PBhz5aZmZlvalGdgva7774z0822b9/ebAWnm+3atauZsnb+/PmuatWqMd2sjeSdFUpRJ5w59XBERISZYnTHjh2uDz74wBUTE+N6//33800tqd8Vn3/+uWvTpk2uO+64o8ipJVu1amWmrF2xYoWZeYzpZkPTgAEDXLVr1/ZMN6tTjupU40899ZTnMdQJ+88e+NNPP5lNT72nTZtmru/du9dvn7/OJKXTzfbv399MN6vnqHrsYbrZEPOvf/3LnEzqehY6/azOLwz70QNBUZuubeGmB4ChQ4ea6d70f+Y//vGPJnzktWfPHlf37t3N3NL6xfL444+7srOzLfiLUBrBgjrhTF9++aX5EUF/eLrqqqtcr7/+er77dXrJsWPHmpMAfUynTp1c27Zty/eY48ePm5MGXe9Ap6R+4IEHzMkJQk9GRoY5Lui5QtmyZV1XXHGFWdMg77Sg1Al7W7JkSZHnEBo6/fn56xoYOt21PoeGWQ0spS1M/1N67SMAAAAA7IgxFgAAAAB8RrAAAAAA4DOCBQAAAACfESwAAAAA+IxgAQAAAMBnBAsAAAAAPiNYAAAAAPAZwQIAAACAzwgWAICgk5iYKNOnT7e6GAAALxAsAMDhBg4cKL179zbXb7rpJvnLX/5Saq/9zjvvSKVKlQrtT05OloceeqjUygEA8F2EH54DAIB8srKyJCoqqsTvSrVq1XhHASDE0GIBAPC0XCxbtkxeeuklCQsLM9uePXvMfZs3b5bu3btL+fLlpUaNGtK/f39JTU31vHPa0jF8+HDT2hEXFyfdunUz+6dNmybNmzeXyy67TBISEmTo0KFy+vRpc9/SpUvlgQcekPT0dM/rjRs3rsiuUPv27ZM77rjDvH5sbKz86U9/kqNHj3ru13939dVXy3vvvWf+bcWKFeWee+6RU6dO8ekCQCkhWAAADA0U7du3lyFDhsjhw4fNpmEgLS1NbrnlFmnVqpWsXbtW5s+fb07q9eQ+r//85z+mleKHH36QWbNm/d+XTHi4vPzyy7JlyxZz/3fffSdPPfWUua9Dhw4mPGhQcL/eE088UejTyM3NNaHixIkTJvgkJSXJb7/9JnfffXe+x+3atUvmzZsnX331ldn0sZMnT+bTBYBSQlcoAIChv/JrMIiJiZGaNWt63pVXXnnFhIqJEyd69r311lsmdGzfvl2uvPJKs69hw4YyZcqUfO9m3vEa2pLw97//XR5++GF59dVXzWvpa2pLRd7XK2jx4sXy888/y+7du81rqnfffVeaNm1qxmJcc801ngCiYzYqVKhgbmuriv7bCRMm8AkDQCmgxQIAcFEbN26UJUuWmG5I7u2qq67ytBK4tWnTptC/XbRokXTq1Elq165tTvj1ZP/48eOSmZlZ7Hd969atJlC4Q4Vq0qSJGfSt9+UNLu5QoWrVqiUpKSl8ugBQSmixAABclI6J6Nmzp7zwwguF7tOTdzcdR5GXjs+4/fbb5ZFHHjGtBlWqVJEVK1bI4MGDzeBubRnxp8jIyHy3tSVEWzEAAKWDYAEA8NDuSTk5OfnekdatW8unn35qWgQiIor/tbFu3TpzYv/iiy+asRbq448/vuTrFdS4cWPZv3+/2dytFr/88osZ+6EtFwCA4EBXKACAh4aH1atXm9YGnfVJg8GwYcPMwOl7773XjGnQ7k8LFiwwMzpdLBQ0aNBAsrOz5V//+pcZbK0zNrkHded9PW0R0bEQ+npFdZHq3LmzmVmqX79+sn79elmzZo3cf//90rFjR2nbti2fHgAECYIFAMBDZ2UqU6aMaQnQtSR0mtf4+Hgz05OGiK5du5qTfB2UrWMc3C0RRWnZsqWZbla7UDVr1kw++OADmTRpUr7H6MxQOphbZ3jS1ys4+Nvdpenzzz+XypUry4033miCxhVXXCEfffQRnxwABJEwl8vlsroQAAAAAEIbLRYAAAAAfEawAAAAAOAzggUAAAAAnxEsAAAAAPiMYAEAAADAZwQLAAAAAD4jWAAAAADwGcECAAAAgM8IFgAAAAB8RrAAAAAA4DOCBQAAAADx1f8Duvs2zVPc+ycAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss with Accelerated Gradient: 0.2767001688480377\n"
     ]
    }
   ],
   "source": [
    "new_network = BaseNetwork(nn.SELU(), X_train.shape[1],y_train.shape[1],[32,16,8])\n",
    "\n",
    "accelerated_optimizer = AcceleratedGradient(new_network, X_train, y_train, learning_rate=0.001)\n",
    "accelerated_optimizer.run(max_iters=1000)\n",
    "accelerated_optimizer.plot_loss()\n",
    "print(\"Final loss with Accelerated Gradient:\", accelerated_optimizer.obj_history[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
