{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: 12/12/2025\n",
    "\n",
    "## CID: 02232170\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Approximation theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "$$ f(x) = A_2\\sigma(A_1x + b_1)$$\n",
    "Noting some important details: $x$ is of size d and non-negative ($ \\in [0,1]^d$) and the width of each layer is required to be $d+3$.  We want $w$ ReLU layers which is the same number of rows as $A_1$ and columns as $A_2$. We will rename A1 to $A^{(1)}$  A2 to $A^{(2)}$ and b1 as $b^{(1)}$ to denote the layer they belong to and reference their rows and columns easily.\n",
    "$$\n",
    "   [\\sigma(A^{(1)}x + b^{(1)})]_i = \\sigma(\\sum_{j = 1}^d A_{i, j}^{(1)}x + b_i^{(1)}) \\tag{ ith element after activation}\n",
    "$$\n",
    "We can rewrite f as: $\\sum_{i = 1}^w A_i^{(2)}[\\sigma(A^{(1)}x + b^{(1)})]_i$.\n",
    "\n",
    "<br> The new network we will construct will separate each of the rows of $A^{(1)}$ into w hidden layers, and use $ReLU$ as the activation function. what we need to do <br>\n",
    "1. Preserve x across all layers(of width d) so that we can apply each row of $A^{(1)}$ independently <br>\n",
    "2. compute $sigma(A^{(1)}x + b^{(1)})$ across w hidden and ReLU layers (of width 1) <br>\n",
    "3. store the sum of positive and negative values from previous layers (of width 1 each). We do this because $ReLU$ will be applied on our new layers and only outputs non-negative values, so we need to keep track of positive and negative contributions separately. Note that $x \\in [0,1]^d$ so we dont need to worry about $ReLU$ erasing any values from x. <br>\n",
    "4. The final layer will subtract the negative sum from the positive sum. <br>\n",
    "Dimensionally, we have d+3 nodes per layer, which meets the requirement. <br>\n",
    "\n",
    "Let's define the layers: <br>\n",
    "- $W^{(1)}$ : First layer weights of size (d+3) x d \n",
    "$$\n",
    "   W^{(1)} = \\begin{bmatrix}\n",
    "   I \\\\\n",
    "   A_1^{(1)} \\\\\n",
    "    \\bold{0} \\\\\n",
    "    \\bold{0} \n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "Where I is the identity matrix of size d x d,  $\\bold{0}$ are zero vectors of size 1 x d, $A_1^{(1)}$ is the first row of $A^{(1)}$. <br> We will denote our new bias vector as $c^{(i)}$ for layer i. \n",
    "$$\n",
    "c^{(1)} = \\begin{bmatrix}\n",
    "   \\bold{0} \\\\\n",
    "   b_1^{(1)} \\\\\n",
    "    0 \\\\\n",
    "    0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Thus post-activation we have the following calculations:\n",
    "$$\n",
    "\\sigma(W^{(1)}x + c^{(1)}) = \\begin{bmatrix}\n",
    "   x \\\\\n",
    "   \\sigma(A_1^{(1)}x + b_1^{(1)}) \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- For layer 2 to w we get the following input, Weight and bias: <br>\n",
    "$$\n",
    "\n",
    "    W^{(i)} = \\begin{bmatrix}\n",
    "    I & 0 & 0 & 0\\\\\n",
    "    A_i^{(1)} & 0 & 0 & 0 \\\\\n",
    "    \\bold{0} & \\sigma(A_{i-1}^{(2)}) & 1 & 0 \\\\\n",
    "    \\bold{0} & \\sigma(-A_{i-1}^{(2)}) & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "x' = \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of positive contributions from previous layers} \\\\\n",
    "    \\text{Sum of negative contributions from previous layers}\n",
    "    \\end{bmatrix}\n",
    "    \\;\n",
    "c^{(i)} = \\begin{bmatrix}\n",
    "    \\bold{0} \\\\\n",
    "    b_i^{(1)} \\\\\n",
    "    0 \\\\\n",
    "    0 \n",
    "    \\end{bmatrix}\n",
    "$$ \n",
    "Note that in all our $c^{(i)}$ vectors, $\\bold{0}$ is a zero vector of size d x 1. <br>\n",
    "Explanation of the weight matrix: <br>\n",
    "- First d rows preserve x  due to Identity matrix<br>\n",
    "- (d+1)th row computes $A_i^{(1)}x + b_i^{(1)}$ <br>\n",
    "- (d+2)th row adds the positive contribution from this layer(if any) to the sum of positive contributions from previous layers <br>\n",
    "- (d+3)th row adds the negative contribution from this layer(if any) to the sum of negative contributions from previous layers <br> \n",
    "I have used $\\sigma(A_{i-1}^{(2)})$ to denote whether the output from the previous layer was positive or negative, which determines whether we add to the positive or negative sum. But, if we were to construct it in reality, we would put a zero on row d+2 if $A_{i-1}^{(2)}$ is negative itself otherwise. Likewise for row d+3. This makes it so that the negative and positive contributions are stored as positive values and 0 is added to the layers. We have $I_2$ in the bottom right of W^(i) so that we sum the previously gained contributions to the new contribution. <br><br>\n",
    "\n",
    "To make it more clear we will write the output of 1 layer depending on the sign of $A_{i-1}^{(2)}$: <br>\n",
    "\n",
    "If $A_{i-1}^{(2)}$ is positive: <br>\n",
    "$$\n",
    "    \\implies \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of positive contributions from previous layers} + \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of negative contributions from previous layers}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "If $A_{i-1}^{(2)}$ is negative: <br>\n",
    "$$\n",
    "    \\implies \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\text{Sum of positive contributions from previous layers} \\\\\n",
    "    \\text{Sum of negative contributions from previous layers} + \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)})\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Finally for the output layer we have: <br>\n",
    "$$\n",
    "    W^{output} = \\begin{bmatrix} 0_{1 \\times d} & A^{(2)}_w & -1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "such that when applying to the output of the wth hidden layer we get the calculation:\n",
    "$$\n",
    "\\begin{bmatrix} 0_{1 \\times d} & A^{(2)}_w & -1 & 1 \\end{bmatrix} \\times\n",
    "\\begin{bmatrix} x \\\\ \\sigma(A_{w}^{(1)}x + b_{w}^{(1)}) \\\\ \n",
    "\\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}>0} \\\\\n",
    "\\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}<0}\n",
    "\\end{bmatrix} \\\\\n",
    "\n",
    "\n",
    " = A^{(2)}_w \\sigma(A_{w}^{(1)}x + b_{w}^{(1)}) + \n",
    " \\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}>0} \n",
    " - \\sum_{j=1}^{w-1} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})\\mathbb{1}_{A_{j}^{(2)}<0} \n",
    " = A^{(2)}_w \\sigma(A_{w}^{(1)}x + b_{w}^{(1)}) + \n",
    " \\sum_{j=1}^{w-1} A_{j}^{(2)} \\sigma(A_{j}^{(1)}x + b_{j}^{(1)})  \\\\\n",
    " = \\sum_{i = 1}^w A_i^{(2)}[\\sigma(A^{(1)}x + b^{(1)})]_i = f(x)\n",
    "$$\n",
    "Therefore, we define $N(x)$ as the following:\n",
    "$$\n",
    "N(x) = W^{output} \\sigma(W^{(w)} \\sigma(W^{(w-1)} ... \\sigma(W^{(1)}x + c^{(1)}) ... + c^{(w-1)}) + c^{(w)}) \n",
    "$$ \n",
    "and have proved that $$ N(x) = f(x) \\; \\forall x \\in [0,1]^d$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Enter your answer here\n",
    "In our previous answer, we assumed that $ x \\in [0,1]^d$. However, if we relax this assumption and allow $x$ to be in $[-1,1]^d$. One way to handle this is to introduce d extra rows in W and c, $W \\in R^{(2d + 3)\\times d}$ and $c \\in R^{2d + 3}$. We will use it to store $ -x$ alongside x in the first layer.\n",
    "Our new Matrices and vectors will look like this:\n",
    "$$\n",
    "    W^{(1)} = \\begin{bmatrix}\n",
    "    I \\\\\n",
    "    -I \\\\\n",
    "    A_1^{(1)} \\\\\n",
    "     \\bold{0} \\\\\n",
    "     \\bold{0} \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "where, I is the identity matrix of size d x d,  $\\bold{0}$ are zero vectors of size 1 x d, $A_1^{(1)}$ is the first row of $A^{(1)}$. <br> We will denote our new bias vector as $c^{(i)}$ for layer i.\n",
    "$$\n",
    "c^{(1)} = \\begin{bmatrix}\n",
    "    \\bold{0} \\\\\n",
    "    \\bold{0} \\\\\n",
    "    b_1^{(1)} \\\\\n",
    "     0 \\\\\n",
    "     0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $\\bold{0}$ is a zero vector of size d x 1. <br>\n",
    "For $i >1$ we have:\n",
    "$$\n",
    "    W^{(i)} = \\begin{bmatrix}\n",
    "    I & \\bold{0} & 0 & 0 & 0\\\\\n",
    "    \\bold{0} & -I & 0 & 0 & 0\\\\\n",
    "    A_i^{(1)} & -A_i^{(1)} & 0 & 0 & 0 \\\\\n",
    "    \\bold{0} & \\bold{0} & \\sigma(A_{i-1}^{(2)}) & 1 & 0 \\\\\n",
    "    \\bold{0} & \\bold{0} & \\sigma(-A_{i-1}^{(2)}) & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally for the output layer we have: <br>\n",
    "$$ \n",
    "    W^{output} = \\begin{bmatrix} 0_{1 \\times d} & 0_{1 \\times d} & A^{(2)}_w & 1 & -1 \\end{bmatrix}\n",
    "$$\n",
    "Example Output of hidden layer:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\sigma(x) \\\\\n",
    "    \\sigma(-x) \\\\\n",
    "    \\sigma(A_{i-1}^{(1)}x + b_{i-1}^{(1)}) \\\\\n",
    "    \\sum \\sigma(A^{(1)}_j x + b_j^{(1)}) * 1_{A_j^{(2)}>0} \\\\\n",
    "    \\sum \\sigma(A^{(1)}_j x + b_j^{(1)}) * 1_{A_j^{(2)}<0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "We can begin by using the hint.\n",
    "$$\n",
    "h(x) = \\lim_{\\delta \\to 0} \\frac{f((w+\\delta)x) - f(wx)}{\\delta}\n",
    "$$\n",
    "Let's define $g_\\delta(x) = \\frac{f((w+\\delta)x) - f(wx)}{\\delta}$ which is $\\in \\mathcal{F}_{\\sigma, 1} $ since $\\mathcal{F}_{\\sigma, 1}$ is closed under addition and scalar multiplication from the question.\n",
    "Using the Taylor expansion of $f$ about $wx$ we have:\n",
    "$$\n",
    "f((w+\\delta)x) = f(wx) + \\delta x f'(wx) + \\frac{\\delta^2 x^2}{2} f''(z)\n",
    "$$\n",
    "for some $z$ between $wx$ and $(w+\\delta)x$. Thus we have:\n",
    "$$\n",
    "g_\\delta(x) = \\frac{f((w+\\delta)x) - f(wx)}{\\delta} \\\\ =  \\frac{f(wx) + \\delta x f'(wx) + \\frac{\\delta^2 x^2}{2} f''(z) - f(wx)}{\\delta} \\\\ =  x f'(wx) + \\frac{\\delta x^2}{2} f''(z) = h(sx) + \\frac{\\delta x^2}{2} f''(z)\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "|| h - g_\\delta ||_u = \\sup_{x \\in [0,1]} |h(x) - g_\\delta(x)| \\\\\n",
    "= \\sup_{x \\in [0,1]} | \\frac{\\delta x^2}{2} f''(z) | \\leq \\frac{\\delta}{2} \\sup_{x \\in [0,1]} |z^2 f''(z)| \\\\\n",
    "\\leq \\frac{\\delta}{2} \\sup_{x \\in [0,1]} | f''(z)|\n",
    "$$\n",
    "I used that $x \\in [0,1]$. We know that $z \\in [wx, (w + \\delta)x]$ and consider 2 cases of w: w >= 0 means that $z in [0, w + \\delta]$ else if w < 0 then $z \\in [w, 0]$ which means $|z| \\leq |w| + \\delta$. \n",
    "\n",
    "let's work out the second derivative of f and plug it into our current bound:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^m a_i \\sigma(W_i x + b_i) \\\\\n",
    "f'(x) = \\sum_{i=1}^m a_i W_i \\sigma'(W_i x + b_i) \\\\\n",
    "f''(x) = \\sum_{i=1}^m a_i W_i^2 \\sigma''(W_i x + b_i) \\\\\n",
    "\\implies || h - g_\\delta ||_u  \\leq \\frac{\\delta}{2} \\sup_{|z| \\leq |w| + \\delta} | f''(z)| = \\frac{\\delta}{2} \\sup_{|z| \\leq |w| + \\delta} | \\sum_{i=1}^m a_i W_i^2 \\sigma''(W_i z + b_i) | \\\\\n",
    "\\leq \\frac{\\delta}{2}  \\sum_{i=1}^m |a_i| *W_i^2 * |\\sup_{|z| \\leq |w| + \\delta} \\sigma''(W_i z + b_i) |\n",
    "$$\n",
    "Where we use the fact that the supremum of a sum is less than the sum of supremums. Now we can use the fact that \n",
    "$ \\sigma \\in C^\\infty \\implies \\sup_{|z| \\leq |w| + \\delta} \\sigma''(W_i z + b_i) < M_i $ with which we define $M = \\sum_{i = 1}^m W_i^2 |a_i| M_i$ <br>\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    "    || h - g_\\delta ||_u  \\leq \\frac{\\delta}{2}  \\sum_{i=1}^m |a_i| *W_i^2 * M_i \\leq \\frac{\\delta}{2} M\n",
    "$$\n",
    "\n",
    "To conclude, given any $\\epsilon > 0, \\exists g_\\delta \\in \\mathcal{F}_{\\sigma, 1} $ such that $|| h - g_\\delta ||_u < \\epsilon$ by choosing $\\delta = \\frac{2 \\epsilon}{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Suppose we have $w, b, \\epsilon$ as defined in the question, then the base case(n=1) is satisfied from question 1 since we have shown that $h_{1,w,b}(x) = \\frac{d}{dw}  f(wx) $ has a g as previously shown. <br>\n",
    "\n",
    "Assume that for some k>=1, $\\exists g_{k,w,b,\\epsilon} \\in \\mathcal{F}_{\\sigma, 1} $ such that $ \\| h_{k, w, b} - g_{k, w, b, \\epsilon} \\|_u < \\epsilon $ <br>\n",
    "\n",
    "Now, lets consider the case for k + 1:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "h_{k+1, w, b}(x) &= \\frac{d}{dw} (x^k \\sigma^{(k)}(wx + b)) \\\\\n",
    "&= \\lim_{\\delta \\rightarrow 0} \\frac{x^k \\sigma^{(k)}((w + \\delta)x + b) - x^k \\sigma^{(k)}(wx + b)}{\\delta} \\\\\n",
    "&= \\lim_{\\delta \\rightarrow 0} \\frac{1}{\\delta} ( h_{k, w+\\delta,b}(x) - h_{k, w,b}(x) )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "I claim that we can choose $g_{k+1, w, b, \\epsilon} = \\frac{1}{\\delta}( g_{k, w + \\delta, b, \\frac{\\epsilon\\delta}{4}} - g_{k, w, b, \\frac{\\epsilon\\delta}{4}} ) $. From our inductive hypothesis we have, we chose the functions above such that \n",
    "$\\| h_{k, w + \\delta, b} - g_{k, w + \\delta, b, \\frac{\\epsilon\\delta}{4}} \\|_u < \\frac{\\epsilon\\delta}{4}$ and $\\| h_{k, w, b} - g_{k, w, b, \\epsilon} \\|_u < \\frac{\\epsilon\\delta}{4}$. For some $\\delta > 0$ to be determined later. Note that $g_{k+1, w, b, \\epsilon} \\in \\mathcal{F}_{\\sigma, 1} $ since $\\mathcal{F}_{\\sigma, 1} $ is closed under addition and scalar multiplication. \n",
    "\n",
    "Regarding the bound, we have the following proof:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\| h_{k+1, w, b} - g_{k+1, w, b, \\epsilon} \\|_u &=  || h_{k+1,w,b} - \\frac{1}{\\delta}( g_{k, w + \\delta, b, \\frac{\\epsilon\\delta}{4}} - g_{k, w, b, \\frac{\\epsilon\\delta}{4}} ) || \\\\\n",
    "&= || h_{k +1, w,b} - \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) + \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) -  \\frac{1}{\\delta}( g_{k, w + \\delta, b, \\frac{\\epsilon\\delta}{4}} - g_{k, w, b, \\frac{\\epsilon\\delta}{4}}) ||_u \\\\\n",
    "&\\leq || H_{k+1,w,b} - \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) ||_u \n",
    "+ \\frac{1}{\\delta} || h_{k,w+\\delta,b} - g_{k,w+\\delta,b,\\frac{\\epsilon\\delta}{4}} ||_u\n",
    "+ \\frac{1}{\\delta} || h_{k,w,b} - g_{k,w,b,\\frac{\\epsilon\\delta}{4}} ||_u \\\\\n",
    "&< || h_{k+1,w,b} - \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) ||_u + \\frac{1}{\\delta}\\frac{\\epsilon\\delta}{4} + \\frac{1}{\\delta}\\frac{\\epsilon\\delta}{4} \\\\\n",
    "&= || h_{k+1,w,b} - \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) ||_u + \\frac{\\epsilon}{2}\n",
    "\n",
    "\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "In line 3, I used the triangle inequality. in line 4, I used the bound that we found in part 1 of this question. To bound the first term, we can use the Taylor series expansion again:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{1}{\\delta} (h_{k, w+\\delta,b} - h_{k, w,b})  &= \\frac{1}{\\delta} ( x^k \\sigma^{(k)}((w + \\delta)x + b) - x^k \\sigma^{(k)}(wx + b) ) \n",
    "\\end{align*}\n",
    "$$\n",
    "Expanding $\\sigma^{(k)}((w + \\delta)x + b)$ using Taylor series:\n",
    "$$\n",
    "\\sigma^{(k)}((w + \\delta)x + b) = \\sigma^{(k)}(wx + b) + \\delta x \\sigma^{(k+1)}(wx + b) + \\frac{\\delta^2 x^2}{2} \\sigma^{(k+2)}(wz + b)\n",
    "$$\n",
    "Substituting this back we have:\n",
    "$$\n",
    "\\frac{x^k}{\\delta} (\\sigma^{(k)}(wx + b) + {\\sigma^{(k + 1)}(wx + b) \\delta x} + {\\sigma^{(k + 2)}(wz + b) \\frac{(\\delta x)^2}{2}}- \\sigma^{(k)}(wx + b)) \\\\\n",
    "= x^{k+1} \\sigma^{(k + 1)}(wx + b) + \\frac{\\delta x^{k+2}}{2} \\sigma^{(k + 2)}(wz + b) = h_{k+1,w,b}(x) + \\frac{\\delta x^{k+2}}{2} \\sigma^{(k + 2)}(wz + b)\n",
    "$$\n",
    "And, substituting this back into the bound:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&|| h_{k+1,w,b} - \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) ||_u \\\\\n",
    "&= || h_{k+1,w,b} - h_{k+1,w,b} - \\frac{\\delta x^{k+2}}{2} \\sigma^{(k + 2)}(wz + b) ||_u   \\\\ &= || - \\frac{\\delta x^{k+2}}{2} \\sigma^{(k + 2)}(wz + b) ||_u  \\\\ \n",
    "&= \\frac{\\delta}{2} \\sup_{x \\in [0,1]} | x^{k+2} \\sigma^{(k + 2)}(wz + b) | \\leq \\frac{\\delta}{2} \\sup_{x \\in [0,1]} | \\sigma^{(k + 2)}(wz + b) | \\\\\n",
    "&\\leq \\frac{\\delta}{2} M_{k+2} < \\frac{\\epsilon}{2} \\quad \\text{lets choose } \\delta < min (\\frac{\\epsilon}{M_{k+2}}, 1)\n",
    "\\end{align*}\n",
    "$$\n",
    "I used that $x \\in [0,1]$ in line 4, and the fact that $\\sigma \\in C^\\infty$ in line 5 to define $M_{k+2}$ from the definition in the question. <br>\n",
    "Finally lets put it back into the main bound\n",
    "$$\n",
    "\\| h_{k+1, w, b} - g_{k+1, w, b, \\epsilon} \\|_u \\leq || h_{k+1,w,b} - \\frac{1}{\\delta}(h_{k,w + \\delta, b} - h_{k,w,b}) ||_u + \\frac{\\epsilon}{2} < \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon\n",
    "$$\n",
    "Thus by induction, we have shown that $\\forall n \\in \\mathbb{N}, \\exists g_{n,w,b,\\epsilon} \\in \\mathcal{F}_{\\sigma, 1} $ such that $ \\| h_{n, w, b} - g_{n, w, b, \\epsilon} \\|_u < \\epsilon $.>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "From the previous part, we showed that given, $h_{n,w,b}(x) = x^n \\sigma^{(n)}(wx + b), \\forall \\epsilon >0, \\exists g_{n,w,b,\\epsilon}, ||h_{n,w,b} -  g_{n,w,b,\\epsilon}||_u < \\epsilon$. If we set w=0, then $h_{n,0,b}(x) = x^n \\sigma^{(n)}(b) = p_n(x) \\sigma^{(n)}(b)$.\n",
    "We can define a different function $$g'_{n,0,b,\\epsilon} = \\frac{g_{n,w,b,\\epsilon}}{\\sigma^{(n)}(b)}$$. we know that g' is in $\\mathcal{F}_{\\sigma,1} $ because $\\mathcal{F}_{\\sigma,1} $ is closed under scalar multiplication.\n",
    "Now we can compute the bound with $\\epsilon' = \\epsilon |\\sigma^{(n)}(b)|$:\n",
    "$$\n",
    "|| p_n - g'_{n,0,b,\\epsilon'}||_u = \\frac{1}{|\\sigma^{(n)}(b)|} || h_{n,0,b} - g_{n,0,b,\\epsilon'} ||_u < \\frac{\\epsilon'}{|\\sigma^{(n)}(b)|} = \\epsilon\n",
    "$$\n",
    "where we used that $h_n,0,b = p_n(x) \\sigma^{(n)}(b) $ and $ ||h_{n,0,b} - g_{n,0,b,\\epsilon'}||_u < \\epsilon'$ from the previous part.\n",
    "\n",
    "For the proof to be complete we need it to be that for all n, $\\sigma^{(n)}(b) \\neq 0$ for some b. Now we know that  $\\sigma$ is not a polynomial, which means that there cannot be a derivative of $sigma$ that is 0 everywhere. To see this, suppose there is a k such that $\\sigma^{(k)}(x) = 0$, then intergrating k times would mean that $\\sigma$ is a polynomial of degree at most k-1, which is a contradiction. Thus for all n, there exists some b such that $\\sigma^{(n)}(b) \\neq 0$. Concluding the proof, we are allowed to divide by $\\sigma^{(n)}(b)$ in the earlier steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "Our previous part used that x is in [0,1], we would like to extends this to [-r,r] for any r > 0. Suppose we define $y = 2rx - r$ then $x \\in [0,1] \\iff y \\in [-r,r]$.\n",
    "Then p_n(y) can be written as:\n",
    "$$\n",
    "(2rx -r)^n = \\sum_{k=0}^n \\binom{n}{k} (2rx)^k (-r)^{n-k} = \\sum_{k=0}^n \\binom{n}{k} (2r)^k (-r)^{n-k} x^k \\\\ = \\sum_{k=0}^n \\binom{n}{k} (2r)^k (-r)^{n-k} p_k(x) =  \\sum_{k=0}^n c_k p_k(x)\n",
    "$$\n",
    "where $c_k = \\binom{n}{k} (2r)^k (-r)^{n-k}$. From part 3, we know that p_k(x) has an approximation in $\\mathcal{F}_{\\sigma,1}$ and we can call it $a_{k,\\delta}$. Let's define a function $$A_n(y) = \\sum_{k=0}^n c_k a_{k,\\delta}(\\frac{y +r}{2r}) = \\sum_{k=0}^n c_k a_{k,\\delta}(x)$$\n",
    "That uses the approximations of $p_k(x)$ and the coefficients from $p_n(y). Note that $A_n(y) \\in \\mathcal{F}_{\\sigma,1}$ since it is a linear combination of functions in $\\mathcal{F}_{\\sigma,1}$. Let's fix $\\epsilon>0 $ and choose $\\delta= \\frac{\\epsilon}{\\sum_{k=0}^n |c_k|}$ then we have the following bound:\n",
    "$$\n",
    "|| p_n(y) - A_n(y) ||_u = || \\sum_{k=0}^n c_k p_k(x) - \\sum_{k=0}^n c_k a_{k,\\delta}(x) ||_u = || \\sum_{k=0}^n c_k (p_k(x) - a_{k,\\delta}(x)) ||_u \\\\\n",
    "\\leq \\sum_{k=0}^n |c_k| || p_k(x) - a_{k,\\delta}(x) ||_u < \\sum_{k=0}^n |c_k| \\delta = \\epsilon\n",
    "$$\n",
    "Where I used the triangle inequality in line 3. Thus we have shown that we can find an approximation for $p_n(y)$ on the interval [-r,r] using functions from $\\mathcal{F}_{\\sigma,1}$.\n",
    "<br>\n",
    "Coming back to the question, we want to find an approximation for exp(x) on the interval [-r,r] and can do so using the Taylor series:\n",
    "$$\n",
    "exp(x) = \\sum_{k=0}^\\infty \\frac{x^k}{k!} = \\sum_{k=0}^n \\frac{x^k}{k!} + R_n(x) \\\\\n",
    "R_n(x) = \\frac{exp(z) x^{n+1}}{(n+1)!} \\text{ for some z between 0 and x}\n",
    "$$\n",
    "\n",
    "So we know that $p_k(x) = x^k$ has an approximation in $\\mathcal{F}_{\\sigma,1}$ and will call it $a_{k,\\delta}$ and set $\\delta = \\frac{\\epsilon}{2 \\sum_{k=0}^n \\frac{1}{k!}}$. Let's define a new function E and show that it approximates exp(x).\n",
    "$$\n",
    "E_n(x) = \\sum_{k=0}^n \\frac{a_{k,\\delta}(x)}{k!}\n",
    "$$\n",
    "\n",
    "Now lets bound the error:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sup_{|x| \\leq r} | exp(x) - E_n(x) | &= \\sup_{|x| \\leq r} | \\sum_{k=0}^n \\frac{x^k}{k!} + R_n(x) - \\sum_{k=0}^n \\frac{a_{k,\\delta}(x)}{k!} | \\\\\n",
    "&= \\sup_{|x| \\leq r} | R_n(x) + \\sum_{k=0}^n \\frac{x^k - a_{k,\\delta}(x)}{k!} | \\\\\n",
    "&\\leq \\sup_{|x| \\leq r} | R_n(x) | + \\sup_{|x| \\leq r} | \\sum_{k=0}^n \\frac{x^k - a_{k,\\delta}(x)}{k!} | \\\\\n",
    "&\\leq \\sup_{|x| \\leq r} | R_n(x) | + \\sum_{k=0}^n \\frac{1}{k!} \\sup_{|x| \\leq r} | x^k - a_{k,\\delta}(x) | \\\\\n",
    "&< \\sup_{|x| \\leq r} |R_n(x)| + \\sum_{k=0} \\frac{1}{k!} \\delta = \\sup_{|x| \\leq r} |R_n(x)| + \\frac{\\epsilon}{2}\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "Here I used the triangle inequality in line 3 and the fact that supremum of a sum is less than the sum of supremums in line 4. <br>\n",
    "Now we can bound the two terms separately.\n",
    "$R_n(x) = \\frac{exp(z) x^{n+1}}{(n+1)!} \\rightarrow 0$ as $n \\rightarrow \\infty$. Hence, for sufficiently large n, $R_n(x) < \\frac{\\epsilon}{2}$. <br>\n",
    "\n",
    "Finally, we have:\n",
    "$$\n",
    "\\sup_{|x| \\leq r} | exp(x) - E_n(x) | < \\frac{\\epsilon}{2} + \\sum_{k=0}^n \\frac{1}{k!} \\delta = \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "The hint that $\\mathcal{F}_{\\exp,d}$  is a universeal approximator means that for all $\\epsilon>0$ there is a function $E_{\\epsilon} \\in \\mathcal{F}_{\\exp,d}$ such that $ ||f - E_{\\epsilon} ||_u < \\epsilon$, for all $f \\in C([0,1]^d)$. From the definition of universal proximator, we know that for some $m, W, a, b$ we can write:\n",
    "$$\n",
    "E_{\\epsilon}(x) = \\sum_{i=1}^m a_i exp(W_i x + b_i)\n",
    "$$ \n",
    "where $W_i$ is the ith row of W. and $a_i, b_i \\in \\mathbb{R}$ are elements of a and b. <br>\n",
    "\n",
    "from part 4, we know that each of exp(W_i x + b_i) can be approximated by some function $G_{i,\\delta}$ so that $\\sup_{y_i = W_i x + b_i \\in [-r,r]} | exp(W_i x + b_i) - G_{i,\\delta} | < \\delta$ for some $\\delta >0$ to be determined later. we can set r = \\max_{i} |b_i| + sup_{x \\in [0,1]^d} |W_i x| to make sure that $y_i$ is in the interval [-r,r]. <br>\n",
    "Now we can define a new function $$G_{\\delta}(x) = \\sum_{i=1}^m a_i G_{i,\\delta}(x) $$ which is in $\\mathcal{F}_{\\sigma,d}$ since it is a linear combination of functions in $\\mathcal{F}_{\\sigma,d}$. <br>\n",
    "We show this below by subbing in the definition of $G_{i,\\delta}$.\n",
    "\n",
    "$$\n",
    "G_{\\delta}(x) = \\sum_{i=1}^m a_i G_{i,\\delta}(W_i x + b_i) = \\sum_{i=1}^m a_i ( \\sum_{j=1}^{k_i} \\alpha_{i,j} \\sigma( W_{i,j} x + b_{i,j} ) )\n",
    "$$\n",
    "Thus we have shown that $G_{\\delta}(x) \\in \\mathcal{F}_{\\sigma,d}$.\n",
    "<br>\n",
    "Lets bound the error with $\\delta = \\frac{\\epsilon}{\\sum_{i=1}^m |a_i|}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "|| f - G_{\\delta} ||_u &= || f - E_{\\epsilon} + E_{\\epsilon} - G_{\\delta} ||_u \\\\\n",
    "&\\leq || f - E_{\\epsilon} ||_u + || E_{\\epsilon} - G_{\\delta} ||_u \\\\\n",
    "&< \\epsilon + || \\sum_{i=1}^m a_i exp(W_i x + b_i) - \\sum_{i=1}^m a_i G_{i,\\delta}(W_i x + b_i) ||_u \\\\\n",
    "&\\leq \\epsilon + \\sum_{i=1}^m |a_i| || exp(W_i x + b_i) - G_{i,\\delta}(W_i x + b_i) ||_u \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Here I used the triangle inequality in line to and the fact that the supremum of a sum is less than the sum of supremums in line 4. <br>\n",
    "we know that $||\\exp(W_i x + b_i) - G_{i,\\delta}(W_i x + b_i) ||_u \\implies \\sup_{y_i = W_i x + b_i \\in [-r,r]} | exp(W_i x + b_i) - G_{i,\\delta} | < \\delta$ from earlier. Thus we have:\n",
    "$$\n",
    "|| f - G_{\\delta} ||_u < \\epsilon + \\sum_{i=1}^m |a_i| \\delta = \\epsilon + \\sum_{i=1}^m |a_i| \\frac{\\epsilon}{\\sum_{i=1}^m |a_i|} = \\epsilon + \\epsilon = 2 \\epsilon\n",
    "$$\n",
    "Epsilon is arbitrary so we can replace \\epsilon with epislon/2 and \\delta with \\delta/2 to get our result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "We can prove this by contradiction.\n",
    " Suppose that there exists some  function $\\sigma$ such that $\\mathcal{F}_{\\sigma,1}$ is a universal approximator and $\\sigma$ is a polynomial function. Suppose $f \\in \\mathcal{F}_{\\sigma,d}$ We also know that \n",
    " $f(x) = a^T \\sigma(Wx + b), a \\in \\mathbb{R}^m, W \\in \\mathbb{R}^{m \\times 1}, b \\in \\mathbb{R}^m$. we can see that f is a polynomial in x of at most degree N, where $\\sigma = \\sum_{i=0}^N c_i z^i$ defines N as its degree. So we know that $\\mathcal{F}_{\\sigma,1} \\subseteq \\mathcal{P}_N([0,1];\\mathbb{R})$.<br><br>\n",
    "\n",
    " Asuume that $\\mathcal{F}_{\\sigma,1}$ is a universal approximator in $C([0,1];\\mathbb{R})$ then for any f $F$, there is a g in $\\mathcal{F}_{\\sigma,1}$ such that $|| f - g ||_u < \\epsilon$ for any $\\epsilon >0$. Since $\\mathcal{F}_{\\sigma,1} \\subseteq \\mathcal{P}_N([0,1];\\mathbb{R})$, then any convergent sequence in $\\mathcal{F}_{\\sigma,1}$ must converge to limit that is also in $\\mathcal{P}_N([0,1];\\mathbb{R})$. Let's consider the function $f(x) = x^{N+1} \\in C([0,1];\\mathbb{R})$ which is not in $\\mathcal{P}_N([0,1];\\mathbb{R})$. Since $\\mathcal{F}_{\\sigma,1}$ is a universal approximator, there exists a sequence of functions $g_n \\in \\mathcal{F}_{\\sigma,1}$ such that $|| f - g_n ||_u < \\frac{1}{n}$. This means that $g_n$ converges uniformly to f. However, since $g_n \\in \\mathcal{F}_{\\sigma,1} \\subseteq \\mathcal{P}_N([0,1];\\mathbb{R})$, the limit of this sequence must also be in $\\mathcal{P}_N([0,1];\\mathbb{R})$. This is a contradiction since f(x) = x^{N+1} is not in $\\mathcal{P}_N([0,1];\\mathbb{R})$. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Exercise 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAADyCAYAAAAMag/YAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAY65JREFUeJzt3QmUnFd95/2n9rX3Xd1qtSRLsiwv8gbGGGwWs0MgBCYkkyE5ZCUkmZksJ5mchORNQk4ymeVkZjIEMkMgCwnkTQK8YDDEBmzjfZVla997X6u69vU9t4h0bPP7l1VNl9Syv59zOEb/7npuPeu9z+2q5+er1+t1DwAAAAAAAFhn/vVeIAAAAAAAAMDEEwAAAAAAANqGTzwBAAAAAACgLZh4AgAAAAAAQFsw8QQAAAAAAIC2YOIJAAAAAAAAbcHEEwAAAAAAANqCiScAAAAAAAC0BRNPAAAAAAAAaAsmni6Q3/md3/F8Pp+3sLBwoZoEsEHO64mJCe/Hf/zHv6+2brvttsb/AFzY8xvAhffwww97N998s5dIJBrn4bvf/W7OR+Blyo2j3/GOd1zst4HvU/D7XQAAAAAArIdyuey9733v86LRqPff/tt/8+LxeGMiCsBL1zPPPON97nOfa/yh1k004aWHiScAaLODBw96fj8fMAUA4MUcPXrUO3nypPfJT37S+8mf/MlG7ciRI2w44CU+8fS7v/u7jU/3M/H00sSd0EtILpeT9Uql4pVKpQv+fgB8VyQS8UKhUNPNkc1m2VwAgJe9ubm5xjbo7u6+JLdFrVbzCoXCxX4bAFrA/XL7MfF0gblnwbz//e/3Ojs7vb6+Pu+XfumXvqdz+uu//mvv+uuv92KxmNfb2+v98A//sHf69Onn/Y6bDb7yyiu9Rx991Hvta1/b+Bjyf/pP/8k7ceJE4zvwf/Inf+L99//+373t27c3bnofeuihxvfkXXsvdObMGS8QCHh/+Id/2Pb1B16O5/ULn/H0l3/5l43z9Fvf+pb34Q9/2BscHPTGxsbO/fwTn/hE49x114BXvOIV3j333HPB1wl4Obn33nu9G2+8sfHVHnfu/fmf/7kclP7e7/3euX7Vndeu3y0Wi99z0+meD7Vp06ZG3/y6172u8Zfc9XjWG/BS586RW2+9tfH/3dftXF9pPd/wfM7J//gf/2OjX67X6+dqv/ALv9BY7p/+6Z+eq83OzjZq//t//+9zNbecj370o95ll13WWP7mzZu9X/u1X/uec9697iMf+Yj3N3/zN96ePXsav/vVr351XbcLcKl7/PHHvbe+9a2NsXIymfTe8IY3eA888MC5cbE73x3XZ7pzyv3vm9/85vf01W5c7Prqbdu2eZ/5zGe+p52VlRXv3//7f984X9256M7fP/qjP2r0zWdZ98uur0b78FW7C8zdnLqO0U3yuJPNdXrLy8vnTpw/+IM/8H7rt36r8Xvu48Xz8/Pe//gf/6MxueRO2Of+9WdxcbFxAruJqX/7b/+tNzQ0dO5nn/rUpxo3vj/90z/dOJHGx8e997znPd7f//3fe//1v/7XxkTTWZ/97GcbHfKP/uiPXuCtAbw8zmuLm3QaGBjwfvu3f/vcJ57+z//5P97P/MzPNB6q6jrOY8eOee9617sak9CuEwWwvvbt2+e96U1vapyLbsLI3cy6m83n9qmO65M//elPez/0Qz/k/fIv/7L34IMPNs75Z5991vunf/qnc7/3G7/xG94f//Efe+985zu9N7/5zd6TTz7Z+C+fgABenOv/RkdHvY997GPeL/7iLzYmhN25eN99933P757POfma17ym8Zyo/fv3N/5g67g/5rivv7v/ujbO1hw33nbcTarre92NrhtL7969u3GtcMs6dOiQ98///M/Pey933XVX4/k0bgKqv7+frwoBz+HOP3cuukknN3nrvgXg/sDjJpXdH2HdeefORTd+dpPH7nxzzv737Ndt3bn+oQ99yPvgBz/o/d//+38bE9Xuwxpuwvfst3/cxPXk5GTjWuLuf7/zne80+uXp6enGJNNzvfB+2Y210UZ1XBAf/ehH3Z9a6u9617ueV//whz/cqD/55JP1EydO1AOBQP0P/uAPnvc7+/btqweDwefVb7311sbrPv7xjz/vd48fP96od3Z21ufm5p73s6997WuNn91xxx3Pq1999dWN5QFY//Pa2bJlS/2DH/zguZ9/6lOfavz8lltuqVcqlXP1UqlUHxwcrO/du7deLBbP1T/xiU80fp/zFFh/7373u+vRaLR+8uTJc7Vnnnmm0R+fHSY98cQTjf//kz/5k8977a/8yq806nfddVfj3zMzM43+2i3zuX7nd36n8XvPvQ4A0O6+++7G+fL5z3/+e/rbs873nHRjYffvP/uzP2v8e2Vlpe73++vve9/76kNDQ+de94u/+Iv13t7eeq1Wa/z7r/7qrxq/d8899zxv+W7c7ZZ33333nau5f7vf3b9/P7sUEFyfGA6H60ePHj1Xm5qaqnd0dNRf+9rXNv7tznd3Lrnz/4XcONr97Nvf/va5mju3I5FI/Zd/+ZfP1X7v936vnkgk6ocOHXre63/913+90aefOnXqRe+X0T581e4C+/mf//nn/dt93Nf5yle+4v3jP/5j4y8s7tMT7qs7Z/83PDzs7dixw7v77ruf91o3M/sTP/ETsp33vve9jb/ePtcb3/jGxkf/3UeBz3r66ae9p556qvGJKQDrf14381M/9VPP+/ThI4880ni2xc/+7M964XD4XN39Raerq4vdA6yzarXqfe1rX2tEtbu/jJ7l/srqPqV01tlz2X1t57ncpyycL3/5y43//su//EvjE1Pu04zqmgBgfZzvOenGwpdffrn37W9/u/Fv98kp1+/+6q/+auPrdYcPHz73iadbbrml8fUb5/Of/3zjOuBe+9wx+etf//rGz184JnefsrjiiivYvYDoZ++8885GP+u+HnfWyMiI9yM/8iONTxWm0+kX3W7u/HKfmjrLndu7du1qfDPgLHfeut/p6el53nnr7oHd+zh7HWh2v4z24at2F5ibQHou951S93Ff911T91/3h5MX/s5ZL3w4sfso8nNvTp9r69at31Nzy3dfp3PfX3cfRXTPnnCTUO57sme/Vwtgfc/rZl54nroUH7U8d+4/t7MGsD7c19nz+bzsd92A9uzNrTs33TntnhXxXO4PQ+4r8GfP3bP/feHvuY/vu4EwgPVxvuek425Ez57LboLphhtuaPzPnZfu3+6rfO4rse4m+Cw3IeW+smfdlJ59AHqzcTeA7/az7r7T9akv5CZ33YcuXvgsY+W5fxw6y/Wr7tEWzz1v3QcqOG83JiaeLrKzf1lx3Inn/n3HHXc871MQZ7kHsT2Xe/CwxfrZv/t3/877z//5Pze+m/6BD3zA+9u//VvvHe94B5+mANp0XjfT7BwGcOme2wA2zjnpPsn0yU9+svHJCDfR5Cai3Otc3f3bfRvAjcGf+2kK9++rrrqq8VxU5YXPXKQ/B9pL3Rs7zw0OcOft7bff3niOlLJz587n/Zvz9sJi4ukCczOxz/2riHtQmjtJ3IOJ3QnlTh738xeeGOvFPVjx2muvbXzSyaVonTp1qvHwcgDtOa9bsWXLlnPLO/txfqdcLnvHjx/3rrnmGnYTsI7cX0XdwPPs122e6+DBg887N9057X7vuQ87dV/VcQk6Z8/ds/9114DnXhNcGMhz/yoL4Ptzvuekc3ZC6etf/7r38MMPe7/+67/e+Ld7oLH7FoCbeHLJz+4hxc/95LL7FJRL3mLCGfj++ln3LZvn9qlnHThwoPHJRTeR6/7/98udt5lMpvHVOmw8POPpAvtf/+t/Pe/fZyd9XDrdD/7gDzYmn373d3/3ebO3jvu3G7iuhx/7sR9rfNfWPdnfRcy6tgG057xuhfvov+ugP/7xj3ulUulc3cXMuoE0gPXl+lz3LCf3KWD3h5iz3Fds3LOfznrb297W+O8LE3HOfhri7W9/e+O/7iY1GAw+L5Ld+Z//83+y64B1dL7npOMmgd3jKVwinftDzqtf/epzE1JHjx71/uEf/sG76aabGufuWe55qy4Zy31S6oXc13PPJtECePF+1iXHfuELX3jeIyjcJLH75o375KFLu3OTv873M9515+3999//vP77LLdc9wxGXDx84ukCc59acPGsb3nLWxonxl//9V83vlN+9pMMv//7v9+IfHQnpnsIW0dHR+M1LhbWRT3+yq/8yvf9Hlx77iOIbpk/93M/9z3PjgKwvuf1+XLnorsGuAhY94mnf/Nv/k1j2S7ulWc8Ae3h/tjz1a9+tXET6h4K7gambvLYxTO7Z0U47lx28c2f+MQnGoNX9yDhhx56qBHl7vrq173udY3fc8+K+aVf+iXvv/yX/3LumuA+NeG+Qu8i1vnkBLA+zvecPMud33/3d3/X+Prc2eetXXfddY2b3UOHDj3v+U5n/0j7uc99rhH24R4k7iar3MOJ3acyXN3d2Lo/FgF4cW5s6z5x6CaZXD/rJnn//M//3CsWi94f//EfN35n7969jUmqP/qjP/JSqVQjRMuNhQcHB897E7vQgC9+8YuNx8i4YB73KUY3Sbxv377GBLO7v3Z9MS4OJp4usL//+7/3fvu3f7vxMV930n3kIx9pPHPpLFd3X7Nzf5Vxg2HHffzQzRS7Qex6cANjtzz3oEXXsQJo73ndCjfB7Aa37vWuA3WDZNeJ/tZv/Ra7CWiDq6++unET6dKx3Hnsvobu+t/p6elzE0/OX/zFXzQmgN0nEN0fbtxDjN0fij760Y8+b3lu0Oy+VuA+KfGNb3zDe9WrXtX4lLEbcLswDwDr43zPyedOPLnz8CzXX7vz052nz32+k+O+/uM+CenG45/5zGcay3fntWvPTS6365EYwEuR+0OOe56aOz//8A//sPE12Ve+8pWNP9S6/zru/HWf+Hc//9CHPtQYC7tJ31Ymntw5+q1vfcv72Mc+1ki4c+eu+zSVO19dv05C9MXlq7/wO114WXjPe97TmP11z6EAAADt4z6R4T5l4f7q+5u/+ZtsagAA8LLCM55ehtxfcb/85S/zaScAANaZe/7LC519Ds1tt93G9gYAAC87fNXuZcQ9K+a+++5rfDTZPUvGPUcGAACs71dv3Vd/3MOPk8mkd++993qf/exnG19xP/tQYwAAgJcTJp5eRtx3Xn/iJ37CGx8fbzx80X2XFgAArO8zo9yzY9wDU9Pp9LkHjruv2QEAALwc8YwnAAAAAAAAtAXPeAIAAAAAAEBbMPEEAAAAAACAtmDiCQAAAAAAABf34eL/4T/8lKxv3zJhviYzvyTrR48dkPXrbt4j6zOrU2Ybi5l5We8e6pH1Wshe5eWllP5BVZeDgYSs5/Nhs41dO66V9Xe++b2y/u27vynrX/zi35ltJHujsn7ltVfK+qatm8xlLS7rfbh185isjyYHZX1petZsY3l+Ub+via2yHuzoN5eVL9VkvVap6N9PZ2Q9YOxzpyORlPXXv+YN3kZ2xwldr9fr5mt8Pp/3ctRsm7RqXbdhrfX3Vb8Qu9B4X2tZd2vb+9bx7yTW7n3bZd6G9bd/90lZn5+zr62puTlZP3Jov6yP9nfLekcybrbR06uvx+Gw7gfnl/T13onFIrKeTudk3QrIGB2zxyVeQPeP+w+elvXeAb1+wUDJbKKrM9rSeriUWUskHJN1X0hvq7pPtz27kjbbKBv9pj+g31cuZ697LKL3eySs+2CvWtDlnO6bHV9Nd9A/+0u/4W1kv/rT+vzK5Iyxp+d5Cyu6Pmpkw1w21iHrxcKq2cbQmL5Ov+qm62U9v2rv/6ef2ifrg3163NTTq9/vasre/4l4r6zf8QU9yHnb266W9XLRvhYVq7r9YrEo692DeuzrnDyjr9FbtuyW9bu/8oSs15uMSxP6cuD5jS64L2n3zb2d+poTDOjfLxX0OVzzAk2uOXpl3vX/NFnJi+zXfu3XZb1QLJuvWc3o697Ssr4eZwv6+PI12ZbW/Y3fr8dMgYC97/0+PTiqVfT7qlb1/ipW7LFq1Zh6qFvr6NP905VX7DDbsA78/QdPyXo2X7Hfb00vK2AMSTePjsh6f7++bjlzcwuyvpJalvW40Tc3fhbTP4uFdT0Y0tu97m+yD439/nef+VPvxfCJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAODiPly813i41pjxgEun0tUp6x2j+mHW73mdfjjzIwceMdvYd0g/EKxe1Q/LypabPECsqH/mM9Y9V9UPW9u8xX7g2eCAXvc77/yGrPf36IdRvu1Nt5ttFIr6IZILi/rhZXveZD8U+/7HH9DLWtUPPEst6vpj9z9stnH8sH4g5Nvf/oOyvlo4bi6rUtdzqfMLM7Key+qHet7+utvMNqpF+8Gal6KX6wPEL9Q2MR+W3aQN6zX+tTys21ufB39fqIfQm8tq8bnqzZ4Pfyke8geefUbWC1n9wGqnnNc/GxnW/VBfrw7MKDR50HNvr35g5uysDv6IR/TDr535Bf2w356ePllfMR7QGonO2W0s6tf0D26R9Xxer3s9rB946iwv5Ft6MHEiobe7Yz3fM2lsx4yxz2v5rNlGT7fehwsLOlykI2o/bD4S0SdX0HjY6/GTJ2U9Pa+PHyeZsNvfyLI5PTYrNRlSvP7WUVm/5prtsj5phPdkUvbDxTePDMl6Lm089Lxi/816dHhA1n2eHl/nMtnWrzld+oH/uy7T72tmUo8Zw9YD7xs/0/cQ4YB+eP7ynB77NlT1sX/08DFZ37VbX4vOnLCDli7fqQMVFuemZT3a5GMH2Zx+WHjceJF1JQz47Qcgp/P2A7k3qrpxL+hrcivtC+itEwjo46tW07/vMx763WxZ1p4JNHlKfSik93F3X19L/f/iin3+ZrL6+Jqc1f1/0AoFq9nnrxWAUTQe3l4o2MsqV/R2LJf1MVwt6frxY7qvc4JGKEfQp/dHssseR3nGfXDNWFbJeDh9M9aD68/rtWt+JQAAAAAAANAEE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgLawMyBf4IM33CTr2WUdf+gsL+hIY/+Sfs0Dn/sHWf/St+402xg0YmCvvuUVsv70GTvOsGrEulYDOraw5umo0HpJxyI6uayOxAwEYrJ++uQZWR/ttaMU+yM6WjNS1hGa1WU79vLYqaOynvfp+MXe7m5Z9410mW3s6r9R1udyOpJ68qQdW1ss6fc1uLlT1mNJHcv82MlHzTa6e5Lmz9A6O5T80potN9Nuzbodj+v36djetfB7xrKM5uvG+1q/d7RGTeKE5a+v6Q1f9LU0RawfBANN+sdNst7Tra9hheyKrAeabJelJf2aYl5HJq+klsxlzc3rMUOhoKORC0UdW5zL5cw2duy6XNZ9ft0/1vy6T/EZEelOuarfb8oY+3hVO8442qPbKazqfrCc1+uRtA8TL1jU+7A3Yqx7QI+VnJIRY53K6nFGalZHxMei9jgqGmztWrBRBK2xWdzuBSe2JGQ94JuW9b17+2X9maf0ueVkFmdlvW7ss3hMj32d7qi+UlUq+nYjGtVjMK+krx9Orbgq61s398h6eiUl6x0JeyxXKOj2yxV9PtZr9jYJ1vU6zszoa2EopMe+vYN6fO1kivo1/qA+jyZP2dfh/m7j1jBh7EMjCr5WsK+RFZ1qv6Hl8vp6mM3bK5PN6Z9VjT41mdTHZKWir6v/urCWhgZ9Sfv+8co9O2Q9Gtf3qCWj7xob12MP5+jxU7JeNg6K2ZkFWa+U7G1SMsYfqZS+FmSMfetEY/oaXDLuN5equn+sNbnZCQb1unR16OPBuJ1vCAR0/1g1jlOfMbb2++27rUh07XdiG/UeDgAAAAAAAJc4Jp4AAAAAAADQFkw8AQAAAAAAoC2YeAIAAAAAAEBbMPEEAAAAAACAi5tqN3PooKzXinYi2qn9T8j67Ip+Qn0tq9M4tm3ZbLZhPXV96YxuI1i1kyfCPp18VqjqNnz+eMtpDVXjKfjdXb2y3t+ln6afmTpitjEwpJM9Hn/4cVm/8rq95rJiRtreakknAxQr+un4oZidHrK4kJb1TEon7dSqdjzP4489JutjKX1sdQ/rfdjtt4+TVE0fW4DiW8eEOrx81SpGWllc9xHOypJOPquXdeJLb4/uA2dndeqV40/phKlQQA8vZufmzWUVK7p/XF7RfURvr+43r73+OrONXE6ve7GgxzJbJ7bKek93n9lGqaoHAbmM3lZHDx8ylxU04idnTx+T9Z07dWpfMGgP906fPi3rlZJej85uvd2d5cUZWS8ax+/w0ICsx8JmjqM3O6sT3Ta6yUmdwvgzP6uTfZ3OuD6Hlxf0GDBS12Od6/bqtCpnZU4f+1Mn9Xbujdnpaql5PT6KRPRYa3pWXw+2TNipWKWCTnDzjLTlvk49jg2F7XFeZkWfq5WyHn+WmqRc5gv62I8aiXOJpL6mZ3N6TOwkE8OyfuKkkZzXJBUr2amvbcWyPk6KRX2dyKf18e4ELsGPPeSNFNWFJTshMBbV+7Kry0j6NlLi8gU7SbRa1TszUNfvd3DAThnv6tSJd30D+rpSqen+KVew9304vE3WBwZ0X3DypO6fUmk74dzobrxSXm/HctGOnKtU9M+qxkt8ft3X+nz2QW9sRi+1qs+5UpNEv+4ufb/dkdD7NhLR18GokVDq+ANNYnJfxCV46gMAAAAAAOBSwMQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAAcHFT7fadOizrIZ/9JPh8QD9Wfimgn9pfM2IWck0enl4v62U9vU8n6vVP6MQX51W3vFnWUzoEx5uc0kk/kydPmG10J3WawPgmneAxPzMp60UjHc/5y09/VtajSf2k+wMH7fe7d7dOW3lwn06Pi5T1U/OrJXsnxms6IeXJR5+V9acfe8pc1vvf/35Zf3S/fr8LizqBZXdyxGwjFl770/xf6ur1esvJbi/12W9r1Y1Ntebt2OqyzN9fQwifr7Um1tSG2ba1TayYkEvUnj1XyXo6bScdWdsgHNLbLBTSKSZ9vTptxslndUrM6qpOhdqydae5rENHdLrb9p06kSsW02lV+ZKdqJPN6w59z5U63XXTpjFZrxvpQ06hrFO3kgmdGtjdoevO0489JOs7xvWYIVTWx0NuWe+PxrJGdBJuuabfV6ZkR2LVBnTiXbFiHHMRnfqUzxvJZS4JL2Cn7WxkW3RAojd52h7TbLpOj0WCVZ0Sd+aEHtNU+u3jNW6c9+Oj+rwvG8mQjt9INl5d1deJDuOcWJ22E6tCQeNYMtKkVjP6WCpl7D6iaqxiOKgToIpG8lWjnax1PdKvqVdysr5ls06uc5aWdfpVR7cer14xcYW5rJNHdWKiz9Pr4TfWw1e3R3fRxKV3DpeMyPKODju1O2gcL2Hj7ttnpIX5/fb2yhtJbV2JDll/xU12iubY6HBLY7ZyWR8TxYJ9vckYfXA0qu8fe3r0ffPcwqLZxoGDx2U9EtFJkumMTs51akZ8nZVe5zfuapqE2nm1mt7AJWOOo9nwtlgxkuLzel8NDek+O1Cz33DYv/Y7t5f6PR8AAAAAAAAuEiaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgIubavdDH3ifrD/19JPma+5/SD9xfj6imy1XdFJKR9+Q2UZPp05jmSnoVIaSp5/y7zz6uE7uS+dKsh6J6JSBWMDerA/d+y1ZP3pYv98brr1O1stVOxbq1je+Rdbnl1OyXvfstIT+xKCsb+4alfWUkWRU0mEQDQ988zuyHg3rtJPLt+8yl/VXf/lpWf+xH/+grN//lN4fhbSd2rNlQqccYW2pay91NSN+otm2Ws/t2PKy1tJ0qyl8a2ij1XS+Zuvd6rI2gpWUTio5euSo+Zo9u3Vy0dLCjKxPTk3Lei6rE5OcPiNx5vIrrpT1Bx962FxWd2+/rNeMwKigERPjaxL5cvkOnapXKetO6u677tTvqazHBc7giE4GCoV1+tHcjN7ujZ9N63115tAzst4Z09tkbNQeR6WqOkWrt0+nmtUC9t8sOwZ1Qs6Tz+jxVdWvj+tck1SkhSU7zWgje/e79fmYTumEZGd5Wa/rrq3bZN1fnNLLmbVT4srGmDzsMxJ87eGR1xXX47biqh5/+owUvGrF3v/lrH4DkbBOSCwY6XXFgk4Dc3Il/Rp/2BjMNkkdKxV1mlRPn74fiSR0Etrqir0PS8b77ejQ1+d77t9nLitsnN6d3fp9xaI6XbRWsdNFV1J2auVG1dmpj+16k5jeinFf6zPGIEG/3maDgzr92+nqGZf1G66/VtZHhnQ/60Qieh8Hg/oaUTM650LB7h+XVvS1wDOuNx1Gintfv71NBgb69PvK6b5uaXG+5TG8Z/SD1ape93pdb1vH6lGt7Vtr8rmhqnFsVYw00vwZ3f/09tjzJQP9up8/H3ziCQAAAAAAAG3BxBMAAAAAAADagoknAAAAAAAAtAUTTwAAAAAAAGgLJp4AAAAAAADQFkw8AQAAAAAAoC10PqJw3906dj7l2XGZ2153q6xfPbZJv5mYjqqcX9bxh87CvI5lvDy5Q9arTeIMyxUdWxhL6HXctX1C1lOzOs7W2T46KuvJfh11PLugo3TrJSNf2vO8aIeOkUxll2T96W/eZy7rqaeekvUP/eQHZX1pSbdx7JS9TYI1Pf/57NNPy/rmUX38OMNDejt+9m8+LesDoz2yfu2OG8w2xobt9vHS4LPT2E11I8L05bDuns930dqwtrvPa+09bXQ9A8OyHpmeMV9z4PBRWQ8aKem1qo5+7u6xo3MDxrKKeR1V/upXv8Zc1sKy7j9qZR1P7DNy3eNh4015njd1Um+TRx57XNZXU2lZ337ZFrONek2PGe598End9kOPmMt65Z6dsp6aOy3rN9+kI7SjkbjZRiikt1dnUsekJ4yxkuMzRpU7xgZk/diUjrEe2jxmtjE7O+1dipJdUVnfOrHXfM2McbweOajP+7EBPQaaP6PPLadSNca+RrR62N8kyrtUlPWEsax6VUell8r63HZ6uvQY9/DBOVnv79XHXj1q378srqzIeqiu+xV/wH6/HQm9vfoHumR9OaX3VaVqd5xGgru3vLos68GwuSgv3hWR9YwR7R5O6tj1YkHHtzul2qXXPyfi+hqay+tjvvGzQl7Wl+b1PdFtN14h6yPD+ph3evr6ZX1806Csd3bqe20nGtMHRsCvz99SRZ9DoZC97yu1ekvnfCaT0csp29u9c0xvk9tv1fd2qUV97XCePq5/Vq3qzs5vjlX1te67P9PXiIAxwPI3uQZ7ft1+1bh2Fcp6f8wurdpNRO3xxIvhE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgIubavf4wUOyfmLZfhJ8d/5yWX/mzjtl3e/pBJXb3/xOs43+ka2y3jWs59QW5nXCg1Mu6liIQF0/aX95TidPdBsJD06ppNtYWdTLSnTp1DVf1E6EWErrBIBYMinrH/7wR8xl3XO33leJsH6ifSGkUwbmJ+1ElVg8IetDRkJdNKqPE2duXqfdDA/pVKaVtE4NfOPrbzfb6B/U++TlxAoxW89gNzOtrMUEtfVsey0uxPt9OWj5eHhphAyeMzamU77CQTup9Rt3flXWK0byVLGkE2QXZuxU0mv3Xi3r0bjub0JRu3/sMvrOdEr327GITuDp7rATVwa69fvKpnVK3dTUpKyHm/3Zzkj62bdvn247Zyf3pjI6WWZsfFzW7/vWt2V9+7idxtrdobd72K9PonReJzU5x08fkfWokUTW06H784OHnjXbSCT0mGGj+9a3HpP1uH0KewMd+ngtZ/U4L7Ooz5VEtEmic0Yfr7WA3v/5in28xiJ6f/qNKNOKkaTZkew221hZ0YlZ1mF5+LCVnKhT5ZzNW/T5dea0HmNWfXZCXkeP3iYBv06J6+rU+3xxSSftOcm4bqOzU69jKGifQ9NzC7KeWtIp4rNHZ1pKPHVisUtvXDS3qLd/sWinqy0t6GPPZ5xDoYBOPps6edhsY3pKJ5zu2bNL1oPGcefUjb7LF9IdXthIRC0H7A7Sb6TRlkv6BF5N6+1eyOnkXKdu3LcP9Oq+7tbX3mwu69nj/yTrdo6l8Z7WcMjXjYS6erOFGWPlurHba57+Qa3JIDqdtrf9i+ETTwAAAAAAAGgLJp4AAAAAAADQFkw8AQAAAAAAoC2YeAIAAAAAAEBbMPEEAAAAAACAi5tqF92mE1+Si3bC2MgmncIzOKaT6GbmdMLY+KYRs41jp0/K+vSsTlHL5+zkibe//R2yPtCr01hOHtUpAytLOuHBKdV0ql0ioZ+0/6Y3vVnWz5zSSTvNjI3pVJuJCZ3e4Qz26XSN5XRa11d1IsN9Dz1htnHNNdfJ+hNGAlDBSHNximWddpJe0EkkN9/2Cln/1v1fN9vYulMfv9uus7fjxlBrOcHNSgyzX9L6XLYRduP5PN22EfLQULPDOlp8U3Yj5vay0iesJl5iqWvt5vfWL73uUkwaPH1cp4XlmiSi+YwYkzNndArO5vHNsn7l1deabSwYKVoTW3fKeiRmpymllvUY4MTRY7L+6lfdIOshI42t0X5Qb5NrrrhM1vdepddjakYnPzknzugko55O3c+fPH7CXNbssh7LvPudOnl19xadBrvv4fvMNq6+5gpZ74xHZH1yWq+fc8jYV5t1t+l5IX08pJftBK989dL8m2lXQo+nIk0SoGLhqKz3xPTYO2ykqwXK9tg31qHTIetGWlet0iQVK6zPvZCRfuXz63qpZLcxOa3Hn1Vj/NEzrFONy0ainnP82BlZz2b0++ro0ueK4zcSqGamdFpoPKlTOWPRJse9cV0LGfGbx07oPsBZSuv7lO5uvY5RI1EvFIzabaTs83ujOjWt+7qQzz6OuhP63Lr5+r2yHqjqFNNyXd9bOdGwvpU/fuRIS/eCzZLXe/r6Zb2zQ6cmZlJ2/5gzUuoW5/T5MDU1J+upVfte8MSJU7K+fUJ3RJPTuo1mCXLWONLnM651PjtZ1O/3t3S/UTXu5xpqxrKswbJPH1uDg3ruwxkasn/2Yi7N3hsAAAAAAAAbHhNPAAAAAAAAaAsmngAAAAAAANAWTDwBAAAAAACgLZh4AgAAAAAAQFsw8QQAAAAAAIC20BmMwk233Sbrs/N2BGHP0ICsx7t0pPC+p56R9XqpYLaRXdHxy4G6jrfcvWuHuawD+/fL+nyvjmJ91StulPVHHs6abdzymltlfXjTqKyHIjq+dGJM/75TqxixuUYc55GDT5nL+v++/lVZzxf1sv7mr78g6909OuLZ+fY9n5L11xrHXDadMpe19bIJWb/sylfK+vBWHRE6n5k22zj10HFZ/6Hr3utditYzWn5NizISPo10Ua/WJEX0Ym6veqvbxE58f8mw1r1Wq6/b8WjFza7ncb0RdHXqfnNqUsd+O9u3b5f1TZt0nPLDjzwk63Nzdj8fi+ro72OndFz3swcPm8vy1XXf1RXXsdwrizqyuZC2LxKxqI40rpR1dPzyio63PnBUxzU7nT3Dsn65Mf44dVL3KY2fHdc/O3zwgKzffPVlst4RtuO4l5f0dszmRmR9ZNQef5ycmpX1qHGcVIM6ir27T4+7nMVTdv+8kRXzOmY73pkwX5NO6cjwVEnX+7v19ow3uRyWy/q8Cxqx4MEmdw4lY2xo/Z07HNLnY75kv+FwRL+mUNR9QTabk/Vk0t7uoaBxvBrbPei3o9KDxgbr7+zRL/Dr61fJuD46kai+V8gW9HUtFtO/7/SF9M8iSV0vlPU+r/v18eOspu37uo1qZbUk6/563nxNyIi2rxkD2ZCn65GwfdLNTE3KetzoNwvZZXNZhaI+XiYmtsl6dMsWWc+vrphtpIz+ZnVpSdZ9db1NlpbsNnwBfax++c5vyno02Wcuqx4wtn1d79u6cd2sNxmT+syfGW3oKY6GalVfJwIB/b76+rtkfdOQvj/+rrXfiPGJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAADAxU21S9b1E9ePzeun0zth4yn8d/3L12X9yUcfk/Xb3vhWsw1/PSzr40ZK3NVXXWkuKx5PyvqWLZv17yd0YkAgeJPZxvy8TnzZf0gn1CzOzcu6v8kj7Xu7dKpJwNNPuq80idfqGdDJMs889ISsDxnbPdckwaLbSGu6y0jUGxy0n7Tvi+hEht7Ry3XbY92ynvPr5TiVykWOVVujmhGa4GuaMKZTEHzWS4x6s4Cxmk9vz6oZE+e1ngxhrLz16/X1nMWvtxSI8d2fWdtxDdvXWpb5+2sJgzOOIWt/rCVxzn7JSyu9ztI/qFNir0vo9CXHb5y/8/O6X/EF9EF58uRJs42hoZGWEvX+5et3mssKGqlNY1foZZVLOk0ovWwnn1YrOplo6oxOBkqv6kQsL6aTYJxwh+6f48aYaO+eneayVq0EoBVdn5zU/f/wgJ3a09mpk4kSHZ2yXizpbegM9Osxw5mZGVnvH9X7NhTW4ytn77XXeZeiVEpfJ59+Ysp8ze7t+pxMhHW/mc/pfRON2dszaCRAVcvGfq613hdUyvqcyOV0CnTVZ9+ejIzo1MhiXqdvWkO2fN4el9aqrdWtdFUnm9XrGI7ra3c4ove5r2Zvk4U5414soJP7ohF97+SUKnr8WynqlU/E9bWwXLFT7QYHxr1LjXXY5wr2/dihFZ3UNnxE9zfXXz6o2y7odFWnr1vfQ5Xz+rg7duSguayBAT3OKOXTsp5O6eOumLP74PkZncS7uKDHJem8Hhf09BipkJ7n7Tug5xM6evU96v5DJ8xlVX06sbJujfr9ekwasKK6PXfZNMbKxvi2Wbp3wGi/p0tfC4aH9NigZqTjNUtMPB984gkAAAAAAABtwcQTAAAAAAAAmHgCAAAAAADApYNPPAEAAAAAAKAtmHgCAAAAAADAxU21yy5Py/pgr049cYrGU9eHhnRK3PjWjKxv37nLbGNsXCeiJHuHZP07Dz5sLmvnFXtk/c5v6BS+BSOh7vjxI2YbM1M6veS1r32trAf9OhWip1enGDgnTxyV9SefvF/W6yE7eWLHdTqhb//B/bI+3KWTKo48e9hs4z3veY+sf/GL/yjr/X32MXdkSqcDZos6/SgQ1aeAv1JuOSlqo7OSxPxGAkKDcQ6vIZTM1GrCWbP0GLuN1n6/WQtmEt46bivzNWuI2zPf71qi+wzWMVRfxxS+9Xy/l6JAQF93ymX7WhUO6w3d26PTPF910ytl/Uqjb3Se3Pe0rE9P6zFDJqP7eWdiVPfbeSOdJxHV65ddtdtYWVmR9bqRKhNP6v6mGtTpcU4kEmlpH1r7w+nv0n19zUi2nU/pNKGuHnvMEK7qv0EurerUr1xO96dOR69OReod3y3rJ2cWZb2rx07hS2ft9jeyalEflze9cpP5mt5OnagU9nTiXKCqt00+b6QzumuIESEbNlIuozGdAO0UVnX7lYo+XoMhI12tZo+z5md1el3VSGEKBfQ2TK3YSWGdPfq8jyd8LaV4N34W0YODUFhv31qtIuuBgN1GV5eRTOXp5LyVVTuZsm4cD+mMfs3CMZ16uqgD3RqaDTs3qnxBr3+hqPeXUy3rY+/gKZ0Gd/llOnUtm7cHQKGQPo7KxnWys0kiWShknCtGv1Kt6vN6YcFOvLfGAOGwvhYEjEN1NWuPfSa26f7m6998SNaz9ungeVbCpl9vK5/P31oKnufOU+M1Vnpdk/MnHNTvN9mhrwWBgF5YsMlAPWis4/ngE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgIubanfg4L2yfnxaJ8Q4b3z/z8j6D9z6Fln/kURXS09cd+7+pk6cO/XII7KeLVmPiPe8Z76k0+CGh3XiSK+RNlPM20kZsaie65s+c1zW+wb6ZX1x2U4MWEzrKAl/r96+PcN2ekyqoJMM/AGdZHDvPXfJeiyYMNv4h8/9rax3GYmJ+w88YS7Li+v3VTPSTrIrOi0p2WmnDO2+zE542sj865gK1mrCWLMUM5+RJrWWxlsNSrHeV/Pl1FtMjzN+v0krVtJf3bd+O7HVdW+WJtgsrWPd3pdx6fYbTVdeYil4p0+ekvXeXvtaVa3pqJZDB5+R9WKuKOuFkp3aU8jptKzJ5ZWWEt+cwWGdalcrpGX92NETsp5esvvHSkWvS91IaQlFdHqdP2If8ytG+4mE0Q/6dBJcQ1W/37iRyDo2NiLrJb+9D1cL+uQqGLGm0U49LnE6e/S6TM7pfbiwrOvRpB6vOF2ddkLfRnbF5Xqd4jH7YlUxxmDRuD4uIwF9jNVjRnqcO+9TOmXKV9fHeD6vx1mO30h79FX1tSVf0GOzasUeq4cD+hoSjxnJn0aiXm+ffRzVjTipZJdev1DU3ofRmN6OPr+xHY0+MBLRqVROyYgRP3jwtKyv6NOuIW0EINaMlO18Qb/hQtreJhn7NmnDsg7Jar1J0rVxrJ6a0Rvgjnt03zwyYCdJJoy+qLdLt9030NVywmVP32BL/fmJkzrV1gmF9XFcMw6Kmk9fuzJN0gT/5dtGel1Bn3N+Yz85VeMeJWCcD4GgTg0MBuxrsD+k+/NSSY/hQiH7/YaNtEy/MVgOG0mGoSbRk8Wivp6fDz7xBAAAAAAAgLZg4gkAAAAAAABtwcQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BY6v0+Yz+ho5HS5YL7mnvsflPUv3f2IrJ86reMXb7zxerONL33hH2R9x86dsh7t6DOXlTFiXTNZHWdYq+p6tWxHiIYiOk5xYWlR1pNGVHaky45VzSzrmMNyVEc/RnrtmM6F+SVZ7x3RMbThTh2/GGsSPX382JSsrxRmZL0etiM0b3/L62V9dIuOmJ6Z0W3sTNjHyZ7tV3ovJfW6fbz6jFxfK8K+yaJaZi3L57MjPi3WS6w26kaM+Frat36/2Xa3fraWdV+v97WWttfzeGi1DesYbfaajayjQ19zV1ftXOpyRfcF1aqOFK4Zu3h0dNRs49i998v6Slq/r2pZ97ON16zocUagmpf1WkX3BUYidIMvoIc9Pr+u14woZX/djpT3avpn2ZzeJom4HetuRSDXjHj6maWUrI9uGjLbSHbreO0Oo17x7PjwUl1HM+975qCsL6V0rvu27Xrs4/T393uXonA0I+vFoj7unXi8U9Zn5/V4OWLFbJfsPi0e0PHf+aw+xrKpkr2sSLCl65c/qM+VlSX7uhY04sIrxvUgHNbbpNxkrJ5K6X21eWKTrKezC+aykgE9Xg8F9fvyGadXejVntjE7p6+Rkahuuydoj/sLFX0NyZf0voqEY7KeSNjHXM3omzYyM44+rM+fZn2tr67PkxPT+np4ZmbZbCMU0Nu5M6rrlWsmzGVt26z7iUh0Xta7unQfkey276EWjT6qYNzazc7rbXLH3Q+ZbeQqVj8fbnk6JBLUJ2TAGEv4jd8Ph3Xf6MTi+nxcTuttFQzbfXDNuH8pG+OSSERfh6pF+3oTiVrb8cXxiScAAAAAAAC0BRNPAAAAAAAAaAsmngAAAAAAANAWTDwBAAAAAACgLZh4AgAAAAAAwMVNtXtmVicQXHfDq83XzC/pR9R/5977ZP34qdOyPjmt0zuct779B2R9emZO1jNZO4kmldHrOD2tE0cWF3Qbt932SrONa67ZI+uRuH7afS2g3+/s0qzZxsTurbJ+avqUrFebhFUNTOj0mO5NOnGm6Om0k7kp+/12VnUixFXXXCHru6+43FzWwCb9fv0xnQAwGNYJDjfdbO/Dr379Tv2aH77R28h8RjqTlVz3ry+S5VrNihJbQ+KcGSxjtL2WpLZaraXfX8/0uLWkxDVLvFsv69lGvd7a9m2+rNaOrRYPnxexfvt9vR0/cVLWS2U7HWhiYlzWL99ztayvrmZl/b57v2O2MWskn/b09Mj6zJTuh5xiXqfkhuu6vmSkwWazej2caEQnMAWMkJZwUP+gWrLT+UqBfEvJNRUjIdcJBXUqVc1Ij6v59LBuYVmndDnxhE5O8+X1GK6n304sqhnDymhU9/PdPv33z/lZPb5qvK9LMJXSGTDSgJulGk5P6fOrY0CPwaJ+vS9PHrLPu3xdH8uBih43VexD30sV9TGTK+h0pu4ufZ1IdtnJhbmcvh7U6lbanddSCp4zPrFZ1uueHpOHgvq64qRW9PudX9TJfb6AXg9/yE5PCwZ0MlV3tz62llP2vVBPn3GdWtHHYjan06+ahAZ6TTbXhhU0+oK6cf40Z43H9fWzaiSSNR3HhnV9NW+/39k5vY/n5nWqXq6oxx+hZkl/Pt13PfrkIVnv6NfjmIpnJHi6a5exr6z0WivZzYmG9GsCAX197OzS1+Byxb57KVX1zwKB1lOxrWF3wRhfpdI6NXBsyL4G542E3vPBJ54AAAAAAADQFkw8AQAAAAAAoC2YeAIAAAAAAEBbMPEEAAAAAACAtmDiCQAAAAAAABc31e6Vr9PpcZ/4xF+Yr/nQB39a1ksl/RT8LeNjsv72d7zVbGPLuE5wu/feh2X91NSCuax8QT/pPxbTqTIdnQlZv/ub95htbNuxTdajPbqN5ZROGKgG7YSDXEE/ub5spCKEo/bT/NNZnegXjOo5y9e/41ZZ99XstKhsRj9RPxzQh+fw8LC5rHkj5ShnHHM333yzrD/6xKNmG48+8YD+wQ97LzkXIl2tVWtJSlvPlDorUclv5O35rHqTbVs3Ek/qxt8K6j6drrGWfWgmAzZN0VjH7dti++uanLeBU+3Gtuhkl4JxvXeKRuJMf/+grEeNdK26kULjdPfq5JP5uRlZn57WdWfLJp0yGgrrc6hqpMSUKvaxmsnpdK2+oQFZ91lpQjU7EauU1ql2lmYJQH4jpS4c07FQ3X06ca67w05OiyX1WCaeMOpGOp+TzRdbSgCKGeuxmraTCfft2+ddigrG+ei3IhXdeT+u09UWZ/X2efZJnX7Zlegy2+iI6PFn0Ui5rJR1iplTr+jzZcAYtx16dkq3YYd1eom4PmY6k/oYz+R1omPFeK/O6Un9vgoF/ZqOTvvv+P6wPofjyc6WksJKRTuNbDmt+4FCRV/vVvTQviEc0+939y6dNO0P6OvXsSN2IvnMtH0vtlEFg8GWx0Y1I1HZqltjkJpnj/FqxmvyJZ2WGgzq67pTMvrUxUW9v6zVKFXtA8xK1Ttt3J9XZ/R1yGckOTr+FtNV+/t1uqaTNO6RA15rCczZgj1mCCd0G+lcvuV077qRFBsw0vniRh8cMq5bTrV63tNH34NPPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAABtwcQTAAAAAAAA2uK8H0s+s2ikW9TstJt//qcvyfpv/sZvyPoT+56Q9YkxO8Vs31P6NYsL+un4WzbrZCCnVtfzcIcPH5T1akUne6RXdRKdUzSiOqYXdIJGKresl1O3U3MWFub0awr6OfiHnzlmLitmhB909+uElJm8Xo9AxE5kiIZ1okvJZyQZNZkuLRqpAddceb2sL8zo7Tu7MGm2keizE2g2tvVLqDOTxNYxBM9uYy3JYy0muzX5dV+92lKq3cqcPpYeuM9Ov3zTW3SSpy/W3XKqXavMzdssPa7W/vTD9UzOuxRZiYahkN0H+4J6m5VKOgEpltDH19atOj3WyWRyLaWYDQ/r5DonldIJTOFOnURT9+s2Sk0Ox+Ws0XeurMpyf5/eJp6Vdte4Fug3kF/VST8FI8HVCYV02k0k2NFaEl2nTtBygkbSTzCs2641SQPLGSm11Yrumw8dOiLr5aq9E61koo2uu0cnN6dS+thzFhf1eK5e1teD8QmdglfM2ftsfk6nvmVSel/2JOxr8ahxfmdX9HUiFtfHfq5stzF1Up/DuT7jNT697uW6nRK3bfuorC8t6/1RqduJVdWKbmfToN5W6dPzsj46Yd+/jIX0+XL4qE45XEzpsa9z+pRel6ce2y/rPcYlcst4k/c7oI/TjcwfMPpgz+6DS0aynN/ou6y0O6s/bZaQXDcGc/WA/X7LNX2sRmO6v8lm9XmdydmxlPmSPh+3bNbHhBUSuzJlH8M14z7RSiD0N0k0Dhj3osbwyu0sWY7F7RS+mpVs6rf6wSYpmn79s6CRatfdrccGIeN4d6Kddkrui+ETTwAAAAAAAGgLJp4AAAAAAADQFkw8AQAAAAAAoC2YeAIAAAAAAEBbMPEEAAAAAACAtmDiCQAAAAAAAG1hZ/i+QLWq4xd/4D3vN19z9NlndaNB3WwspqMGS2Ud/eyMb94k64P9Ot9zfn7GXNbho8dk/aq918j69PQpWS9UdbykkynpeNq6X8dL1sNG3GvVjm6NJnUsY1efjlnOG3GYTrJDLysS13GRR6d0dOumPr2fnFpNr0uxpNd9bmHBXNbw8LCsP/zww7K+nNLLWsrpOFsnX7Kjj19qXuoR9vb66fhUJ1DXP6vmdFT6333q47KeXpw22yim9M9+6IM/L+tlIzbXqfv03xdqxt8dmiTRr9txYkXaXiiX4nHdY0Tezs/reO+Gmu5XKpVaS9HPY2M6Bt558EF9bQ1FdH8eslOhvWw+JevRoF6PcDwu6wmfEU3sed6ZBd0HF+d1NHNXT6+sDxp1p2aMWVZX9DUin9Fx9k6io0vW/YE+vay8zp72h+xt0tmpj61s3o7Ethw4cEDWA8a4b9cVu1s5dBsSHUnvUvSd7xyU9bGxAfM1Q4P63Ht6nx5fz83ofTY+Yh+vlYC+Hnb26+NicECfd042p8eT+bIe54UTMVk/M2mPs/o26XOibMS0W0npMWN860wuTcr61q1bZH1uwR4zRqJ6HR98RJ8raWPVH35y0WyjS18OvN1X6vPriqt0dL1z6oRel1hUXzvnp7Oy/uD9+h7JKRu3Nr/pbVyhkL4PLtV1v/ld1lij3tLYpNmYxRzF1nVne2rSvofaMtwj61VjfxmntReO6GPe6R7Q16KVFX3tyFYLLe0Pp+LpdQ8bA5ByIW8vK6yvd/G4XsdiRV+H/MZY3Ennsq0dD+Zed/fUenyXz+l1XF7Sx0Pn2JDZxkCvPk7OB594AgAAAAAAQFsw8QQAAAAAAIC2YOIJAAAAAAAAbcHEEwAAAAAAANqCiScAAAAAAABc3FS7ZKd+Cv3YJp0i5ly/91pZ9xkJGjt37Wkp9cwp5vXT2Ec36ZSQkycfMZfV3aVTeB64/25Z9xuBGLe86dVmG/WEftr9V+7+kqxfufcKWe/qsVNdOob1E/gj4aisd/rs9IHpGZ3skZrU6UOeX+9bv98+1PKZbEtJOx0Jnc7nPHP4GVmfnDot65WaTqPwR+zEgMGRfu+lpFlaxsVMH7PeV5MAN/P9+u3cD9123Y5UyqR04ssXPvMJWb//G/8s61dfvs1so7Y6K+u5BZ0SE++3E2rKfuP8NhI2fBc3cK7lfbuWFJiLnaq3FieOHZb1ri6d8OQEQrGW+gKvro+JE6f09dMZGh6R9anJM/oFIXu/FIs6NclnJOGVjXS+Iyd1v+Vkisa5XdXLevpZnXZ7+VZ77BMN6OMrFNDbt6vD7tPKRZ30UynrhJqREWPM0GWn0NSMyCIrHeeRx54wlxUK63FU94Duz2vGBf3xx5802+jvvzT74K4evQ0u332d+ZqPf/xrsh43DpnX33aTrK+ml8w2EkmdGpVd0ePrUs2IuHLXFmN8tprS5/ZCSke4bdlj72NfVY8nZ8/odUyl9TjvtlfvNNt45tAhWa+GdWpgqmCnQ2/q0+fe8GYdRReY12NiL2QnfGeNYMxvf0unH8ZiHeayxrfqsUk4ou87Ygm93Rdm7G1y/LidIrZR+YzBUb1mjyd8fiNVuKr7IZ+RxlZvMiatGV1q1VjW9GKu5c+jTIzoY3gwqa9pS1Y0o+d5i4s6nbEe0MdXNltqeRyXjOkb9L5u497ZSKt2KkbE6mJan3RdPd2ynl61t3vOSJD1BfS1rlZtcswZ9zVB4z68w0iJtY53Z3pO36OcDz7xBAAAAAAAgLZg4gkAAAAAAABtwcQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAAC4uKl2maxOU5icnjNfs2v7VllPZ3UiWiq1LOuzczNmG8uLCy0lALztnW8xl7W4tCLrU7N6Ha+9Waf29Y7rJ9o7pZBOcnjLu26X9UJV/34obO+6YlG/plTWSSTptE4bcSIxnX403j/RUkJdbtWI3GgkLOn0gUJGH3PL8yfMZcUT+v1aoXqRoPGU/147NTAYMSKWLlFrSfiyk8SsNrwL8n7NJLN6i0lpTdrPGmkdVSNlqpTR15UTBx432/CX9GsOPLxF1vfeYl/XAp2jsl7zdOJZzfh7RLOUuFatZVnr2f6lyEryKhR04otTrehr/oyRBjszqxMbz5wxEuoaiTM6qSUa18fX5Jkpc1nVgr7m1zv1spZSeiyRNvqh7/5Mp8fUjVS7jHG+R3x2ytDuy8Zl3TqEu7vtxLnBEZ2elzRS6uoVvX7BJt1WpVJvbb8biZjOyLhe97l5nWRUKevtuH37drMN3yX6N9M9u6+X9U9+UifXOdsv0/v55lteI+vTkzqBMhSxx4zRqD6/chl9wJabRMvOnDHSjsr6AAzEdb3TSEFstDGtU9SSxmtynr7ejWwbM9tYrRv3Iwv6GnndTVeby7r/O0/p92UEuy3rpr3b3/RGs43HHjkg66cn9bofP2Gnjj3wsE6UvPlmnWCaKehzOJSw92Hdd+ml2iWMxMZi2U5erxj3XXXjGmpe2ZqMoX1Gqp4/qJeW6Oq1l2V0FLMLekzaldT3XJFItEmCfEjWnzxwUtZLdf37XUbyudPRpX/WndCpq4Wi7jedsLEuM3P63Kr59TbMl+3kPGP4YfIbaYlOwEijmxjX529Xh77+GyG8Dd099pjlxVyavTcAAAAAAAA2PCaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAABtYeervsBTjz8j63XPzgB89KFHZb2vr1vWh0aHZL2jS8dIO4MjOg61YMQyp7MZc1mdA7r9HVdfI+uJXh1BeGT6oNlGYjAs67WAjnLMFXUsdDIQN9uIJ3XkZzqdbiku2gl16kOkXNDv12ccD/66fZysLuh9YiTde8GQjsNsvKbFaM2+YR0rmuiyo0BXszpW9FLVLKa+buwEu956G63ye02W1SRyVvH5dOxpvUkbm8a3yPqHfubDsv7MQ3fJ+niHHcG75zIdod5TnZP1zGkdf+z0XqFjTys+fS0KGjGt1Ra37Ua2nsfjhdLTNyjrlYp9HJ0+PSnr09PTsj41raPQl1I6vtwp5EuyHjainMNh3W86dSMGOLWqo7cPHjwq67WA3ca2LRNeK4J+3XfF/PYJEU/qcy4a0Puqu7PLXNbAoL4WdPf2yfrU1JSs1yt2XPSmsVFZr9b0a7r77DjuuflFWQ8Y/Xa1oMcfoZCO0HZWVi7NPvhjH7tb1t9w+4D5mptvvlnWDxzcJ+u1qh77bhvX+9h5dt/Tsr7rsitk/dt367G9EzROi2hEn0fjW/X5OD1rX3NCUT3+9QV1G2XjT+z3PfqY2cY1110p688evlfWr4jY49Ltl+t1XFzKyfrAJr2sz//jN8w2FvXQwLvuhl2y7gsZL3Dx9dVlWb/vPt1v+I3utK8nabaR6Lav0RtVPJFs6Zr3Xfrgq5v3RHpjNhuyJOJ6LBeL6mtoZ9K+f4yF9fsNVXU9VyzLetRnX78zOd2f9xj3aYGo3u7HzujxiuPzkq2N+a2bFzf+MO6dRzbpa+qqsX6+qj1WC4X19ipV9PXcq1XNZSU69f3r9gn9fjtjepvEY/Y1rVLU477zwSeeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAAAXN9Xu4Yd1ikW5bCelDA3rFJ5cTj8h/j/88kdkPdmpU9qcTEanm6QKuo1qyE5XyxR1ikZXTCe4rJb00/zjTdIa8kWd4OIL6ifU+2qtpYo5S0t6PU6dOqXfU14/gd/x1/Xc5PCgTgD0fPqp/allO4Um1qtTDo8fOSHro6ObzWXNTetEn65+nTI0OqQTg7JG0o4TbZKY9FJL8roU07/WQ73e+npHIzqV4hc+8nOy3lnUx6rTFdHXln4j4aoc16kUTiGvz71gQl9XK8Y530J3gTbIZo2E06SdHDQyMiLrp06flvVcUSeo9PXpBDVnNa3fVyGn00qzTZJlk1GdotLZoa/fE1sv0wsK2dfo05Mzsh4M6uPbV9d9c8pKm3Epqgm9HsM9Ok2oULL7YCvd7ehRneg3a/SBfT12cp41BogYCYRnpnS6lROO6OtKNGKk8BhxmVYKb6ONsE5x2uhuec02Wb/sMp085nzxC1+R9VhM91E3veJGWf/G1+8z2+jt0duzYuyyjk6dPuWspvT5HUvoYyltJEAdP22PGS+7TI/J/X69rJoRsPXFO+w2XnmLThq0usczU3ZKXCikz72n9ukx7uvf8HpZTyTnzTYCfj0GeOopnbI9PKb7BmfTmE4SLxn3SDPT+r5qacW+1o+O2dejjSqzmmk5Wda6vlmjrEBA/6TbSHxzIkYSXcnog9OrenzZaN9IvIsmdD2zqhMQC03S0uNxvayRuD4mZhdSst4Rs/t5n5EaWKnoNLZq1U6Jixnt9Bp96uJKquX7ikpZH0N+I2W6Ward2JC+dkVDelmxqB77DPToe3OnboyLzgefeAIAAAAAAEBbMPEEAAAAAACAtmDiCQAAAAAAAG3BxBMAAAAAAADagoknAAAAAAAAtMV5xxR19egn6nd32ckE1tPjZ6Z0ksPK4oKsDw3bT1ZfTusUi3CXnlOrNEm1e+qJfbI+6tcpapUVnQzQPaITN5zhYZ0Gd/qkTpzbdplO7ZldmDTbePKJJ2S9Z0C/r527d5rLqpf0k/Ynp8+0lFbgC9hP87eSlCJJnQw0c0InMjmhkE726A7rWJPMwqKsp1fzLadLbXRWEuLFTq5r9X01S3S8IOtipDkUjITNzpg+Jq/Ytt1sIhnUaaE1IxWjHLVTniaz+roajuqksEpAv9+63+4ufMYuqdVqLe+ntez39bNxUxxzeZ0S4/fb79lvJLVde+21sn7dja9oKRHVefCBh1tKY7t8xw5zWeWSPu4PPauTmcpl3Qf7jRQrJ5HokPWuDt13eZ4+hv1NUl2SHXq8NDyq06JeeaPeH049pN/XY089JesjIzr5slDS4zFnfkFfu7I5vT9GBjeZy6r79Njr2NHjsp5aTbecXLd5s51su5Gl0nos+yd/cof5ml1G4N3rb71B1r/x9cdl3Qi4ahge0SlIKxm9/xeX7cTBiLHb9lyzV9a//BWdtjevw7IaNlf1Cd7fo+uVuk5hbBKc6D17cFbWR8f0sXfX3SfNZRmBt96KsY6f/vRdsp5vMvTcMjEq6/ML+kXFqh4XOOFIVNa7BvT93lJGr0haD68bFtN2stpGlTDGWUGvydjESCXzBXW/3d2lr/fdnXZ6bcFIhowZr/FbA7ZGkqXu7+aWVlpKPgv57f6xYiTCX7f3Kln/wle+Juv93XbifW+/vqYtzOtkxs64vayAdf+Y1sf9yJBOAV7N28f81FyqpVS7zm5rvOJ5W8d1YuWgke4erOuxQchIS3QCvrWnXPOJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAW5z3Y8kXF+ZlvVTMma9ZmJuR9VpVP0H9zGmdCtHTZz+9PWCsQSatn8AfTOjEJmfbxJisxwMxWU92Dsp6tNN+On4kpJe1fWKrrPf16afQn5o8ZrYxYiTnjW4eaymJzllN6diPeFKnXmRyOqGuu9tOJqz7dOpGImFs94i9rNUVnQywuKATPFIZnRQ1P6eTD5x0zj7mN7L1THxbz4S8Vl/T7PdbTT5bz/ebWtYRLn4jFWthyU6VKQRLLSWF1Wt2jFexpJNYkn1GIpiR7FZbQ6iclcjRzIXYhxcmIW99lUr62po3UgidgBHvluzUqWt+I6lkfHzcbGN0WKcpHTz4rKw//YRO3XKWreScIZ3SUizqYzhuJNc5dX9r52/BiJJqlmrnD+ntGAjp8yHTpE8pGYkzh44caikNbmKrTsh1qjX9vqqVSssph8lOnXwVi+n+fGlJ97U9Q3oc4xQK+lzY6L7ytcdkPaWHLQ2vulmnn66k9Hl/7Kg+lrZu02NJJxjW++zpA/oYG9hkL2t4SKdJHTh8VNbzOjjPm5kzm/AeeFSnXP7YB14t64GIkYqpD++GbFafR6965W2y/sz+vzKXNWWEUFc9vQ/DYX2uHDtuRxOGQvo8SnTodOjjp4wN7673Y3p8n+yKtFQ/OW23ESrYCeMb1fZxneaZXbUjGEPG9bhkJLKWy/p6v7Rst2GF1NWNlO9A02GZleKq+80Fo8/uTdhj0huu1ymuAwM69bWrQ99TB4x788bPfK1dn4plu0/xGdtk+2U6oTdX1Tvk4OETZhvBgH5NKKzHEgM99jxDR1xv+0RULysc0L8fDtpjy4gV1Xke+MQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAW+hsPSFkRBBXjIhnx+fTEYS9vTrKebi/T9YLqzrO2KkFdKTxUIeOTFxa1ZHJzkhEx/dWVnTm6uHHdETrciltt7FjWNaHdun3W/brdc9mV802enp01G0hl5f1gM+efwwYUeWLi3o7Do1slvVYPG62Uano+NBVI8o5X9AxpM7Sio6oHxvSkcTdA4Oy3tWj6849993rXYp8NR3XWWuSRu83zvu6FWG/hpR6f721yHuf9YLGD4163d/ar1v5tI0f6nLeiHaPBnWcbj2gI4idqdkZWe/r1+d2vW7HFmdL+hrSYexD62rQbLN7xrXeZOyPf11Yi8tq7dcbL1nDazbq+WvF0Tu9PTqeOJPW/UfQOFajETu22MotvuaqvbJ+5RV7zEXt379f1pcW9HV9bk5nrp+Z0uePk8vouPloVEeI1+u6H9p71RVmG9devUvWJ489I+uLy/Y+jCb0OZ9M6HHUalr38yvL9rhk9+7d+n0t6AjvcNC+3hx45oCs12p6rBaL61joonE9dSqX4PnrpDKt1Rt847L8qb+8W9av3tst672Denzr3P+APi63TuhjrH9Ej1edTWP6/d537zdkvVLXtyHRhD7vnKxx+J2emZT18e1bZL3wjZNmG489eVTWb7hxq6z3Delt5WSLegdPT+lzolbT6x5qkmB+5IQ+78fH9fHg99vn8NyC7h/KxokXMMYyviZt5Ep63TeyG67eJuuxoL6HcR5+SvdpxYLex8lkl6zXmgzUg0H9s7xxz2cN3514RI8BggFjuqBs9ad6Oc41V+n+5tBRfT7G47pvDoR13+EUy3qfVIz7ykRCt+Hs2qn7884u3TcfPaXHJYuL82YbXXG9vXbtMI45e/N6fd16u3Ql9X14JKQPiGDQniIKGOO+88EnngAAAAAAANAWTDwBAAAAAACgLZh4AgAAAAAAQFsw8QQAAAAAAIC2YOIJAAAAAAAAFzfVrlTXyUj1esB8zdCoTr648bprZf2+7zxgLMlOPwgbT4J/0xtvk/U9m3aayzq+/5Csn3ryhKwHjSCn4oqdHrN/ZkrW40m9TVaWdbJLNqXTZpzhLWP6B369u1NLKXNZOSM9b2lZvybRoZ/y7/Pbc5yFgk5GXE7rdbxh11XmshZXdNreydOnZH14i07h6+yzE0ou26UT8i5VzdLKzPQv6wdGXIaVUPevL/LazQjnMzV9t8Y6Do/o827q4BlZjzdppbNXJxDljJSS/j6dhOL0+q0EIiPpbw2JhdZrrLoRcPXdZlqMnDPf70vMqpHu2tHRYb5mZVFfQ/uHdGpnzUqCaXL9jseSLSUdWUl0zrXX6n5weVmvR5ex7pPTs2Ybc3M68W5pQb+vUEiPcaYmdZ/ifOeBB2V9pF+/32Mn9DXCef0bdHreW9/6Dln/0hfvkPWn9j1rtjE0sknWe4yk4cUVfR1ytmzRCWL5vE4/Sq9mWkofcvoHdTrwRjdibOflZT0udP70z4z0uqv1NX/P9W+T9S/+89+abezY1ivrQ5svl/VUQe9L56t3PyHr4bg+9vPL+poT7bBjm3bu0sly4YR+zfDQhKyvZu1UuwcePC3r/QPflPWlBXvcf+2118v66ZOPynq9psfEXfal3rNCuXKpFVkf7LG3bzan90l6yThXjeVEm9xhFjJ2auVG1d+lYwVvvF6ntDlXXqXPoa/frdOxj57U/VNnp763aqbSbKBlyJd0X1+o6/3Vaxx4tZqddHzilO479+3X6Zpd3fr6tJpvcgwZ48JdO/W14zWvebW5qHljbJDL6/N0ZVmPP67Yodt29ly5q6V7l+5OOyk+FtXHadKoWwl1oSYxmplMsyjW5vjEEwAAAAAAANqCiScAAAAAAAC0BRNPAAAAAAAAaAsmngAAAAAAANAWTDwBAAAAAADg4qbavecDb5H1w0eOma+55qrrZP3Qs4dlvR7ST2lfWdBJZY32H3lS1neNbZP1pf12Eo1vVT+hvqeiU22WjPQ6X1ovx8nl9boUj8zLenJUJwZ1R+0n2hfTOv0o1t3RUgqN88C3vyPriURM1oN+nWRQKNjJeflCtqWj8zuP68SgZkkKp09Py/pTR56W9Y4uO9UuELaTHDcyX7P4Ous1VpSZGSTW6u+72W9fawl5awkxazEprfmy9Hx9rKNb1pdS+noQ77YTRzJZI4mmVyebROP28VrK6eM16Dvvy3+D32dvw3qL52OzXWiG6rV6zBn76VJNwls2kkSTSXvfl8s69aVopCN2dehl5XJZe1sayTlWqI3fzEBy1yi9rFhM9ze+gD62EzE7jaW/V6/j+Gi/rE9O68Sxw8ftfn5lQffnuYxOiZ0YN5JoG+uekPV4XG/HH3jv+2T9kcd02phz74P6Zzt27JD1xRV9fWq2r6yEnGQy2dJ6v1gS70YW9vQxM7HZXtdDp/W5l/PrZMpDU/r43rTr7WYb1YBxjE/pY6xa1vvMKWR1Iuuubbp/nHtYp1lH4/YYN72qU/UefGhS1hMJncBUbhL6ZYSIep///HFZ32YPo71r9ui+dkBfcrzUgq7HmwSbDRrJX1NnlmQ9U9HJdU7dCLybGNP7JJPW+6O7yVDZCNne0DaN6JTPfNHuC6oVPdZ4/w/cLut33PktWU9limYbFU9fWxcW9f1mrUkafdXoz6Nh3Uahosdf/rARs+h53vScfl9VY8xWNMLrllJ2umpXt76mvvpVr5D1WLTZeFGv44mT+lqwY9toy/fa2VympXvtaFinBjuxiN5XwaA+Fv1GanG1aicTNkstfDF84gkAAAAAAABtwcQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtzjvQ8vJrJ2S9e9iOVV1c0JmgRZ+OQDw+dVrWq0U7fnn7Tv2++gd17GV01Y69PHVcR7EmfTq2MFzXsZP1vI4WdTLTOmY5uaqjCQcruu1gzciX9Dzv5NSMrEejOt5yfk7/vpMx4p8rFR3tmUnrmOPB4QGzjVmjDUvngJ0pGwzqQ/rYpD62Oru7ZD0cM/JkG5GfTTJ4N7CLGSFfrxuR9xeZ9b58fntb1Y35+qpfHzOnZvQ5P3XwmNnGVTt0vHo2r69fx07pa5eTj+sI14gv2NI2MZJu13TM+dayT4xIW+vvJxv0kFuzUFAfX5WyvaLxuO6fjxw6KuvjW3QMcCymI32dTFZfv1dWVmR989i4uaxEVLdTyOrjvl7V/WClakdP5/O6j6pW9fkwPKIzz2+55RazjWOHj8h6PKiP1YgRc+z09Oj+bmlZr8fCkrHdJ7aZbVQqeox1anJa1uMJHVXtlIp625fLOr49Yuzz9Ipev8ayKmuPcr6YVlLGeMpnn189g5tlfdvu18n6pu03y/pQzd6ej97//8r6yZk5Wb/h+leZywpG9bj/voe/JusRI8q7UrU7nCee1jHmVsJ32rgWhXTqeEPOSGrv1UNGb1U30fDsU8/IendMr2OsX18Piil77Nkd0+edzxh6Ty+bi/K2TejzO2kMi/uS+to52h8w2/D7z/v2c8OIRPV7DgT1vVWDcUwm4npjvvddb5L1+eWM2cT+Z3V/U6vofnNyeslcVrGsj6OScR++WtbHZGxc95vOSlqvS9HoO7J5o58v2dvE7+ntu7ig73efeOKEuay6MWLdtUP3qT1d+iJRNcYrTk9nr9fK2KtWta8FfuOaah2MdWP4EQjYF8j+fj3Hcj74xBMAAAAAAADagoknAAAAAAAAtAUTTwAAAAAAAGgLJp4AAAAAAADQFkw8AQAAAAAAoC3OO1ZgMaOfBD+/OmW+pmtUP9X+yqHtsj4w0iHr/Z12Ilpnh05fyOV1ekjNDivzMn6dAJBa1KlUnvE0/+XUrNlGZ0jP9UVXdNv+MzoqI9BtPx1/15ZhWX/2mE4CqcfsRIZdV+yU9arxRP3lZR2VEYnodL7Gssr6Sfv9ffqp+ZGYvax8XkeR9A7pZYVDelnlJokBPu/STLVbCzNIrMX0BzM2YS2NrEHNaz29zl6WkcLj6QSImpGK6ffZF6OgT7cRCuplHT55ylzW8N49sl7xdOJMzW8n0ayXZoeDZyXhNX2R4K+vIWVx4/4tZmLb5bJeLtt9wfCw7gvCYX3sPfboQ7J+9dVXmm10GQkulVK5pWu0U63WW1rH1ZTuH+dmzphtBEP6OAoE9DapVfT1vmqsn3PD9de2lOC1MGuPo6pGGlwsqt9vIKDP30zGTvS1Em/zeSPhqGCnBhYKOtU3HAq21Lbn6fV2sstNIsQ2sOkVfcwke/V56lx38w/K+mvf+B5ZL3t67Ls0Y6faxbv3ynrFp4/LfN2+dShWdNJU74BObcoZSX+zc/Z1ohbQ9xapTFbWV1b1ssIB+3rf363Po1JB78NYt7koz1/Sx3JmRY99t47pNNKU307xigT0um8ZM5JCy/b23TKor5HLS7r9zUNxWS+V7LFyyEze2rjCQT3GCwWCLY81ajWrro+J7i77Pnhi86Cs33LTNbL+6GNPm8uamtWplKendMJpblUfEytNYh67OnV67vbLxltKyL1x6AqzjeERvU18VZ3ONzZiX4O3XrZV1kNGLGahoM/3gKfPk+++Mb3fIxHr2LKPuZKRUmuNcUrG9SnWJN39+0kqv/TOfAAAAAAAAFwSmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAABtwcQTAAAAAAAALm6qXSCiEx6CUXsRmZVFWV81khEWp5ZkfffEhNnG7JJ+Av/ivK53enaC2/W33yTr99xxp6yfOXFa1kd3j5lt9Cd1WkXGSOGbPaDbmI6mzTZeu/sHZP3G618h6+mafgJ+s+0bDOsn2geM1L5i0U7BOfDss7K+c4eRqOe3368lbKQPWE/mb5YUZaUPwE6oM4LK/nUntH/L+Zq+gdYYYSRe1Ui1u+a6V8r61z9zj9nGlVt1wkalohvP5uwEqK5unehY8QVaSiasXWJ/wfh+Ujc2os4uHZtkBCA2ZDK6rw2E9bHaPzjQUlpp4311draUVmanmHlePK5TXzJpncj17AGdzhOL2cmM8YBuPxyyXqOPo94u3Zc3SzkqFnSS1OS0nYQbDur31TegU3t6evT5Hk3Y/ebcou7nixXdD9abXLS7e3X7AeMSvLSkE4tmjTGckyvYCX0bWSmoEyBrYb3NnJWUvvLmC/oYm13S50oma58TI5tvkfXePj3+XFiy+66wb7mlYVPNSMjrapL0d+K0XsczM/r8KuvANy9QsXu1/gn9s6Dxkp1b9b51hnr1Ol4xrq+d+Zy+H+if0CneTrWsx9jVqj5XbrzWTqyyQqi7dfC4Nzigr4WLs3ayWaGg13Ej8xtJfFYqWLPks3DYuKc2rvdWknizFO6uRI+s979e3+s66ZyRsB6MtjQ2WDHuHRvt9+r31d2jz6GQkT5drZVbHvNHwjrlsVa3+8eQkYRbMPqhRCzU8n1IzUjPtfpNX5NEzkCgtfvdaDLeUkLu93tPtRHvFwAAAAAAAPASwMQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAAcHFT7bJZ/cT1we5N5mt6AvpJ6SuVeVl/77veKuur+ZzZxiNPPiLr+Yp+2nzPpu3msuZqOinupg/cLuuJmE6FWJrVaX7OqQOHZf3Y/oOyninp5IexvbvMNuqdOnpiaHyrrPvSdkLeclZv+5UVnWQwvmW45dSHHZfpdTl46JCsx+L6if3OalanOHl1PceaSOiUkFLFTuHLZC+9NI41M5ILmqUataq+foFzF4TPSIOrGdtq5+6rZP1+I23OyZd1dE7RCN4oNAlhDMd1ck7FZ59HG/GvFK0eJz4jne9SFTCSXXxGkqSTWtWpQom4TqhJdupUmcF++1hdWlhuKSUuFLaHHfmC7m9qdX2Aj23W/c3c/LTZRkeHvuanF/V6xGI6BSdnpP841aR+v9GETn8a27rDXNaMke52euG4rPcbaXezC3bKUCSirwXd3TpJsZmpyZmW0p3mjPXL5+0+ONFhxGttcK++/UdlfWhws/maL/zjXbJ+/SumZN0KOI3Ft5ht9PfpBOFShz72ywX9npzhHn1+nVrU9xD+gN6X6ZQ97j9zek7WizrUzosbAW7D/WYT3mCvrvcZYZbbRu3+NOrpWL1oSF8Lo516W8UTdsqUFRztM2JPOzvs63B3n94niYS+3p4+o6+3saTdN/mDl17/HDH6tHBQ3+s6eWPHBIOhllJfm6WIra7qe5KakVje1WUny8aMpPpiWS8rGdFjholRu++wEvqCQd12xBgz+P3xltuwkgkLBTvVzmfc78RjejvWjOjrpql24fOejvnX92Sz0ujM7W70zc3eb9Pk9w1+LwEAAAAAAICXKCaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAABtcd75fdmMjoSMWjmlnufNT8/Len1FR4s+eNc9st6/ecRso7dHRzbG+3tkfWxwk7msaFSvSymg131m+bSsR4yYTOcVr3uVrFeuulzW6wEdZ7gQNHJjXWSiEY04bcQWe2F7H0YjOjs2lTkl68lOvR75rI77dAoFHUvtN5JY8zk7ZrlS0hGPfp8+1CtlvazNY2NmG4HAy2e+tm6l4Ropm3XjBc0i3y+1+e+6lTBa1+tRMTbW5ZfvMdsol3W0e7Wm29ixY5e5LJ8/2NqKGPHLuLjKxrVqZWXFfE0sFmkpPjcY1H1BNmNHm1vxxPG4jjouWrnfjX4lJevptD4fOjp0dHtH9zazjYceuF/WJ4bG9XvK6b52cHjIbMNvbMdwRMfTh8s1c1lDm8Zbijqu1PWyOjs7zTZmZ3Ucei6n93swpNfDqRrtr67ofes3xkuxuD08rZR1LPRG1zWgj8uKMT5xVlfTsv7Yow/L+vg23a/0DQ6abeSyuv1sTo+nggH7ePW8fEvx8cWcvn6k0vZ1bfv2zbK+vJSR9VpRL2vH1l6zjdEePWbZPqava76iMb5uXAv1OpaKelzc16vPr0jUjjePhPX2LZeN8bXfvqZXjOvRyvKSrJdKxZaHEskOvR0vRc3Gt35/a9H2tVq1pbG1E4vFWmojENDvyYkk9XGUbNK+UjHWw6nV9PGVzeq5AZ9xfWx2LxYKBVvqNz2vtfVz8nl9rQsEzntq5ZygsU+sfVU3tmEz1rKsMVyxVDKXFQy2vo7n2lvzKwEAAAAAAIAmmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAABtwcQTAAAAAAAA2sJXb/aofAAAAAAAAGCN+MQTAAAAAAAA2oKJJwAAAAAAALQFE08AAAAAAABoCyaeAAAAAAAA0BZMPAEAAAAAAKAtmHgCAAAAAABAWzDxBAAAAAAAgLZg4gkAAAAAAABtwcQTAAAAAAAAvHb4/wHM8FBXyiXW/QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torchinfo\n",
    "import torch\n",
    "\n",
    "# Setting device so that code can run on GPU if available\n",
    "device_name = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda:0\"\n",
    "elif torch.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "device = torch.device(device_name)\n",
    "print(device.type)\n",
    "\n",
    "\n",
    "# Will apply the following transformations to the images:\n",
    "#explanations are given at the bottom of this file\n",
    "# Note we optionaly resize the images to 32x32 pixels in case they are not already that size.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "\n",
    "# We are Loading training and test datasets.\n",
    "# Then, we apply the transformations on them. so that they are in the right format.\n",
    "# ImageFolder automatically assigns labels to images based on folder name.\n",
    "trainset = torchvision.datasets.ImageFolder(root='./Linnaeus_5_32X32/train', transform=transform)\n",
    "testset = torchvision.datasets.ImageFolder(root='./Linnaeus_5_32X32/test', transform=transform)\n",
    "\n",
    "\n",
    "# we will split the testset into validation and test sets.\n",
    "# 50/50 split and randomly split. In general, this should be fine, but for small datasets, there is\n",
    "# a chance that the split may have unbalanced classes.\n",
    "val_size = int(0.5 * len(testset))\n",
    "test_size = len(testset) - val_size\n",
    "valset, testset = torch.utils.data.random_split(testset, [val_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "# These dataloaders will help us to load the data in batches and iterate over them efficiently\n",
    "# We can also choose to shuffle(whcih we do with the training data).\n",
    "# num_workers means how many subprocesses to use for data loading. We set it to 2 here.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) \n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = trainset.classes # this is how we get the class names\n",
    "\n",
    "# creating subplots to show some sample images from each class\n",
    "fig, axes = plt.subplots(1, len(classes), figsize=(15, 5))\n",
    "for i, class_name in enumerate(classes):\n",
    "\n",
    "    # we use class_idx because the labels are stored as indices so we convert class name to index\n",
    "    class_idx = trainset.class_to_idx[class_name]\n",
    "    for img, label in trainset:\n",
    "        if label == class_idx:\n",
    "            img = img / 2 + 0.5  # unnormalize\n",
    "            npimg = img.numpy()\n",
    "            \n",
    "            axes[i].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "            axes[i].set_title(class_name)\n",
    "            axes[i].axis('off')\n",
    "            break\n",
    "plt.show()\n",
    "\n",
    "# The lines below combine transformations of turning an image into a\n",
    "# pytorch tensor and normalising the tensor to have mean 0.5 and standard deviation 0.5 for each colour channel\n",
    "#  (Red, Green, Blue).\n",
    "# This is useful to do before feeding the images into a neural network \n",
    "# becaause it imrpoves model convergence and helps iwth stability (i.e. no large gradient updates)\n",
    "# transform = transforms . Compose (\n",
    "#  [ transforms . ToTensor () ,\n",
    "#  transforms . Normalize ((0.5 , 0.5 , 0.5) , (0.5 , 0.5 , 0.5) ) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "\n",
    "        # kernel size 3 means we are using 3x3 filters\n",
    "        # padding 1 means we are adding 1 pixel of padding around the input image\n",
    "        # out put channels is 64 means we are using 16 filters\n",
    "        # each pixel will contribute to 16 different feature maps\n",
    "\n",
    "        # first conv layers: it has 3 input channels (RGB), 64 output channels, kernel size 3, padding 1\n",
    "        # 64 and 128 should be sufficient to capture  features like edges, corners, textures etc and provide a good balance of complexity and generalisation\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3)\n",
    "        # self.conv4 = nn.Conv2d(256, 256, 3)\n",
    "        \n",
    "        # max pooling layer: kernel size 2, stride 2 (we are reducing the image size by half) \n",
    "        # also helps to reduce overfitting as it introduces generalisation\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # fully connected layers we push our flattened image through a regular neural network \n",
    "        self.fc1 = nn.Linear(256*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # conv then ReLU then maxpool\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        # x = (F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# I have tried 4 conv layers 2 linears, 4 conv layers 3 linears, 3 conv layers 2 linears\n",
    "# and found this generalises the best for this dataset.\n",
    "# Some results that I observed are found in the next few questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 16, 16]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]          73,856\n",
      "         MaxPool2d-4            [-1, 128, 8, 8]               0\n",
      "            Conv2d-5            [-1, 256, 6, 6]         295,168\n",
      "            Linear-6                  [-1, 120]       1,106,040\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "            Linear-8                    [-1, 5]             425\n",
      "================================================================\n",
      "Total params: 1,487,445\n",
      "Trainable params: 1,487,445\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.01\n",
      "Params size (MB): 5.67\n",
      "Estimated Total Size (MB): 6.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "# Create the network and move it to the device (GPU or CPU)\n",
    "net = myCNN()\n",
    "net = net.to(device)\n",
    "\n",
    "torchsummary.summary(net, (3, 32, 32))  # Batch size of 32, 3 color channels, 32x32 images   \n",
    "# This lets us see which order the layers are executed and the output shape of each layer. We can also see the \n",
    "# total number of learnable parameters in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have slightly deviated from what the question says. I have implemented 2 things\n",
    "\n",
    "# 1. Validation set to monitor validation loss and accuracy.\n",
    "# Dependent on this, we can save the best model at that point in time\n",
    "\n",
    "# 2. Early stopping if validation loss isn't improving. We allow 8 epocs with no improvement before stopping training \n",
    "# This allows us to prevent overfitting to the training data and get better generalisation on test data\n",
    "\n",
    "BEST_MODEL_WEIGHTS = None # to store the best model weights so that we can load it again later on\n",
    "\n",
    "def train(net, optimiser, criterion, trainloader, valloader, nr_epochs=30):\n",
    "    losses = [] # store the losses\n",
    "    best_val_loss = float('inf') # so we track the best validation loss and can compare the model\n",
    "    epochs_no_improve = 0 # counter for early stopping\n",
    "    global BEST_MODEL_WEIGHTS\n",
    "    for epoch in range(nr_epochs):  \n",
    "        # iterating per epoch\n",
    "        print(f'Epoch {epoch+1}/{nr_epochs}')\n",
    "\n",
    "        # going through all the batches in the training set\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients so that we don't accumulate gradients from previous batches\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # compute the output for all the images in the batch.\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            #compute the loss between the outputs and the labels\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #compute the gradients, i.e. backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            #update the weights \n",
    "            optimiser.step()\n",
    "\n",
    "            #statistics to print later\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                print(f'  Batch {i+1}, Loss: {np.mean(losses[-10:]):.3f}', end= ' || ')\n",
    "            \n",
    "        print('')\n",
    "        net.eval()  # set the model to evaluation mode\n",
    "        # we will accumulate validation loss and statistics to calculate accuracy on validation\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Note: correct and totlal are for calculating accuracy for validation set\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                # getting the inputs and the classes of the validation set\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs) # running it once in eval mode\n",
    "                loss = criterion(outputs, labels) # compute the loss\n",
    "                val_loss += loss.item() # accumulate the validation loss\n",
    "                _, predicted = torch.max(outputs.data, 1) # get the predicted class\n",
    "                total += labels.size(0) # total number of labels\n",
    "                correct += (predicted == labels).sum().item() # number of correct predictions\n",
    "        val_loss /= len(valloader) # average validation loss\n",
    "        val_accuracy = 100 * correct / total # validation accuracy\n",
    "        print(f'  Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss: # if validation loss improved\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            #saving the best model weights so we can load them later\n",
    "            print(\"  \\n Saving best model weights\", end='\\n')\n",
    "            BEST_MODEL_WEIGHTS = net.state_dict() \n",
    "            \n",
    "        else:\n",
    "            epochs_no_improve += 1 # incrementing the counter if no improvement\n",
    "        if epochs_no_improve >= 8:\n",
    "            print(\"Early stopping due to no growth in validation improvement\")\n",
    "            break\n",
    "\n",
    "        net.train()  # set the model back to training\n",
    "    print('Finished Training')\n",
    "if BEST_MODEL_WEIGHTS is not None:\n",
    "    net.load_state_dict(BEST_MODEL_WEIGHTS) # loading the best model weights saved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  Batch 10, Loss: 1.608 ||   Batch 20, Loss: 1.612 ||   Batch 30, Loss: 1.603 ||   Batch 40, Loss: 1.592 ||   Batch 50, Loss: 1.581 ||   Batch 60, Loss: 1.555 ||   Batch 70, Loss: 1.516 ||   Batch 80, Loss: 1.489 ||   Batch 90, Loss: 1.460 ||   Batch 100, Loss: 1.449 ||   Batch 110, Loss: 1.413 ||   Batch 120, Loss: 1.425 ||   Batch 130, Loss: 1.377 ||   Batch 140, Loss: 1.423 ||   Batch 150, Loss: 1.435 ||   Batch 160, Loss: 1.416 ||   Batch 170, Loss: 1.361 ||   Batch 180, Loss: 1.420 || \n",
      "  Validation Loss: 1.385, Validation Accuracy: 39.70%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 2/30\n",
      "  Batch 10, Loss: 1.372 ||   Batch 20, Loss: 1.363 ||   Batch 30, Loss: 1.313 ||   Batch 40, Loss: 1.319 ||   Batch 50, Loss: 1.307 ||   Batch 60, Loss: 1.307 ||   Batch 70, Loss: 1.310 ||   Batch 80, Loss: 1.306 ||   Batch 90, Loss: 1.286 ||   Batch 100, Loss: 1.309 ||   Batch 110, Loss: 1.216 ||   Batch 120, Loss: 1.299 ||   Batch 130, Loss: 1.296 ||   Batch 140, Loss: 1.220 ||   Batch 150, Loss: 1.246 ||   Batch 160, Loss: 1.283 ||   Batch 170, Loss: 1.201 ||   Batch 180, Loss: 1.188 || \n",
      "  Validation Loss: 1.251, Validation Accuracy: 49.30%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 3/30\n",
      "  Batch 10, Loss: 1.286 ||   Batch 20, Loss: 1.178 ||   Batch 30, Loss: 1.231 ||   Batch 40, Loss: 1.183 ||   Batch 50, Loss: 1.181 ||   Batch 60, Loss: 1.188 ||   Batch 70, Loss: 1.299 ||   Batch 80, Loss: 1.202 ||   Batch 90, Loss: 1.240 ||   Batch 100, Loss: 1.190 ||   Batch 110, Loss: 1.208 ||   Batch 120, Loss: 1.129 ||   Batch 130, Loss: 1.172 ||   Batch 140, Loss: 1.158 ||   Batch 150, Loss: 1.191 ||   Batch 160, Loss: 1.248 ||   Batch 170, Loss: 1.184 ||   Batch 180, Loss: 1.087 || \n",
      "  Validation Loss: 1.193, Validation Accuracy: 51.70%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 4/30\n",
      "  Batch 10, Loss: 1.102 ||   Batch 20, Loss: 1.149 ||   Batch 30, Loss: 1.116 ||   Batch 40, Loss: 1.138 ||   Batch 50, Loss: 1.203 ||   Batch 60, Loss: 1.096 ||   Batch 70, Loss: 1.209 ||   Batch 80, Loss: 1.176 ||   Batch 90, Loss: 1.148 ||   Batch 100, Loss: 1.121 ||   Batch 110, Loss: 1.185 ||   Batch 120, Loss: 1.144 ||   Batch 130, Loss: 1.086 ||   Batch 140, Loss: 1.137 ||   Batch 150, Loss: 1.083 ||   Batch 160, Loss: 1.134 ||   Batch 170, Loss: 1.088 ||   Batch 180, Loss: 1.056 || \n",
      "  Validation Loss: 1.144, Validation Accuracy: 54.30%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 5/30\n",
      "  Batch 10, Loss: 1.095 ||   Batch 20, Loss: 1.061 ||   Batch 30, Loss: 1.140 ||   Batch 40, Loss: 1.100 ||   Batch 50, Loss: 1.111 ||   Batch 60, Loss: 1.129 ||   Batch 70, Loss: 1.069 ||   Batch 80, Loss: 1.087 ||   Batch 90, Loss: 1.018 ||   Batch 100, Loss: 1.117 ||   Batch 110, Loss: 1.027 ||   Batch 120, Loss: 1.069 ||   Batch 130, Loss: 1.015 ||   Batch 140, Loss: 1.172 ||   Batch 150, Loss: 1.134 ||   Batch 160, Loss: 1.026 ||   Batch 170, Loss: 1.064 ||   Batch 180, Loss: 1.130 || \n",
      "  Validation Loss: 1.073, Validation Accuracy: 57.80%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 6/30\n",
      "  Batch 10, Loss: 1.052 ||   Batch 20, Loss: 1.110 ||   Batch 30, Loss: 1.082 ||   Batch 40, Loss: 0.996 ||   Batch 50, Loss: 0.995 ||   Batch 60, Loss: 1.023 ||   Batch 70, Loss: 1.018 ||   Batch 80, Loss: 1.071 ||   Batch 90, Loss: 1.075 ||   Batch 100, Loss: 1.019 ||   Batch 110, Loss: 1.097 ||   Batch 120, Loss: 1.101 ||   Batch 130, Loss: 0.999 ||   Batch 140, Loss: 1.035 ||   Batch 150, Loss: 1.054 ||   Batch 160, Loss: 1.054 ||   Batch 170, Loss: 1.058 ||   Batch 180, Loss: 0.972 || \n",
      "  Validation Loss: 1.102, Validation Accuracy: 56.90%\n",
      "Epoch 7/30\n",
      "  Batch 10, Loss: 1.031 ||   Batch 20, Loss: 0.995 ||   Batch 30, Loss: 1.085 ||   Batch 40, Loss: 1.034 ||   Batch 50, Loss: 1.022 ||   Batch 60, Loss: 1.023 ||   Batch 70, Loss: 1.084 ||   Batch 80, Loss: 1.044 ||   Batch 90, Loss: 0.951 ||   Batch 100, Loss: 1.063 ||   Batch 110, Loss: 1.033 ||   Batch 120, Loss: 1.062 ||   Batch 130, Loss: 1.055 ||   Batch 140, Loss: 1.032 ||   Batch 150, Loss: 1.000 ||   Batch 160, Loss: 0.988 ||   Batch 170, Loss: 0.977 ||   Batch 180, Loss: 1.008 || \n",
      "  Validation Loss: 1.029, Validation Accuracy: 59.70%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 8/30\n",
      "  Batch 10, Loss: 1.092 ||   Batch 20, Loss: 0.967 ||   Batch 30, Loss: 1.021 ||   Batch 40, Loss: 0.986 ||   Batch 50, Loss: 0.928 ||   Batch 60, Loss: 0.991 ||   Batch 70, Loss: 0.993 ||   Batch 80, Loss: 1.058 ||   Batch 90, Loss: 1.010 ||   Batch 100, Loss: 0.999 ||   Batch 110, Loss: 1.007 ||   Batch 120, Loss: 0.963 ||   Batch 130, Loss: 0.981 ||   Batch 140, Loss: 1.025 ||   Batch 150, Loss: 0.964 ||   Batch 160, Loss: 1.041 ||   Batch 170, Loss: 1.053 ||   Batch 180, Loss: 0.888 || \n",
      "  Validation Loss: 1.005, Validation Accuracy: 60.90%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 9/30\n",
      "  Batch 10, Loss: 0.956 ||   Batch 20, Loss: 0.966 ||   Batch 30, Loss: 0.948 ||   Batch 40, Loss: 1.035 ||   Batch 50, Loss: 1.019 ||   Batch 60, Loss: 0.990 ||   Batch 70, Loss: 0.876 ||   Batch 80, Loss: 1.001 ||   Batch 90, Loss: 0.917 ||   Batch 100, Loss: 0.949 ||   Batch 110, Loss: 1.029 ||   Batch 120, Loss: 1.021 ||   Batch 130, Loss: 0.974 ||   Batch 140, Loss: 0.919 ||   Batch 150, Loss: 1.035 ||   Batch 160, Loss: 0.947 ||   Batch 170, Loss: 0.908 ||   Batch 180, Loss: 1.027 || \n",
      "  Validation Loss: 0.993, Validation Accuracy: 61.10%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 10/30\n",
      "  Batch 10, Loss: 0.976 ||   Batch 20, Loss: 0.910 ||   Batch 30, Loss: 0.961 ||   Batch 40, Loss: 0.972 ||   Batch 50, Loss: 0.979 ||   Batch 60, Loss: 0.895 ||   Batch 70, Loss: 1.023 ||   Batch 80, Loss: 0.918 ||   Batch 90, Loss: 1.032 ||   Batch 100, Loss: 0.952 ||   Batch 110, Loss: 0.916 ||   Batch 120, Loss: 0.940 ||   Batch 130, Loss: 0.956 ||   Batch 140, Loss: 0.910 ||   Batch 150, Loss: 0.864 ||   Batch 160, Loss: 0.933 ||   Batch 170, Loss: 0.929 ||   Batch 180, Loss: 0.932 || \n",
      "  Validation Loss: 0.991, Validation Accuracy: 61.30%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 11/30\n",
      "  Batch 10, Loss: 1.011 ||   Batch 20, Loss: 0.948 ||   Batch 30, Loss: 0.994 ||   Batch 40, Loss: 0.937 ||   Batch 50, Loss: 0.922 ||   Batch 60, Loss: 0.966 ||   Batch 70, Loss: 0.946 ||   Batch 80, Loss: 0.887 ||   Batch 90, Loss: 0.918 ||   Batch 100, Loss: 0.923 ||   Batch 110, Loss: 1.008 ||   Batch 120, Loss: 0.880 ||   Batch 130, Loss: 0.913 ||   Batch 140, Loss: 0.929 ||   Batch 150, Loss: 0.900 ||   Batch 160, Loss: 0.794 ||   Batch 170, Loss: 0.868 ||   Batch 180, Loss: 0.914 || \n",
      "  Validation Loss: 0.945, Validation Accuracy: 63.80%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 12/30\n",
      "  Batch 10, Loss: 0.915 ||   Batch 20, Loss: 0.929 ||   Batch 30, Loss: 0.866 ||   Batch 40, Loss: 0.889 ||   Batch 50, Loss: 0.909 ||   Batch 60, Loss: 0.882 ||   Batch 70, Loss: 0.910 ||   Batch 80, Loss: 0.945 ||   Batch 90, Loss: 0.887 ||   Batch 100, Loss: 0.959 ||   Batch 110, Loss: 0.883 ||   Batch 120, Loss: 0.894 ||   Batch 130, Loss: 0.903 ||   Batch 140, Loss: 0.976 ||   Batch 150, Loss: 0.898 ||   Batch 160, Loss: 0.944 ||   Batch 170, Loss: 0.937 ||   Batch 180, Loss: 0.901 || \n",
      "  Validation Loss: 0.959, Validation Accuracy: 62.40%\n",
      "Epoch 13/30\n",
      "  Batch 10, Loss: 0.955 ||   Batch 20, Loss: 0.860 ||   Batch 30, Loss: 0.848 ||   Batch 40, Loss: 0.840 ||   Batch 50, Loss: 0.832 ||   Batch 60, Loss: 0.863 ||   Batch 70, Loss: 0.884 ||   Batch 80, Loss: 0.876 ||   Batch 90, Loss: 0.893 ||   Batch 100, Loss: 0.835 ||   Batch 110, Loss: 0.891 ||   Batch 120, Loss: 0.808 ||   Batch 130, Loss: 0.893 ||   Batch 140, Loss: 0.896 ||   Batch 150, Loss: 0.898 ||   Batch 160, Loss: 0.895 ||   Batch 170, Loss: 0.884 ||   Batch 180, Loss: 0.790 || \n",
      "  Validation Loss: 1.031, Validation Accuracy: 60.30%\n",
      "Epoch 14/30\n",
      "  Batch 10, Loss: 0.952 ||   Batch 20, Loss: 0.930 ||   Batch 30, Loss: 0.906 ||   Batch 40, Loss: 0.779 ||   Batch 50, Loss: 0.844 ||   Batch 60, Loss: 0.877 ||   Batch 70, Loss: 0.891 ||   Batch 80, Loss: 0.869 ||   Batch 90, Loss: 0.895 ||   Batch 100, Loss: 0.925 ||   Batch 110, Loss: 0.866 ||   Batch 120, Loss: 0.828 ||   Batch 130, Loss: 0.901 ||   Batch 140, Loss: 0.780 ||   Batch 150, Loss: 0.912 ||   Batch 160, Loss: 0.821 ||   Batch 170, Loss: 0.798 ||   Batch 180, Loss: 0.865 || \n",
      "  Validation Loss: 0.919, Validation Accuracy: 64.60%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 15/30\n",
      "  Batch 10, Loss: 0.909 ||   Batch 20, Loss: 0.872 ||   Batch 30, Loss: 0.784 ||   Batch 40, Loss: 0.883 ||   Batch 50, Loss: 0.841 ||   Batch 60, Loss: 0.877 ||   Batch 70, Loss: 0.726 ||   Batch 80, Loss: 0.790 ||   Batch 90, Loss: 0.889 ||   Batch 100, Loss: 0.860 ||   Batch 110, Loss: 0.804 ||   Batch 120, Loss: 0.908 ||   Batch 130, Loss: 0.853 ||   Batch 140, Loss: 0.813 ||   Batch 150, Loss: 0.887 ||   Batch 160, Loss: 0.778 ||   Batch 170, Loss: 0.870 ||   Batch 180, Loss: 0.882 || \n",
      "  Validation Loss: 0.936, Validation Accuracy: 63.20%\n",
      "Epoch 16/30\n",
      "  Batch 10, Loss: 0.820 ||   Batch 20, Loss: 0.740 ||   Batch 30, Loss: 0.802 ||   Batch 40, Loss: 0.808 ||   Batch 50, Loss: 0.804 ||   Batch 60, Loss: 0.778 ||   Batch 70, Loss: 0.777 ||   Batch 80, Loss: 0.890 ||   Batch 90, Loss: 0.842 ||   Batch 100, Loss: 0.809 ||   Batch 110, Loss: 0.779 ||   Batch 120, Loss: 0.742 ||   Batch 130, Loss: 0.918 ||   Batch 140, Loss: 0.834 ||   Batch 150, Loss: 0.779 ||   Batch 160, Loss: 0.914 ||   Batch 170, Loss: 0.811 ||   Batch 180, Loss: 0.817 || \n",
      "  Validation Loss: 0.918, Validation Accuracy: 65.80%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 17/30\n",
      "  Batch 10, Loss: 0.789 ||   Batch 20, Loss: 0.807 ||   Batch 30, Loss: 0.799 ||   Batch 40, Loss: 0.753 ||   Batch 50, Loss: 0.722 ||   Batch 60, Loss: 0.748 ||   Batch 70, Loss: 0.725 ||   Batch 80, Loss: 0.770 ||   Batch 90, Loss: 0.850 ||   Batch 100, Loss: 0.846 ||   Batch 110, Loss: 0.808 ||   Batch 120, Loss: 0.741 ||   Batch 130, Loss: 0.755 ||   Batch 140, Loss: 0.863 ||   Batch 150, Loss: 0.806 ||   Batch 160, Loss: 0.788 ||   Batch 170, Loss: 0.769 ||   Batch 180, Loss: 0.822 || \n",
      "  Validation Loss: 0.886, Validation Accuracy: 65.60%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 18/30\n",
      "  Batch 10, Loss: 0.745 ||   Batch 20, Loss: 0.755 ||   Batch 30, Loss: 0.853 ||   Batch 40, Loss: 0.728 ||   Batch 50, Loss: 0.719 ||   Batch 60, Loss: 0.762 ||   Batch 70, Loss: 0.920 ||   Batch 80, Loss: 0.703 ||   Batch 90, Loss: 0.765 ||   Batch 100, Loss: 0.822 ||   Batch 110, Loss: 0.789 ||   Batch 120, Loss: 0.786 ||   Batch 130, Loss: 0.741 ||   Batch 140, Loss: 0.757 ||   Batch 150, Loss: 0.779 ||   Batch 160, Loss: 0.871 ||   Batch 170, Loss: 0.819 ||   Batch 180, Loss: 0.855 || \n",
      "  Validation Loss: 0.921, Validation Accuracy: 64.80%\n",
      "Epoch 19/30\n",
      "  Batch 10, Loss: 0.812 ||   Batch 20, Loss: 0.796 ||   Batch 30, Loss: 0.762 ||   Batch 40, Loss: 0.841 ||   Batch 50, Loss: 0.737 ||   Batch 60, Loss: 0.682 ||   Batch 70, Loss: 0.749 ||   Batch 80, Loss: 0.687 ||   Batch 90, Loss: 0.715 ||   Batch 100, Loss: 0.735 ||   Batch 110, Loss: 0.777 ||   Batch 120, Loss: 0.745 ||   Batch 130, Loss: 0.783 ||   Batch 140, Loss: 0.601 ||   Batch 150, Loss: 0.816 ||   Batch 160, Loss: 0.755 ||   Batch 170, Loss: 0.774 ||   Batch 180, Loss: 0.797 || \n",
      "  Validation Loss: 0.917, Validation Accuracy: 65.70%\n",
      "Epoch 20/30\n",
      "  Batch 10, Loss: 0.699 ||   Batch 20, Loss: 0.736 ||   Batch 30, Loss: 0.740 ||   Batch 40, Loss: 0.690 ||   Batch 50, Loss: 0.718 ||   Batch 60, Loss: 0.736 ||   Batch 70, Loss: 0.793 ||   Batch 80, Loss: 0.754 ||   Batch 90, Loss: 0.739 ||   Batch 100, Loss: 0.737 ||   Batch 110, Loss: 0.690 ||   Batch 120, Loss: 0.761 ||   Batch 130, Loss: 0.777 ||   Batch 140, Loss: 0.754 ||   Batch 150, Loss: 0.737 ||   Batch 160, Loss: 0.724 ||   Batch 170, Loss: 0.771 ||   Batch 180, Loss: 0.723 || \n",
      "  Validation Loss: 0.860, Validation Accuracy: 67.20%\n",
      "  \n",
      " Saving best model weights\n",
      "Epoch 21/30\n",
      "  Batch 10, Loss: 0.726 ||   Batch 20, Loss: 0.731 ||   Batch 30, Loss: 0.642 ||   Batch 40, Loss: 0.585 ||   Batch 50, Loss: 0.641 ||   Batch 60, Loss: 0.710 ||   Batch 70, Loss: 0.769 ||   Batch 80, Loss: 0.704 ||   Batch 90, Loss: 0.712 ||   Batch 100, Loss: 0.725 ||   Batch 110, Loss: 0.613 ||   Batch 120, Loss: 0.734 ||   Batch 130, Loss: 0.738 ||   Batch 140, Loss: 0.805 ||   Batch 150, Loss: 0.747 ||   Batch 160, Loss: 0.765 ||   Batch 170, Loss: 0.776 ||   Batch 180, Loss: 0.690 || \n",
      "  Validation Loss: 0.865, Validation Accuracy: 67.00%\n",
      "Epoch 22/30\n",
      "  Batch 10, Loss: 0.697 ||   Batch 20, Loss: 0.695 ||   Batch 30, Loss: 0.721 ||   Batch 40, Loss: 0.676 ||   Batch 50, Loss: 0.641 ||   Batch 60, Loss: 0.690 ||   Batch 70, Loss: 0.693 ||   Batch 80, Loss: 0.698 ||   Batch 90, Loss: 0.717 ||   Batch 100, Loss: 0.653 ||   Batch 110, Loss: 0.751 ||   Batch 120, Loss: 0.649 ||   Batch 130, Loss: 0.686 ||   Batch 140, Loss: 0.720 ||   Batch 150, Loss: 0.711 ||   Batch 160, Loss: 0.650 ||   Batch 170, Loss: 0.682 ||   Batch 180, Loss: 0.659 || \n",
      "  Validation Loss: 0.876, Validation Accuracy: 67.10%\n",
      "Epoch 23/30\n",
      "  Batch 10, Loss: 0.572 ||   Batch 20, Loss: 0.675 ||   Batch 30, Loss: 0.696 ||   Batch 40, Loss: 0.630 ||   Batch 50, Loss: 0.655 ||   Batch 60, Loss: 0.673 ||   Batch 70, Loss: 0.611 ||   Batch 80, Loss: 0.656 ||   Batch 90, Loss: 0.676 ||   Batch 100, Loss: 0.710 ||   Batch 110, Loss: 0.717 ||   Batch 120, Loss: 0.701 ||   Batch 130, Loss: 0.645 ||   Batch 140, Loss: 0.630 ||   Batch 150, Loss: 0.731 ||   Batch 160, Loss: 0.712 ||   Batch 170, Loss: 0.704 ||   Batch 180, Loss: 0.689 || \n",
      "  Validation Loss: 0.884, Validation Accuracy: 66.30%\n",
      "Epoch 24/30\n",
      "  Batch 10, Loss: 0.606 ||   Batch 20, Loss: 0.663 ||   Batch 30, Loss: 0.626 ||   Batch 40, Loss: 0.625 ||   Batch 50, Loss: 0.565 ||   Batch 60, Loss: 0.700 ||   Batch 70, Loss: 0.697 ||   Batch 80, Loss: 0.589 ||   Batch 90, Loss: 0.601 ||   Batch 100, Loss: 0.624 ||   Batch 110, Loss: 0.592 ||   Batch 120, Loss: 0.626 ||   Batch 130, Loss: 0.665 ||   Batch 140, Loss: 0.704 ||   Batch 150, Loss: 0.734 ||   Batch 160, Loss: 0.709 ||   Batch 170, Loss: 0.599 ||   Batch 180, Loss: 0.593 || \n",
      "  Validation Loss: 0.879, Validation Accuracy: 67.60%\n",
      "Epoch 25/30\n",
      "  Batch 10, Loss: 0.647 ||   Batch 20, Loss: 0.646 ||   Batch 30, Loss: 0.595 ||   Batch 40, Loss: 0.585 ||   Batch 50, Loss: 0.601 ||   Batch 60, Loss: 0.665 ||   Batch 70, Loss: 0.656 ||   Batch 80, Loss: 0.603 ||   Batch 90, Loss: 0.595 ||   Batch 100, Loss: 0.599 ||   Batch 110, Loss: 0.678 ||   Batch 120, Loss: 0.667 ||   Batch 130, Loss: 0.620 ||   Batch 140, Loss: 0.590 ||   Batch 150, Loss: 0.624 ||   Batch 160, Loss: 0.691 ||   Batch 170, Loss: 0.623 ||   Batch 180, Loss: 0.655 || \n",
      "  Validation Loss: 0.922, Validation Accuracy: 65.10%\n",
      "Epoch 26/30\n",
      "  Batch 10, Loss: 0.674 ||   Batch 20, Loss: 0.672 ||   Batch 30, Loss: 0.597 ||   Batch 40, Loss: 0.612 ||   Batch 50, Loss: 0.701 ||   Batch 60, Loss: 0.575 ||   Batch 70, Loss: 0.626 ||   Batch 80, Loss: 0.585 ||   Batch 90, Loss: 0.623 ||   Batch 100, Loss: 0.573 ||   Batch 110, Loss: 0.666 ||   Batch 120, Loss: 0.584 ||   Batch 130, Loss: 0.573 ||   Batch 140, Loss: 0.596 ||   Batch 150, Loss: 0.630 ||   Batch 160, Loss: 0.578 ||   Batch 170, Loss: 0.625 ||   Batch 180, Loss: 0.591 || \n",
      "  Validation Loss: 0.889, Validation Accuracy: 66.80%\n",
      "Epoch 27/30\n",
      "  Batch 10, Loss: 0.593 ||   Batch 20, Loss: 0.520 ||   Batch 30, Loss: 0.587 ||   Batch 40, Loss: 0.554 ||   Batch 50, Loss: 0.635 ||   Batch 60, Loss: 0.557 ||   Batch 70, Loss: 0.630 ||   Batch 80, Loss: 0.570 ||   Batch 90, Loss: 0.637 ||   Batch 100, Loss: 0.627 ||   Batch 110, Loss: 0.597 ||   Batch 120, Loss: 0.544 ||   Batch 130, Loss: 0.611 ||   Batch 140, Loss: 0.551 ||   Batch 150, Loss: 0.599 ||   Batch 160, Loss: 0.563 ||   Batch 170, Loss: 0.516 ||   Batch 180, Loss: 0.582 || \n",
      "  Validation Loss: 0.889, Validation Accuracy: 67.60%\n",
      "Epoch 28/30\n",
      "  Batch 10, Loss: 0.547 ||   Batch 20, Loss: 0.530 ||   Batch 30, Loss: 0.603 ||   Batch 40, Loss: 0.513 ||   Batch 50, Loss: 0.618 ||   Batch 60, Loss: 0.570 ||   Batch 70, Loss: 0.537 ||   Batch 80, Loss: 0.518 ||   Batch 90, Loss: 0.577 ||   Batch 100, Loss: 0.553 ||   Batch 110, Loss: 0.506 ||   Batch 120, Loss: 0.506 ||   Batch 130, Loss: 0.508 ||   Batch 140, Loss: 0.559 ||   Batch 150, Loss: 0.572 ||   Batch 160, Loss: 0.654 ||   Batch 170, Loss: 0.585 ||   Batch 180, Loss: 0.523 || \n",
      "  Validation Loss: 0.887, Validation Accuracy: 68.90%\n",
      "Early stopping due to no growth in validation improvement\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = net.to(device) # making sure the model is on the right device\n",
    "# train(net, \n",
    "#       optim.SGD(net.parameters(), lr=0.01,\n",
    "#                  momentum=0.9), nn.CrossEntropyLoss(),\n",
    "#                    trainloader, valloader,\n",
    "#                      nr_epochs=30)\n",
    "\n",
    "train(net,\n",
    "      optim.Adam(net.parameters(), lr=0.0001), nn.CrossEntropyLoss(),\n",
    "        trainloader, valloader,\n",
    "          nr_epochs=30) \n",
    "\n",
    "\n",
    "# some decisions made intially: we used Adam optimiser with learning rate 0.01, as our criterion.\n",
    "# From the lecture notes, we know that Adam uses an adaptive learning rate that combines AdaGrad and RMSProp.\n",
    "# Using the first and second moments of the gradients to change the learning rate\n",
    "# Benefits of Adam: Faster convergence, good for complex models\n",
    "# However, it is very easy to overfit with Adam because of its fast learning rate nad we also have less control over when wanting to change the learning rate.\n",
    "\n",
    "# I noticed that Adam seems to constantly get stuck in a local minimum after some epochs with learning rate 0.01,\n",
    "# Hence, I decided to try Adam with a lower learning rate of 0.0001. This seemed to help and allowed us to see\n",
    "# gradual improvement in validation loss\n",
    "\n",
    "\n",
    "# One might consider SGD because of its improved generalisation, but it is slower to converge.\n",
    "# we used CrossEntropyLoss which is suitable for multi-class classification problems like ours. and ran for 30 epochs\n",
    "\n",
    "# Some things I noticed during training:\n",
    "# 1. Using 3 convolutional layers instead of 4 seemed to help with generalisation. \n",
    "# 2. Using Adam with lower learning rate of 0.0001 helped with convergence and generalisation. It also tended to reach 28-30 epochs without early stopping\n",
    "#    which means that the model was still learning and not overfitting too quickly and the initial learning rate allowed gradual updates to the weights.\n",
    "# 3. When using SGD or Adam with higher learning rates, the model tended to converge very quickly and no longer improved validation loss after about epoch 10.\n",
    "#    This was concerning as it meant that the model struggled to get out of local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class berry: 71.92%\n",
      "Accuracy for class bird: 49.54%\n",
      "Accuracy for class dog: 80.21%\n",
      "Accuracy for class flower: 69.35%\n",
      "Accuracy for class other: 55.90%\n",
      "Overall Accuracy: 65.00%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, testloader):\n",
    "    # Accuracy is (number of correct predictions) / (total number of predictions) * 100\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # we want a map from class index to class name for printing later\n",
    "    class_idxes = {}\n",
    "    for _, class_name in enumerate(classes):\n",
    "        class_idxes[trainset.class_to_idx[class_name]] = class_name\n",
    "\n",
    "    # we first set the correct and total counts for each class to 0\n",
    "    class_correct = list(0. for i in range(len(classes)))\n",
    "    class_total = list(0. for i in range(len(classes)))\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for data in testloader:\n",
    "            # Get the inputs; data is a list of [inputs, labels]\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Compute the outputs by passing the inputs to the model\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get the class with the highest 'energy' (prediction)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update correct and total counts for each class\n",
    "            for i in range(labels.size(0)):  # Iterate over each image in the batch\n",
    "                label = labels[i]\n",
    "                class_total[label] += 1\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "    accuracy = 100 * correct / total\n",
    "    for i in range(len(classes)):\n",
    "        if class_total[i] > 0:\n",
    "            class_accuracy = 100 * class_correct[i] / class_total[i]\n",
    "            print(f'Accuracy for class {class_idxes[i]}: {class_accuracy:.2f}%')\n",
    "    print(f'Overall Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "accuracy(net, testloader)\n",
    "\n",
    "\n",
    "# I decided to try Adam with a lower learning rate of 0.0001 instead of SGD with momentum\n",
    "# -------------------------\n",
    "# Adam lr=0.0001: 3 conv layers and 3 fully connected layers, max pooling after first 2 conv layers\n",
    "# -------------------------\n",
    "# Accuracy for class berry: 79.79%\n",
    "# Accuracy for class bird: 55.81%\n",
    "# Accuracy for class dog: 56.91%\n",
    "# Accuracy for class flower: 77.25%\n",
    "# Accuracy for class other: 69.95%\n",
    "# Overall Accuracy: 67.90%\n",
    "# -------------------------\n",
    "# This is my best model yet! It seems that Adam with a lower learning rate is able to generalise better across all classes.\n",
    "# It seems to be because we are able to have more gradual updates to the weights and avoid excessive overfitting.\n",
    "# Some classes like bird and other still have lower accuracy, perhaps due to class imbalance or inherent difficulty in distinguishing those classes,\n",
    "# but overall, this model is performing better than the previous ones.\n",
    "# -------------------------\n",
    "# Adam lr=0.001: 3 conv layers and 3 fully connected layers\n",
    "# -------------------------\n",
    "# Accuracy for class berry: 69.79%\n",
    "# Accuracy for class bird: 59.30%\n",
    "# Accuracy for class dog: 72.77%\n",
    "# Accuracy for class flower: 73.93%\n",
    "# Accuracy for class other: 43.24%\n",
    "# Overall Accuracy: 64.30%\n",
    "# -------------------------\n",
    "# This model is not performing as well as the Adam lr=0.0001 model. It seems that the higher learning rate is causing the model to overfit to certain classes while neglecting others.\n",
    "# The class 'other' has particularly low accuracy, which could be due to the model not being able to learn the features of that class well enough since it is a more diverse class.\n",
    "\n",
    "\n",
    "# SGD Outputs\n",
    "# -------------------------\n",
    "# SGD with momentum: 3 conv layers and 3 fully connected layers, max pooling after first 2 conv layers\n",
    "# -------------------------\n",
    "# Accuracy for class berry: 60.78%\n",
    "# Accuracy for class bird: 60.32%\n",
    "# Accuracy for class dog: 63.35%\n",
    "# Accuracy for class flower: 74.65%\n",
    "# Accuracy for class other: 61.58%\n",
    "# Overall Accuracy: 64.30%\n",
    "# -------------------------\n",
    "# SGD with momentum: 2 conv layers and 3 fully connected layers, max pooling after first 2 conv layers\n",
    "# -------------------------\n",
    "# Accuracy for class berry: 68.48%\n",
    "# Accuracy for class bird: 29.63%\n",
    "# Accuracy for class dog: 78.39%\n",
    "# Accuracy for class flower: 60.82%\n",
    "# Accuracy for class other: 75.36%\n",
    "# Overall Accuracy: 62.00%\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# Theses two CNNs with SGD have very interesting results. While the first CNN was able to generalise better across all classes,\n",
    "# the second CNN had very high accuracy for some classes (dog, other) but very low for others (bird).\n",
    "# This shows that the second CNN is overfitting to certain classes while neglecting others.\n",
    "# This could be due to the fact that the second CNN has less capacity to detect complex features.\n",
    "# or because of the randomness in the splitting of the test and validation set. For this reason i chose 3 conv layers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 2: Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset created.\n",
      "Data Loaders created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "folder_name = 'Linnaeus_5_32X32'\n",
    "new_folder_name = 'Linnaeus_5_32X32_small'\n",
    "\n",
    "# if it already exists, remove it\n",
    "if os.path.exists(new_folder_name):\n",
    "    shutil.rmtree(new_folder_name)\n",
    "\n",
    "#copy all the contents \n",
    "shutil.copytree(folder_name, new_folder_name)\n",
    "for subfolder in ['train', 'test']:\n",
    "    subfolder_path = os.path.join(new_folder_name, subfolder)\n",
    "    for class_folder in os.listdir(subfolder_path):\n",
    "        class_folder_path = os.path.join(subfolder_path, class_folder)\n",
    "        images = os.listdir(class_folder_path)\n",
    "\n",
    "        # keep only bird or dog subfolders\n",
    "        if class_folder not in ['bird','dog']:\n",
    "            shutil.rmtree(class_folder_path)\n",
    "\n",
    "print(\"Filtered dataset created.\")\n",
    "\n",
    "# Now we will create dataloaders for the small dataset\n",
    "trainset_small = torchvision.datasets.ImageFolder(root='./Linnaeus_5_32X32_small/train', transform=transform)\n",
    "testset_small = torchvision.datasets.ImageFolder(root='./Linnaeus_5_32X32_small/test', transform=transform)\n",
    "# These dataloaders will help us to load the data in batches and iterate over them efficiently\n",
    "# We can also choose to shuffle(which we do with the training data).\n",
    "trainloader_small = torch.utils.data.DataLoader(trainset_small, batch_size=64, shuffle=True)\n",
    "testloader_small = torch.utils.data.DataLoader(testset_small, batch_size=64, shuffle=False)\n",
    "print(\"Data Loaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = myCNN()\n",
    "net.load_state_dict(BEST_MODEL_WEIGHTS)  # load the weights from previous best model\n",
    "net = net.to(device)\n",
    "\n",
    "def partial_forwardCNN(x):\n",
    "    # We are running through our CNN up to the flattening layer\n",
    "    x = net.maxpool(F.relu(net.conv1(x)))\n",
    "    x = net.maxpool(F.relu(net.conv2(x)))\n",
    "    x = (F.relu(net.conv3(x)))\n",
    "    # x = (F.relu(net.conv4(x)))\n",
    "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "    return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # The input dimensions match the flattened dimensions from the CNN\n",
    "        self.fc1 = nn.Linear(256*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,2)\n",
    "        # Since we now only have two classes (bird and dog), the output layer has 2 neurons.\n",
    "\n",
    "        # Using LogSoftmax for output layer to get log-probabilities because we will use NLLLoss as criterion.\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # fully connected linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # output layers\n",
    "        x = self.fc3(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 32, 120]       1,106,040\n",
      "            Linear-2               [-1, 32, 84]          10,164\n",
      "            Linear-3                [-1, 32, 2]             170\n",
      "        LogSoftmax-4                [-1, 32, 2]               0\n",
      "================================================================\n",
      "Total params: 1,116,374\n",
      "Trainable params: 1,116,374\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.12\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 4.26\n",
      "Estimated Total Size (MB): 5.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp = mlp.to(device)\n",
    "torchsummary.summary(mlp, input_size=(32, 256*6*6))  # Input size matches the flattened output from CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(CNN, MLP, optimizer, criterion, trainloader, nr_epochs=30):\n",
    "\n",
    "    CNN.eval()  # set CNN to evaluation mode\n",
    "    for epoch in range(nr_epochs):\n",
    "        MLP.train()  # set MLP to training mode\n",
    "        print(f'Epoch {epoch+1}/{nr_epochs}')\n",
    "        cur_loss = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass through CNN and MLP\n",
    "            with torch.no_grad():  #clear gradients\n",
    "                cnn_output = partial_forwardCNN(inputs)\n",
    "            outputs = MLP(cnn_output)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass and optimization\n",
    "            loss.backward()\n",
    "\n",
    "            #backpropogate through mlp only and update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics to print later\n",
    "            cur_loss += loss.item()\n",
    "            \n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                print(f'  Batch {i+1}, Average Loss: {cur_loss / 10:.3f}', end=' || ')\n",
    "                cur_loss = 0.0    \n",
    "        print('')\n",
    "    print('Finished Fine-tuning')\n",
    "\n",
    "# Here, we only training the MLP and keeping the CNN frozen. This should help us to adapt the model to the new classes\n",
    "# without overfitting too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  Batch 10, Average Loss: 0.660 ||   Batch 20, Average Loss: 0.619 ||   Batch 30, Average Loss: 0.553 || \n",
      "Epoch 2/30\n",
      "  Batch 10, Average Loss: 0.538 ||   Batch 20, Average Loss: 0.480 ||   Batch 30, Average Loss: 0.506 || \n",
      "Epoch 3/30\n",
      "  Batch 10, Average Loss: 0.462 ||   Batch 20, Average Loss: 0.456 ||   Batch 30, Average Loss: 0.486 || \n",
      "Epoch 4/30\n",
      "  Batch 10, Average Loss: 0.448 ||   Batch 20, Average Loss: 0.468 ||   Batch 30, Average Loss: 0.425 || \n",
      "Epoch 5/30\n",
      "  Batch 10, Average Loss: 0.453 ||   Batch 20, Average Loss: 0.416 ||   Batch 30, Average Loss: 0.436 || \n",
      "Epoch 6/30\n",
      "  Batch 10, Average Loss: 0.409 ||   Batch 20, Average Loss: 0.395 ||   Batch 30, Average Loss: 0.394 || \n",
      "Epoch 7/30\n",
      "  Batch 10, Average Loss: 0.408 ||   Batch 20, Average Loss: 0.392 ||   Batch 30, Average Loss: 0.378 || \n",
      "Epoch 8/30\n",
      "  Batch 10, Average Loss: 0.367 ||   Batch 20, Average Loss: 0.350 ||   Batch 30, Average Loss: 0.355 || \n",
      "Epoch 9/30\n",
      "  Batch 10, Average Loss: 0.351 ||   Batch 20, Average Loss: 0.351 ||   Batch 30, Average Loss: 0.360 || \n",
      "Epoch 10/30\n",
      "  Batch 10, Average Loss: 0.332 ||   Batch 20, Average Loss: 0.346 ||   Batch 30, Average Loss: 0.360 || \n",
      "Epoch 11/30\n",
      "  Batch 10, Average Loss: 0.318 ||   Batch 20, Average Loss: 0.340 ||   Batch 30, Average Loss: 0.345 || \n",
      "Epoch 12/30\n",
      "  Batch 10, Average Loss: 0.284 ||   Batch 20, Average Loss: 0.317 ||   Batch 30, Average Loss: 0.318 || \n",
      "Epoch 13/30\n",
      "  Batch 10, Average Loss: 0.280 ||   Batch 20, Average Loss: 0.294 ||   Batch 30, Average Loss: 0.317 || \n",
      "Epoch 14/30\n",
      "  Batch 10, Average Loss: 0.266 ||   Batch 20, Average Loss: 0.296 ||   Batch 30, Average Loss: 0.293 || \n",
      "Epoch 15/30\n",
      "  Batch 10, Average Loss: 0.305 ||   Batch 20, Average Loss: 0.239 ||   Batch 30, Average Loss: 0.242 || \n",
      "Epoch 16/30\n",
      "  Batch 10, Average Loss: 0.251 ||   Batch 20, Average Loss: 0.265 ||   Batch 30, Average Loss: 0.259 || \n",
      "Epoch 17/30\n",
      "  Batch 10, Average Loss: 0.260 ||   Batch 20, Average Loss: 0.235 ||   Batch 30, Average Loss: 0.206 || \n",
      "Epoch 18/30\n",
      "  Batch 10, Average Loss: 0.209 ||   Batch 20, Average Loss: 0.218 ||   Batch 30, Average Loss: 0.219 || \n",
      "Epoch 19/30\n",
      "  Batch 10, Average Loss: 0.196 ||   Batch 20, Average Loss: 0.195 ||   Batch 30, Average Loss: 0.235 || \n",
      "Epoch 20/30\n",
      "  Batch 10, Average Loss: 0.188 ||   Batch 20, Average Loss: 0.201 ||   Batch 30, Average Loss: 0.229 || \n",
      "Epoch 21/30\n",
      "  Batch 10, Average Loss: 0.198 ||   Batch 20, Average Loss: 0.201 ||   Batch 30, Average Loss: 0.188 || \n",
      "Epoch 22/30\n",
      "  Batch 10, Average Loss: 0.176 ||   Batch 20, Average Loss: 0.160 ||   Batch 30, Average Loss: 0.194 || \n",
      "Epoch 23/30\n",
      "  Batch 10, Average Loss: 0.165 ||   Batch 20, Average Loss: 0.154 ||   Batch 30, Average Loss: 0.195 || \n",
      "Epoch 24/30\n",
      "  Batch 10, Average Loss: 0.156 ||   Batch 20, Average Loss: 0.157 ||   Batch 30, Average Loss: 0.161 || \n",
      "Epoch 25/30\n",
      "  Batch 10, Average Loss: 0.134 ||   Batch 20, Average Loss: 0.135 ||   Batch 30, Average Loss: 0.154 || \n",
      "Epoch 26/30\n",
      "  Batch 10, Average Loss: 0.131 ||   Batch 20, Average Loss: 0.146 ||   Batch 30, Average Loss: 0.126 || \n",
      "Epoch 27/30\n",
      "  Batch 10, Average Loss: 0.122 ||   Batch 20, Average Loss: 0.131 ||   Batch 30, Average Loss: 0.121 || \n",
      "Epoch 28/30\n",
      "  Batch 10, Average Loss: 0.114 ||   Batch 20, Average Loss: 0.102 ||   Batch 30, Average Loss: 0.129 || \n",
      "Epoch 29/30\n",
      "  Batch 10, Average Loss: 0.113 ||   Batch 20, Average Loss: 0.136 ||   Batch 30, Average Loss: 0.110 || \n",
      "Epoch 30/30\n",
      "  Batch 10, Average Loss: 0.109 ||   Batch 20, Average Loss: 0.101 ||   Batch 30, Average Loss: 0.100 || \n",
      "Finished Fine-tuning\n"
     ]
    }
   ],
   "source": [
    "fine_tune( net, mlp,\n",
    "            optim.Adam(mlp.parameters(), lr=0.0001),\n",
    "            nn.NLLLoss(),\n",
    "            trainloader_small,\n",
    ")\n",
    "# We are running fine-tuning for 30 epochs with Adam optimiser and learning rate 0.0001 and used NLLLoss as criterion\n",
    "# because our MLP outputs log-probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class bird: 82.75%\n",
      "Accuracy for class dog: 70.75%\n",
      "Overall Accuracy: 76.75%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_fine_tuned(CNN, MLP, testloader):\n",
    "    CNN.eval()  # set CNN to evaluation mode\n",
    "    MLP.eval()  # set MLP to evaluation mode\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # we want a map from class index to class name for printing later\n",
    "    class_idxes = {}\n",
    "    for _, class_name in enumerate(classes):\n",
    "        if class_name in ['bird', 'dog']:\n",
    "            class_idxes[trainset_small.class_to_idx[class_name]] = class_name\n",
    "        \n",
    "    # we first set the correct and total counts for each class to 0\n",
    "    class_correct = list(0. for i in range(len(class_idxes)))\n",
    "    class_total = list(0. for i in range(len(class_idxes)))\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for data in testloader:\n",
    "            # Get the inputs; data is a list of [inputs, labels]\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # forward pass through CNN and MLP\n",
    "            cnn_output = partial_forwardCNN(images)\n",
    "            outputs = MLP(cnn_output)\n",
    "            \n",
    "            # Get the class with the highest 'energy' (prediction)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update correct and total counts for each class\n",
    "            for i in range(labels.size(0)):  # Iterate over each image in the batch\n",
    "                label = labels[i]\n",
    "\n",
    "                # gather stats needed to calculate per-class accuracy\n",
    "                class_total[label] += 1\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "    \n",
    "    # storing overall accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # going through each class and calculating accuracy\n",
    "    for i in range(len(class_idxes)):\n",
    "        if class_total[i] > 0:\n",
    "            class_accuracy = 100 * class_correct[i] / class_total[i]\n",
    "            print(f'Accuracy for class {class_idxes[i]}: {class_accuracy:.2f}%')\n",
    "    print(f'Overall Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "accuracy_fine_tuned(net, mlp, testloader_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge increase in accuracy compared to training solely on the CNN model. We have an even amount of correct classifications showing that there is no bias towards either class.\n",
    "\n",
    "Previous Results From CNN:\n",
    "Accuracy for class berry: 79.79%\n",
    "Accuracy for class bird: 55.81%\n",
    "Accuracy for class dog: 56.91%\n",
    "Accuracy for class flower: 77.25%\n",
    "Accuracy for class other: 69.95%\n",
    "Overall Accuracy: 67.90%\n",
    "\n",
    "New Results From MLP Fine Tuning:\n",
    "Accuracy for class bird: 78.00%\n",
    "Accuracy for class dog: 77.50%\n",
    "Overall Accuracy: 77.75%\n",
    "\n",
    "While the accuracy is much large, the initial problem had a larger number of classes which makes it inherently more difficult. This experiment was especially useful since bird and dog had the lowest accuracies previously and now we are able to identify them much better. However, I'm not entirely convinced that the previous model was doing worse, since it had to classify more classes and it could be useful to do binary classification between all classes or finetune with more classes. This would give a better idea of how how well the model performs in differentiating between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Part 3: Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea behind MI-FGSM is to generate pertubations on imput images in order to mislead a classifier.\n",
    "# The parameter epsilon(e) represents the maximum allowed pertubation on an image.\n",
    "# The algorithm states that an adversarial example x* should satisfy:\n",
    "# ||x* - x||_infinity <= e  where x is the original input image.\n",
    "# A larger pertubation can be to more easily detected.\n",
    "# The aim of this method is to make it so that the Network doesnt classify x* the same as x.\n",
    "# However this may change based on what your purpose is.\n",
    "\n",
    "# Steps\n",
    "# 1. Initialise alpha as e / number of iterations\n",
    "# 2.  Caculate the gradient of the loss function wrt x*\n",
    "# This tells us how to change the image in order to increase the loss and mislead the classifier\n",
    "# the direction of the gradient indicates the direction in which the loss increases most rapidly.\n",
    "# 3. update the momentum term, g_t+1 = mu + g_t*normalised_grad\n",
    "# This helps to stabilise the pertubation updates and avoid local minima by accumulating a velocity vector in the direction of consistent gradient descent steps.\n",
    "# 4. a new adversarial example is generated as follows:\n",
    "# x*_t+1 = x*_t + alpha * sign(g_t+1)\n",
    "# we have generated a new adversarial example with at most epsilon pertubation\n",
    "# we iterated this process T times and return a final x*=x*_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adversarial(CNN, MLP, image, label, epsilon=0.01):\n",
    "    mu = 1\n",
    "    T = 10  # number of iterations\n",
    "    alpha = epsilon / T # step size how big each pertubation update is per time step\n",
    "    g_cur = 0 # initialise momentum term\n",
    "    x_star = image.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    for t in range(T):\n",
    "        x_star.requires_grad = True # we need gradients wrt the input image\n",
    "        # forward pass through CNN and MLP\n",
    "        cnn_output = partial_forwardCNN(x_star.unsqueeze(0))\n",
    "        outputs = MLP(cnn_output) # get the output for the adversarial example\n",
    "\n",
    "        # compute loss\n",
    "        loss = nn.NLLLoss()(outputs, label.unsqueeze(0))\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        grad = x_star.grad.data.squeeze(0)\n",
    "        # get the gradient of the loss wrt the input image\n",
    "        grad_abs = grad.abs()  # Absolute value of the gradient\n",
    "        grad_flat = grad_abs.view(grad.size(0), grad.size(1), -1)  # Flatten the height and width dimensions\n",
    "        grad_mean = grad_flat.mean(dim=-1, keepdim=True)  # Compute mean over the spatial dimensions\n",
    "\n",
    "        # Update momentum term\n",
    "        g_cur = mu * g_cur + grad / grad_mean  # Normalize the gradient\n",
    "\n",
    "        # update adversarial example\n",
    "        x_star = x_star + alpha * g_cur.sign()\n",
    "        \n",
    "        # clamp to ensure we stay within the epsilon ball\n",
    "        x_star = x_star.detach()\n",
    "\n",
    "    # doing a final prediction on the adversarial example\n",
    "    cnn_out = partial_forwardCNN(x_star.unsqueeze(0))\n",
    "    out = MLP(cnn_out)\n",
    "\n",
    "    # getting the predicted class and its probability\n",
    "    probababilities = torch.exp(out)\n",
    "    # we use exp because our MLP outputs log-probabilities and we want actual probabilities\n",
    "    predicted_prob, predicted_class = torch.max(probababilities, 1)\n",
    "    if predicted_class != label:\n",
    "        predicted_prob = 1 - predicted_prob\n",
    "\n",
    "    return x_star, predicted_prob\n",
    "\n",
    "# In this experiment, we will generate adversarial examples for a single image in the test set\n",
    "# and see how well our fine-tuned model can identify them as adversarial\n",
    "# We do this by:\n",
    "# 1. generating the output for the original image\n",
    "# 2. Follow the MI-FGSM algorithm to generate a pertubation that potentially misleads the classifier\n",
    "# 3. We then check if the pertubed image is classified differently from the original image\n",
    "# if it is we return 1 - the probability of the predicted class for the pertubed image\n",
    "# if it is not, we return the probability of the predicted class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.024999976..0.98970586].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of original class after attack: 0.0011\n",
      "Successfully found an adversarial example with p < 0.5\n",
      "Probability of original class after attack: 0.0011\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "successful_adv_image = None\n",
    "successful_original_image = None\n",
    "successful_label = None\n",
    "probability_of_original_class = None\n",
    "\n",
    "for data in testloader_small:\n",
    "    images, labels = data\n",
    "    for i in range(images.size(0)):\n",
    "        image = images[i]\n",
    "        label = labels[i]\n",
    "        if testset_small.class_to_idx['bird'] == label:\n",
    "            adv_image, prob = adversarial(net, mlp, image, label, epsilon=0.05)\n",
    "            print(f'Probability of original class after attack: {prob.item():.4f}')\n",
    "            if prob.item() < 0.5:\n",
    "                # we found a successful attack\n",
    "                successful_adv_image = adv_image\n",
    "                successful_original_image = image\n",
    "                successful_label = label\n",
    "                probability_of_original_class = prob\n",
    "                break\n",
    "    if successful_adv_image is not None:\n",
    "        break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure that successful_adv_image and successful_original_image are found\n",
    "if successful_adv_image is not None:\n",
    "    print(\"Successfully found an adversarial example with p < 0.5\")\n",
    "    print(f'Probability of original class after attack: {probability_of_original_class.item():.4f}')\n",
    "\n",
    "    # Unnormalize the images for plotting (if needed)\n",
    "    # Assuming the images were normalized with mean=0.5, std=0.5 during the preprocessing\n",
    "    successful_adv_image = successful_adv_image / 2 + 0.5\n",
    "    successful_original_image = successful_original_image / 2 + 0.5\n",
    "\n",
    "    # Convert from (C, H, W) to (H, W, C) for proper visualization\n",
    "    np_adv_img = successful_adv_image.detach().cpu().numpy()\n",
    "    np_orig_img = successful_original_image.detach().cpu().numpy()\n",
    "\n",
    "    # Plotting both images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot original image\n",
    "    axes[0].imshow(np.transpose(np_orig_img, (1, 2, 0)))  # Convert from (C, H, W) to (H, W, C)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')  # Hide the axes for better visualization\n",
    "\n",
    "    # Plot adversarial image\n",
    "    axes[1].imshow(np.transpose(np_adv_img, (1, 2, 0)))  # Convert from (C, H, W) to (H, W, C)\n",
    "    axes[1].set_title(f'Adversarial Image\\nProbability: {probability_of_original_class.item():.4f}')\n",
    "    axes[1].axis('off')  # Hide the axes for better visualization\n",
    "\n",
    "    # Add a common title for both images\n",
    "    plt.suptitle(f'Probability of original class after attack: {probability_of_original_class.item():.4f}', fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No successful adversarial attack found.\")\n",
    "\n",
    "# In this section, we find an example of a bird image from the test set that was correctly classified by the fine-tuned model\n",
    "# and generate an adversarial example using the MI-FGSM method with epsilon = 0.05 and display it.\n",
    "# The steps are as follows:\n",
    "# 1. Select an image of a bird from a testing set that was correctly classified by the fine-tuned model\n",
    "# and generate an adversarial example using the function adversarial at  = 0.05, and print\n",
    "# the probability p from the function.\n",
    "# 2. Repeat until we find an image that is successfully attacked with p < 0.5\n",
    "# 3. Plot the original image, the adversarial image, and the probability that the adversarial image\n",
    "# is classified as the true label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial examples with epsilon=0.05: 91.53%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9153094462540716"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_adversarial(CNN, MLP, testloader_small, epsilon):\n",
    "    CNN.eval()  # set CNN to evaluation mode\n",
    "    MLP.eval()  # set MLP to evaluation mode\n",
    "\n",
    "    # to store total number of correctly classified images and total number of attacked images of the correctly classified ones\n",
    "    total_correct = 0\n",
    "    total_attacked = 0\n",
    "\n",
    "    # iterate over the test data\n",
    "    for data in testloader_small:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        # geting batch size so we can iterate over each image in the batch\n",
    "        for i in range(batch_size):\n",
    "            image = images[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            # forward pass through CNN and MLP to check if originally classified correctly\n",
    "            cnn_output = partial_forwardCNN(image.unsqueeze(0))\n",
    "            out = MLP(cnn_output)\n",
    "            # check if originally classified correctly\n",
    "            if out.argmax(dim=1) == label:\n",
    "                # only attack if originally classified correctly\n",
    "                adv_image, prob = adversarial(CNN, MLP, image, label, epsilon)\n",
    "                total_correct += 1\n",
    "\n",
    "                # getting attacked means the prob of original class is less than 0.5\n",
    "                if prob.item() < 0.5:\n",
    "                    total_attacked += 1\n",
    "\n",
    "    accuracy = 100 * total_attacked / total_correct\n",
    "    print(f'Accuracy on adversarial examples with epsilon={epsilon}: {accuracy:.2f}%')\n",
    "\n",
    "    return total_attacked/total_correct\n",
    "    \n",
    "accuracy_adversarial(net, mlp, testloader_small, epsilon=0.05)\n",
    "\n",
    "# This functions calculates the accuracy of  the fine-tuned model against adversarials.\n",
    "# we do this by:\n",
    "# 1. iterate over test set and find correctly classified images\n",
    "# 2. for each correctly classified image, generate an adversarial example using the adversarial function\n",
    "# 3. check if the adversarial example is misclassified (i.e. prob of original class < 0.5)\n",
    "# 4. finally, we calculate the accuracy as (number of attacked images) / (number of correctly classified images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have calculated the success rate for each epsilon value, and plotted the results. using matplotlib and our previously defined function accuracy_adversarial().\n",
    "\n",
    "\n",
    "<br><br> The success rate in this experiment refers to the number of images that were correctly classified originally and rejected as their adversarial counterpart. \n",
    "<br>Looking at the output of this experiment, we can see that with larger epsilons, the change of correctly classifying an disfigured image decrease. This is expected because with larger epsilons, the perturbations to the image are larger and thus the image is more distorted. We can see that the graph is log-like in shape with a fast drop off in accuracy for small epsilons and then a slower decrease for larger epsilons. \n",
    "<br>\n",
    "We can also see that even with epsilon = 0.04, the model able to reject adversarial images as their own class with close to 90% accuracy. This shows that the model is quite robust to adversarial attacks and is able to identify them correctly unless epsilon is extremely small.<br>\n",
    "\n",
    "For epsilon >= 0.06, our model is able to reject adversarial images with almost 100% accuracy within 4%. Any pertubations in this range can be almost guaranteed to be rejected by the model. This shows that the model is very robust to adversarial attacks and can identify them correctly unless epsilon is extremely small. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial examples with epsilon=0.02: 58.31%\n",
      "Accuracy on adversarial examples with epsilon=0.04: 86.32%\n",
      "Accuracy on adversarial examples with epsilon=0.06: 95.44%\n",
      "Accuracy on adversarial examples with epsilon=0.08: 98.05%\n",
      "Accuracy on adversarial examples with epsilon=0.1: 99.67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilons = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "attack_success_rates = []\n",
    "for e in epsilons:\n",
    "    success_rate = accuracy_adversarial(net, mlp, testloader_small, epsilon=e)\n",
    "    attack_success_rates.append(success_rate)\n",
    "# Plotting the attack success rate vs epsilon\n",
    "plt.figure()\n",
    "plt.plot(epsilons, attack_success_rates, marker='o')\n",
    "plt.title('Attack Success Rate vs Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Attack Success Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
